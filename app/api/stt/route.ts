import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'
import { addTranslationJob } from '@/lib/translation-queue'

export async function POST(req: NextRequest) {
  try {
    const formData = await req.formData()
    const audio = formData.get('audio') as File
    const sessionId = formData.get('sessionId') as string
    const language = (formData.get('language') as string) || 'auto'
    const model = (formData.get('model') as string) || 'whisper-1'
    const responseFormat = (formData.get('response_format') as string) || 'verbose_json'
    const temperature = (formData.get('temperature') as string) || '0'
    const prompt = (formData.get('prompt') as string) || ''
    const enableGrammarCheck = formData.get('enableGrammarCheck') === 'true'
    const useGemini = formData.get('useGemini') === 'true'

    console.log('ğŸ¯ Enhanced STT API called with:', {
      audioSize: audio?.size,
      audioType: audio?.type,
      sessionId,
      language,
      model,
      responseFormat,
      temperature,
      hasPrompt: !!prompt,
      enableGrammarCheck,
      timestamp: new Date().toLocaleTimeString(),
    })

    if (!audio || !sessionId) {
      console.error('Missing required data:', {
        audio: !!audio,
        sessionId: !!sessionId,
      })
      return NextResponse.json({ error: 'Audio file and session ID are required' }, { status: 400 })
    }

    // Check if audio file has content
    if (audio.size === 0) {
      console.log('Empty audio file received')
      return NextResponse.json({ transcript: '', confidence: 0 }, { status: 200 })
    }

    console.log('Processing audio file:', {
      size: audio.size,
      type: audio.type,
      name: audio.name,
      targetLanguage: language,
    })

    // Check if OpenAI API key is available
    if (!process.env.OPENAI_API_KEY) {
      console.log('OpenAI API key not configured, using placeholder')

      // Generate realistic placeholder text that varies
      const placeholderTexts = [
        "Welcome to today's lecture on artificial intelligence.",
        'Machine learning is transforming various industries.',
        'Deep learning models require large amounts of data.',
        'Natural language processing enables human-computer interaction.',
        'Computer vision allows machines to interpret visual information.',
        'Reinforcement learning helps AI agents learn through trial and error.',
        'Neural networks are inspired by the human brain structure.',
        'Data preprocessing is crucial for model performance.',
        'Feature engineering can significantly improve results.',
        'Cross-validation helps prevent overfitting in models.',
      ]

      // Use timestamp to create some variation but consistency within same session
      const textIndex = Math.floor(Date.now() / 10000) % placeholderTexts.length
      const randomText = placeholderTexts[textIndex]

      return NextResponse.json({
        transcript: randomText,
        confidence: 0.9,
        isPlaceholder: true,
        message: 'Using placeholder - configure OPENAI_API_KEY for real STT',
      })
    }

    let transcript = ''
    let confidence = 0
    let correctedText = ''

    try {
      // Step 1: Use Whisper for transcription
      const whisperFormData = new FormData()
      
      // ğŸ¯ ì •í™•í•œ íŒŒì¼ í™•ì¥ì ê°ì§€
      const getFileExtension = (mimeType: string, fileName?: string) => {
        // MIME íƒ€ì… ê¸°ë°˜ í™•ì¥ì ë§¤í•‘ (Whisper API í˜¸í™˜ë§Œ)
        const mimeToExtension: { [key: string]: string } = {
          'audio/webm': 'webm',
          'audio/mp4': 'm4a',
          'audio/wav': 'wav',
          'audio/ogg': 'ogg',
          'audio/mp3': 'mp3',
          'audio/mpeg': 'mp3',
          'audio/mpga': 'mp3'
        }
        
        // ğŸš« Whisper APIê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ í•„í„°ë§
        if (mimeType.includes('codecs=opus') || mimeType.includes('codecs=vorbis')) {
          console.log(`âš ï¸ Unsupported codec detected: ${mimeType}, using fallback`)
          mimeType = mimeType.split(';')[0] // codecs ë¶€ë¶„ ì œê±°
        }
        
        // íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ ì‹œë„
        if (fileName) {
          const nameExtension = fileName.split('.').pop()?.toLowerCase()
          if (nameExtension && ['webm', 'm4a', 'wav', 'ogg', 'mp3'].includes(nameExtension)) {
            console.log(`ğŸµ Using extension from filename: ${nameExtension}`)
            return nameExtension
          }
        }
        
        // MIME íƒ€ì…ì—ì„œ í™•ì¥ì ì¶”ì¶œ
        const extension = mimeToExtension[mimeType] || 'webm'
        console.log(`ğŸµ Using extension from MIME type: ${mimeType} â†’ ${extension}`)
        return extension
      }
      
      const fileExtension = getFileExtension(audio.type, audio.name)
      
      console.log(`ğŸµ Audio file analysis:`, {
        size: audio.size,
        type: audio.type,
        name: audio.name,
        extension: fileExtension,
        targetLanguage: language,
      })
      
      // ğŸš« íŒŒì¼ í¬ê¸° ë° í˜•ì‹ ê²€ì¦ ê°•í™”
      if (audio.size < 5000) { // ìµœì†Œ í¬ê¸° ì¦ê°€
        console.log('âš ï¸ Audio file too small, skipping...')
        return NextResponse.json({ 
          transcript: '', 
          confidence: 0, 
          error: 'Audio file too small' 
        })
      }
      
      // ğŸš« Whisper API ì§€ì› í˜•ì‹ ê²€ì¦
      const supportedFormats = ['webm', 'm4a', 'mp3', 'wav', 'ogg']
      if (!supportedFormats.includes(fileExtension)) {
        console.log(`âš ï¸ Unsupported format: ${fileExtension}, skipping...`)
        return NextResponse.json({ 
          transcript: '', 
          confidence: 0, 
          error: `Unsupported audio format: ${fileExtension}` 
        })
      }
      
      // ğŸš« íŒŒì¼ í¬ê¸° ìƒí•œ ê²€ì¦ (ë„ˆë¬´ í° íŒŒì¼ ë°©ì§€)
      if (audio.size > 25000000) { // 25MB
        console.log('âš ï¸ Audio file too large, skipping...')
        return NextResponse.json({ 
          transcript: '', 
          confidence: 0, 
          error: 'Audio file too large' 
        })
      }
      
      // ğŸ¯ Whisper API í˜¸ì¶œ ì¤€ë¹„
      whisperFormData.append('file', audio, `audio.${fileExtension}`)
      whisperFormData.append('model', model)

      // ì–¸ì–´ ì„¤ì • (ISO-639-1 í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
      if (language && language !== 'auto') {
        const languageMap: { [key: string]: string } = {
          'en-US': 'en',
          'en-GB': 'en',
          'ko-KR': 'ko',
          'ja-JP': 'ja',
          'zh-CN': 'zh',
          'es-ES': 'es',
          'fr-FR': 'fr',
          'de-DE': 'de',
        }
        
        const isoLanguage = languageMap[language] || language.split('-')[0]
        whisperFormData.append('language', isoLanguage)
        console.log(`ğŸŒ Using language hint: ${isoLanguage} (converted from ${language})`)
      } else {
        console.log('ğŸ” Using auto language detection for best results')
      }

      whisperFormData.append('response_format', responseFormat)
      whisperFormData.append('temperature', temperature)

      // ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
      if (prompt) {
        whisperFormData.append('prompt', prompt)
        console.log('ğŸ“ Using context prompt for better accuracy')
      }

      console.log('ğŸš€ Calling Whisper API...')
      const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
        },
        body: whisperFormData,
      })

      if (!whisperResponse.ok) {
        const errorText = await whisperResponse.text()
        console.error('Whisper API error:', errorText)
        
        // ğŸš« ì¬ì‹œë„ ë¡œì§ ì œê±° - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨
        console.log('âŒ Whisper API failed, skipping chunk')
        return NextResponse.json({ 
          transcript: '', 
          confidence: 0, 
          error: 'Whisper API failed' 
        })
      } else {
        const whisperData = await whisperResponse.json()
        transcript = whisperData.text?.trim() || ''
        confidence = whisperData.avg_logprob || 0.9
        console.log('âœ… Whisper API success:', { transcript, confidence })
      }

      // Step 2: Use Gemini for grammar correction and improvement (if enabled)
      if (enableGrammarCheck && transcript) {
        console.log('ğŸ”§ Using Gemini for grammar correction and improvement...')
        
        try {
          // Use Gemini API for grammar correction
          const geminiResponse = await fetch(`https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=${process.env.GEMINI_API_KEY}`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: [{
                  text: `You are a professional transcription editor. Your task is to:
1. Correct any grammar, spelling, or punctuation errors
2. Improve sentence structure and clarity
3. Maintain the original meaning and tone
4. Keep the text natural and conversational
5. Preserve technical terms and proper nouns
6. Add proper punctuation where missing

Return only the corrected text without explanations.

Original text: "${transcript}"`
                }]
              }],
              generationConfig: {
                temperature: 0.1,
                maxOutputTokens: 1000,
              }
            }),
          })

          if (geminiResponse.ok) {
            const geminiData = await geminiResponse.json()
            correctedText = geminiData.candidates?.[0]?.content?.parts?.[0]?.text?.trim() || transcript
            console.log('âœ… Gemini grammar correction completed')
          } else {
            console.error('Gemini API error:', await geminiResponse.text())
            correctedText = transcript // Fallback to original
          }
        } catch (geminiError) {
          console.error('Gemini API request failed:', geminiError)
          correctedText = transcript // Fallback to original
        }
      } else {
        correctedText = transcript
      }

    } catch (error) {
      console.error('API request failed:', error)
      transcript = `[STT Error - Audio at ${new Date().toLocaleTimeString()}]`
      correctedText = transcript
      confidence = 0.1
    }

    // Save to Supabase (only if sessionId is valid UUID)
    if (transcript && transcript.trim() && !transcript.includes('[Audio received at')) {
      // UUID í˜•ì‹ ê²€ì¦
      const isValidUUID = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(sessionId)
      
      if (isValidUUID) {
        const supabase = createClient(
          process.env.NEXT_PUBLIC_SUPABASE_URL!,
          process.env.SUPABASE_SERVICE_ROLE_KEY!,
        )

        // ğŸ†• í…ŒìŠ¤íŠ¸ìš© ì„¸ì…˜ ìƒì„± (sessions í…Œì´ë¸”ì— ì—†ì„ ê²½ìš°)
        try {
          const { data: sessionData, error: sessionError } = await supabase
            .from('sessions')
            .select('id')
            .eq('id', sessionId)
            .single()

          if (sessionError && sessionError.code === 'PGRST116') {
            // ì„¸ì…˜ì´ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ìš© ì„¸ì…˜ ìƒì„±
            console.log('ğŸ†• Creating test session:', sessionId)
            await supabase.from('sessions').insert({
              id: sessionId,
              title: 'Test Session',
              created_at: new Date().toISOString(),
              updated_at: new Date().toISOString(),
            })
          }
        } catch (sessionCreateError) {
          console.log('âš ï¸ Session creation failed (continuing anyway):', sessionCreateError)
        }

        // Try to insert with corrected_text, fallback to original_text only if column doesn't exist
        let transcriptId: string | null = null
        
        try {
          const { data: insertData, error: dbError } = await supabase.from('transcripts').insert([
            {
              session_id: sessionId,
              timestamp: new Date().toLocaleTimeString(),
              original_text: transcript,
              corrected_text: correctedText,
              created_at: new Date().toISOString(),
            },
          ]).select('id')

          if (dbError) {
            console.error('Database error with corrected_text:', dbError)
            
            // Fallback: insert without corrected_text
            const { data: fallbackData, error: fallbackError } = await supabase.from('transcripts').insert([
              {
                session_id: sessionId,
                timestamp: new Date().toLocaleTimeString(),
                original_text: transcript,
                created_at: new Date().toISOString(),
              },
            ]).select('id')
            
            if (fallbackError) {
              console.error('Fallback database error:', fallbackError)
            } else if (fallbackData && fallbackData[0]) {
              transcriptId = fallbackData[0].id
            }
          } else if (insertData && insertData[0]) {
            transcriptId = insertData[0].id
          }

          // ë²ˆì—­ íì— ì‘ì—… ì¶”ê°€ (ëª¨ë“  ì§€ì› ì–¸ì–´ë¡œ ë²ˆì—­)
          if (transcriptId && correctedText) {
            console.log('ğŸ”„ Adding translation jobs for all supported languages...')
            
            // ì§€ì›ë˜ëŠ” ëª¨ë“  ì–¸ì–´ì— ëŒ€í•´ ë²ˆì—­ ì‘ì—… ì¶”ê°€
            const supportedLanguages = ['ko', 'zh', 'hi', 'ja', 'es', 'fr', 'de']
            
            for (const lang of supportedLanguages) {
              addTranslationJob(correctedText, lang, sessionId, 1, transcriptId)
            }
            
            console.log(`ğŸ“ Added ${supportedLanguages.length} translation jobs for transcript ID: ${transcriptId}`)
          }
        } catch (error) {
          console.error('Database insertion error:', error)
        }
      } else {
        console.log('âš ï¸ Skipping database save - invalid sessionId format:', sessionId)
      }
    }

    return NextResponse.json({
      transcript: correctedText || transcript,
      originalTranscript: transcript,
      confidence,
      duration: 0,
      grammarCorrected: enableGrammarCheck && correctedText !== transcript,
    })
  } catch (error) {
    console.error('STT API error:', error)
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 })
  }
}
