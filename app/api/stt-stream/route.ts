import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'
import { detectLanguage } from '@/lib/translation-cache'
import { saveTranslationToCache } from '@/lib/translation-cache'

// Gemini ê²€ìˆ˜ + ë²ˆì—­ í•¨ìˆ˜ (ì§ì ‘ í˜¸ì¶œ)
async function reviewAndTranslateWithGemini(
  originalText: string,
  detectedLanguage: string
): Promise<{
  reviewedText: string
  translations: Record<string, string>
  quality: number
}> {
  const geminiApiKey = process.env.GEMINI_API_KEY
  if (!geminiApiKey) {
    throw new Error('Gemini API key not found')
  }

  // ì…ë ¥ ì–¸ì–´ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ 3ê°œ ì–¸ì–´
  const allLanguages = ['ko', 'zh', 'hi', 'en']
  const targetLanguages = allLanguages.filter(lang => lang !== detectedLanguage)

  // ì–¸ì–´ë³„ ì´ë¦„ ë§¤í•‘
  const languageNames: Record<string, string> = {
    ko: 'Korean',
    zh: 'Chinese',
    hi: 'Hindi',
    en: 'English'
  }

  // ê²€ìˆ˜ ë° ë²ˆì—­ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
  let prompt = ''
  
  if (detectedLanguage === 'en') {
    prompt = `Here is the raw text from STT. Make MINIMAL corrections - only fix obvious grammar errors, remove filler words (ah, um, like), and add basic punctuation. Keep the original meaning and style intact.

Also translate the corrected text to ${targetLanguages.map(lang => languageNames[lang]).join(', ')}.

Original text: "${originalText}"

Please return a JSON response with this exact format:
{
  "reviewedText": "minimally corrected English text here",
  "translations": {
    "ko": "Korean translation here",
    "zh": "Chinese translation here", 
    "hi": "Hindi translation here"
  },
  "quality": 0.95
}`
  } else {
    const inputLanguageName = languageNames[detectedLanguage]
    prompt = `Here is the raw text from STT in ${inputLanguageName}. Make MINIMAL corrections - only fix obvious grammar errors, remove filler words, and add basic punctuation. Keep the original meaning and style intact.

Also translate the corrected text to ${targetLanguages.map(lang => languageNames[lang]).join(', ')}.

Original text: "${originalText}"

Please return a JSON response with this exact format:
{
  "reviewedText": "minimally corrected ${inputLanguageName} text here",
  "translations": {
    ${targetLanguages.map(lang => `"${lang}": "${languageNames[lang]} translation here"`).join(',\n    ')}
  },
  "quality": 0.95
}`
  }

  console.log(`ğŸ¤– Gemini review + translation for: "${originalText.substring(0, 50)}..." (${detectedLanguage} â†’ ${targetLanguages.join(', ')})`)

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${geminiApiKey}`,
    {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [
          {
            parts: [
              {
                text: prompt,
              },
            ],
          },
        ],
        generationConfig: {
          temperature: 0.1, // ë” ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
          maxOutputTokens: Math.max(Math.ceil(originalText.length * 4), 800), // í† í° ìˆ˜ ì¤„ì„
        },
      }),
    }
  )

  if (!response.ok) {
    const errorText = await response.text()
    console.error('Gemini API error:', response.status, errorText)
    throw new Error('Gemini API request failed')
  }

  const data = await response.json()

  if (data.candidates && data.candidates[0] && data.candidates[0].content) {
    const candidate = data.candidates[0]

    if (candidate.content.parts && candidate.content.parts[0] && candidate.content.parts[0].text) {
      let content = candidate.content.parts[0].text.trim()

      // JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
      if (content.startsWith('```json')) {
        content = content.replace(/^```json\s*/, '').replace(/\s*```$/, '')
      } else if (content.startsWith('```')) {
        content = content.replace(/^```\s*/, '').replace(/\s*```$/, '')
      }

      try {
        const result = JSON.parse(content)
        
        console.log(`âœ… Gemini review + translation completed`)
        
        return {
          reviewedText: result.reviewedText || originalText,
          translations: result.translations || {},
          quality: result.quality || 0.9
        }
      } catch (parseError) {
        console.error('JSON parsing error:', parseError)
        throw new Error('Failed to parse Gemini response')
      }
    }
  }

  throw new Error('Invalid Gemini response structure')
}

// In-memory session storage for quick access with enhanced duplicate prevention
interface SessionData {
  fullTranscript: string
  lastUpdate: Date
  recentChunks: Array<{text: string, hash: string, timestamp: number}>
  processedHashes: Set<string>
}

const activeSessions = new Map<string, SessionData>()

// Advanced text similarity and hash functions
function generateTextHash(text: string): string {
  // Normalize text for better duplicate detection
  const normalized = text.toLowerCase()
    .replace(/[\s\p{P}]+/gu, ' ')
    .trim()
  
  // Simple hash function (could be upgraded to crypto.createHash)
  let hash = 0
  for (let i = 0; i < normalized.length; i++) {
    const char = normalized.charCodeAt(i)
    hash = ((hash << 5) - hash) + char
    hash = hash & hash // Convert to 32-bit integer
  }
  return hash.toString(36)
}

function calculateSimilarity(text1: string, text2: string): number {
  const normalize = (text: string) => text.toLowerCase().replace(/[\s\p{P}]+/gu, ' ').trim()
  const norm1 = normalize(text1)
  const norm2 = normalize(text2)
  
  if (norm1 === norm2) return 1.0
  
  // Simple substring similarity
  const shorter = norm1.length < norm2.length ? norm1 : norm2
  const longer = norm1.length < norm2.length ? norm2 : norm1
  
  if (longer.includes(shorter) && shorter.length > 10) {
    return shorter.length / longer.length
  }
  
  return 0
}

function isDuplicateOrSimilar(text: string, session: SessionData): boolean {
  const textHash = generateTextHash(text)
  const now = Date.now()
  
  // Check exact hash matches
  if (session.processedHashes.has(textHash)) {
    console.log('ğŸš« Exact duplicate detected (hash match)')
    return true
  }
  
  // Check recent chunks for similarity (last 10 seconds, ë” ì—„ê²©í•˜ê²Œ)
  const recentChunks = session.recentChunks.filter(chunk => now - chunk.timestamp < 10000)
  
  for (const chunk of recentChunks) {
    const similarity = calculateSimilarity(text, chunk.text)
    if (similarity > 0.7) { // 70% similarity threshold (80% â†’ 70%)
      console.log(`ğŸš« High similarity detected: ${similarity.toFixed(2)} with "${chunk.text.substring(0, 30)}..."`)
      return true
    }
    
    // ë¶€ë¶„ í¬í•¨ ê´€ê³„ í™•ì¸ (ìƒˆë¡œ ì¶”ê°€)
    if (text.includes(chunk.text.substring(0, 20)) || chunk.text.includes(text.substring(0, 20))) {
      console.log(`ğŸš« Partial inclusion detected with "${chunk.text.substring(0, 30)}..."`)
      return true
    }
  }
  
  // Check for exact text matches in recent chunks (ìƒˆë¡œ ì¶”ê°€)
  const exactMatches = recentChunks.filter(chunk => 
    chunk.text.trim() === text.trim() || 
    chunk.text.trim().includes(text.trim()) ||
    text.trim().includes(chunk.text.trim())
  )
  
  if (exactMatches.length > 0) {
    console.log(`ğŸš« Exact text match detected with recent chunk`)
    return true
  }
  
  return false
}

// ğŸ¯ ì˜¤ë²„ë© ì¤‘ë³µ ì œê±° í•¨ìˆ˜ (ë” ì •êµí•œ ì²˜ë¦¬)
function removeOverlapDuplicates(newText: string, session: SessionData): string {
  const recentChunks = session.recentChunks.slice(-5) // ìµœê·¼ 5ê°œ ì²­í¬ í™•ì¸ (3 â†’ 5)
  
  for (const chunk of recentChunks) {
    const existingText = chunk.text
    
    // ì˜¤ë²„ë© íŒ¨í„´ ì°¾ê¸°
    const overlapPatterns = findOverlapPatterns(newText, existingText)
    
    if (overlapPatterns.length > 0) {
      // ê°€ì¥ ê¸´ ì˜¤ë²„ë© íŒ¨í„´ ì œê±°
      const longestOverlap = overlapPatterns.reduce((longest, current) => 
        current.length > longest.length ? current : longest
      )
      
      console.log(`ğŸ”„ Removing overlap: "${longestOverlap}"`)
      
      // ì˜¤ë²„ë© ì œê±°
      const cleanedText = newText.replace(longestOverlap, '').trim()
      
      if (cleanedText) {
        console.log(`âœ… After overlap removal: "${cleanedText}"`)
        return cleanedText
      } else {
        // ì˜¤ë²„ë© ì œê±° í›„ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì™„ì „íˆ ì¤‘ë³µëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
        console.log(`ğŸš« Complete overlap detected - skipping`)
        return ''
      }
    }
  }
  
  return newText
}

// ğŸ¯ ì˜¤ë²„ë© íŒ¨í„´ ì°¾ê¸° (ë” ì •êµí•œ ì²˜ë¦¬)
function findOverlapPatterns(newText: string, existingText: string): string[] {
  const patterns: string[] = []
  const minOverlapLength = 3 // ìµœì†Œ 3ì ì´ìƒì˜ ì˜¤ë²„ë©ë§Œ ê³ ë ¤ (5 â†’ 3)
  
  // ê¸°ì¡´ í…ìŠ¤íŠ¸ì˜ ë ë¶€ë¶„ê³¼ ìƒˆ í…ìŠ¤íŠ¸ì˜ ì‹œì‘ ë¶€ë¶„ ë¹„êµ
  for (let i = minOverlapLength; i <= Math.min(existingText.length, newText.length); i++) {
    const existingEnd = existingText.slice(-i)
    const newStart = newText.slice(0, i)
    
    if (existingEnd === newStart) {
      patterns.push(existingEnd)
    }
  }
  
  return patterns
}

// ğŸ¯ ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ë³‘í•©
function smartMergeText(newText: string, session: SessionData): string {
  const recentChunks = session.recentChunks.slice(-2) // ìµœê·¼ 2ê°œ ì²­í¬
  
  if (recentChunks.length === 0) {
    return newText
  }
  
  const lastChunk = recentChunks[recentChunks.length - 1]
  const existingText = lastChunk.text
  
  // ì˜¤ë²„ë© ì œê±°
  const cleanedNewText = removeOverlapDuplicates(newText, session)
  
  if (cleanedNewText === newText) {
    // ì˜¤ë²„ë©ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return newText
  }
  
  // ì˜¤ë²„ë©ì´ ì œê±°ëœ ê²½ìš°, ê¸°ì¡´ í…ìŠ¤íŠ¸ì™€ ë³‘í•©
  const mergedText = existingText + ' ' + cleanedNewText
  
  console.log(`ğŸ”— Smart merge: "${existingText}" + "${cleanedNewText}" = "${mergedText}"`)
  
  return mergedText
}

export async function POST(req: NextRequest) {
  try {
    const { type, sessionId, transcript, isPartial } = await req.json()

    console.log(`ğŸ¯ STT Stream ${type}:`, {
      sessionId,
      hasTranscript: !!transcript,
      isPartial,
      timestamp: new Date().toLocaleTimeString(),
    })

    switch (type) {
      case 'start':
        // Initialize session with enhanced tracking
        if (!activeSessions.has(sessionId)) {
          activeSessions.set(sessionId, {
            fullTranscript: '',
            lastUpdate: new Date(),
            recentChunks: [],
            processedHashes: new Set()
          })
          console.log(`ğŸš€ Enhanced STT session ${sessionId} initialized`)
        } else {
          // Reset session if already exists
          const session = activeSessions.get(sessionId)!
          session.recentChunks = []
          session.processedHashes.clear()
          console.log(`ğŸ”„ STT session ${sessionId} reset`)
        }
        return NextResponse.json({ success: true })

      case 'transcript':
        // Update session transcript
        const currentSession = activeSessions.get(sessionId)
        if (!currentSession) {
          console.error(`âŒ Session ${sessionId} not found for transcript update`)
          return NextResponse.json({ error: 'Session not found' }, { status: 404 })
        }

        // Enhanced text validation
        const cleanedTranscript = transcript?.trim()
        if (!cleanedTranscript || cleanedTranscript.length < 2) {
          console.log(`âš ï¸ Skipping empty or too short transcript: "${cleanedTranscript}"`)
          return NextResponse.json({
            success: true,
            message: 'Transcript too short, skipped',
          })
        }

        // ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì˜¤ë²„ë© ì¤‘ë³µ ì œê±°
        const processedTranscript = smartMergeText(cleanedTranscript, currentSession)
        
        if (processedTranscript !== cleanedTranscript) {
          console.log(`ğŸ”„ Smart overlap removal applied: "${cleanedTranscript}" â†’ "${processedTranscript}"`)
        }
        
        // Advanced duplicate detection (ì˜¤ë²„ë© ì œê±° í›„)
        if (isDuplicateOrSimilar(processedTranscript, currentSession)) {
          console.log(`ğŸš« Advanced duplicate detection blocked: "${processedTranscript.substring(0, 30)}..."`)
          return NextResponse.json({
            success: true,
            message: 'Duplicate/similar transcript blocked by advanced detection',
            blocked: true
          })
        }

        // Quality check: Skip very repetitive or low-quality text
        const words = processedTranscript.split(/\s+/)
        const uniqueWords = new Set(words.map((w: string) => w.toLowerCase()))
        const repetitionRatio = uniqueWords.size / words.length
        
        if (repetitionRatio < 0.3 && words.length > 5) {
          console.log(`âš ï¸ Skipping highly repetitive text (ratio: ${repetitionRatio.toFixed(2)}): "${processedTranscript.substring(0, 30)}..."`)
          return NextResponse.json({
            success: true,
            message: 'Repetitive text filtered out',
            filtered: true
          })
        }

        if (!isPartial && processedTranscript) {
          // Add to session tracking with enhanced metadata
          const textHash = generateTextHash(processedTranscript)
          const timestamp = Date.now()
          
          currentSession.fullTranscript += processedTranscript + ' '
          currentSession.lastUpdate = new Date()
          
          // Track this chunk
          currentSession.recentChunks.push({ 
            text: processedTranscript, 
            hash: textHash, 
            timestamp 
          })
          currentSession.processedHashes.add(textHash)
          
          // Cleanup old chunks (keep only last 50 or last 5 minutes)
          currentSession.recentChunks = currentSession.recentChunks
            .filter(chunk => timestamp - chunk.timestamp < 300000) // 5 minutes
            .slice(-50) // Keep last 50 chunks
          
          // Cleanup old hashes (keep last 200)
          if (currentSession.processedHashes.size > 200) {
            const hashArray = Array.from(currentSession.processedHashes)
            currentSession.processedHashes = new Set(hashArray.slice(-150))
          }
          
          console.log(`ğŸ“ Enhanced final transcript added to session ${sessionId}:`, processedTranscript)
          console.log(`ğŸ“Š Session stats: ${currentSession.recentChunks.length} recent chunks, ${currentSession.processedHashes.size} hashes tracked`)

          // Save EACH final sentence immediately to Supabase
          const supabase = createClient(process.env.NEXT_PUBLIC_SUPABASE_URL!, process.env.SUPABASE_SERVICE_ROLE_KEY!)

          const dbInsertStart = Date.now()
          console.log(`ğŸ’¾ Inserting transcript to DB: "${processedTranscript.substring(0, 50)}..."`)

          const { data, error: insertError } = await supabase
            .from('transcripts')
            .insert([
              {
                session_id: sessionId,
                timestamp: new Date().toLocaleTimeString(),
                original_text: processedTranscript,
                created_at: new Date().toISOString(),
                is_final: true,
                review_status: 'pending', // ê²€ìˆ˜ ëŒ€ê¸° ìƒíƒœë¡œ ì„¤ì •
                translation_status: 'pending', // ë²ˆì—­ ëŒ€ê¸° ìƒíƒœë¡œ ì„¤ì •
              },
            ])
            .select()

          const dbInsertTime = Date.now() - dbInsertStart

          if (insertError) {
            console.error(`âŒ DB insert error (${dbInsertTime}ms):`, insertError)
            return NextResponse.json({ error: 'Database error' }, { status: 500 })
          }

          console.log(`âœ… Transcript saved (id): ${data?.[0]?.id} - DB insert: ${dbInsertTime}ms`)
          const transcriptId = data?.[0]?.id

          // ğŸš€ Gemini ê²€ìˆ˜ + ë²ˆì—­ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
          console.log('ğŸŒ Starting Gemini review + translation...')

          // ê²€ìˆ˜ ë° ë²ˆì—­ ìƒíƒœë¥¼ 'processing'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
          const statusUpdateStart = Date.now()
          await supabase.from('transcripts').update({ 
            review_status: 'processing',
            translation_status: 'processing' 
          }).eq('id', transcriptId)
          const statusUpdateTime = Date.now() - statusUpdateStart

          console.log(`ğŸ”„ Review & translation status updated to 'processing' (${statusUpdateTime}ms)`)

          try {
            // ì–¸ì–´ ê°ì§€
            const detectedLanguage = detectLanguage(cleanedTranscript)
            console.log(`ğŸŒ Detected language: ${detectedLanguage}`)

            // Gemini ê²€ìˆ˜ + ë²ˆì—­ ì§ì ‘ í˜¸ì¶œ
            const reviewStart = Date.now()
            const reviewResult = await reviewAndTranslateWithGemini(cleanedTranscript, detectedLanguage)
            const reviewTime = Date.now() - reviewStart

            console.log(
              `ğŸš€ Gemini review + translation completed in ${reviewTime}ms for "${cleanedTranscript.substring(0, 30)}..."`,
            )

            // 1. transcripts í…Œì´ë¸”ì— ê²€ìˆ˜ëœ í…ìŠ¤íŠ¸ ì €ì¥
            console.log(`ğŸ’¾ Updating transcript ${transcriptId} with reviewed text: "${reviewResult.reviewedText.substring(0, 30)}..."`)
            
            const { error: updateError } = await supabase
              .from('transcripts')
              .update({
                reviewed_text: reviewResult.reviewedText,
                detected_language: detectedLanguage,
                review_status: 'completed'
              })
              .eq('id', transcriptId)

            if (updateError) {
              console.error('âŒ Error updating transcript with reviewed text:', updateError)
              throw new Error('Failed to update transcript')
            } else {
              console.log(`âœ… Successfully updated transcript ${transcriptId} with reviewed text`)
            }

            // 2. ë²ˆì—­ ê²°ê³¼ë¥¼ translation_cacheì— ì €ì¥í•˜ê³  ID ìˆ˜ì§‘
            const cacheIds: Record<string, string> = {}
            const cachePromises = Object.entries(reviewResult.translations).map(async ([targetLang, translatedText]) => {
              if (translatedText && translatedText.trim()) {
                try {
                  const cacheId = await saveTranslationToCache(
                    reviewResult.reviewedText, // ê²€ìˆ˜ëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ìœ¼ë¡œ ì‚¬ìš©
                    targetLang,
                    translatedText,
                    'gemini-review',
                    reviewResult.quality
                  )
                  
                  if (cacheId) {
                    cacheIds[targetLang] = cacheId
                    console.log(`âœ… Cached translation: ${targetLang} (ID: ${cacheId})`)
                  }
                } catch (cacheError) {
                  console.error(`âŒ Cache error for ${targetLang}:`, cacheError)
                }
              }
            })

            await Promise.all(cachePromises)

            // 3. transcripts í…Œì´ë¸”ì— translation_cache_ids ì—…ë°ì´íŠ¸
            if (Object.keys(cacheIds).length > 0) {
              console.log(`ğŸ’¾ Updating transcript ${transcriptId} with cache IDs:`, cacheIds)
              
              const { error: updateError } = await supabase
                .from('transcripts')
                .update({ 
                  translation_cache_ids: cacheIds,
                  translation_status: 'completed' 
                })
                .eq('id', transcriptId)

              if (updateError) {
                console.error('âŒ Error updating translation_cache_ids:', updateError)
              } else {
                console.log(`âœ… Successfully updated transcript ${transcriptId} with ${Object.keys(cacheIds).length} cache IDs`)
              }
            } else {
              console.log(`âš ï¸ No cache IDs to update for transcript ${transcriptId}`)
              // ë²ˆì—­ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
              await supabase.from('transcripts').update({ translation_status: 'completed' }).eq('id', transcriptId)
            }

            return NextResponse.json({
              success: true,
              transcriptId: transcriptId,
              originalText: cleanedTranscript,
              reviewedText: reviewResult.reviewedText,
              detectedLanguage: detectedLanguage,
              reviewCompleted: true,
              translationCompleted: true,
              translatedLanguages: Object.keys(reviewResult.translations || {}),
              reviewTime: reviewTime,
              totalTime: Date.now() - dbInsertStart,
              sessionStats: {
                totalChunks: currentSession.recentChunks.length,
                hashesTracked: currentSession.processedHashes.size,
                transcriptLength: currentSession.fullTranscript.length
              }
            })
          } catch (reviewError) {
            console.error('âŒ Gemini review + translation failed:', reviewError)

            // ê²€ìˆ˜ ë° ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ìƒíƒœë¥¼ pendingìœ¼ë¡œ ë˜ëŒë¦¼
            await supabase.from('transcripts').update({ 
              review_status: 'failed',
              translation_status: 'failed' 
            }).eq('id', transcriptId)

            // ê²€ìˆ˜ ë° ë²ˆì—­ ì‹¤íŒ¨í•´ë„ transcript ì €ì¥ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            return NextResponse.json({
              success: true,
              transcriptId: transcriptId,
              originalText: cleanedTranscript,
              reviewCompleted: false,
              translationCompleted: false,
              reviewError: reviewError instanceof Error ? reviewError.message : 'Unknown error',
              note: 'Transcript saved but review + translation failed',
            })
          }
        }

        return NextResponse.json({
          success: true,
          message: isPartial ? 'Partial transcript received' : 'Final transcript processed',
        })

      case 'end':
        // Enhanced session cleanup with stats
        const endSession = activeSessions.get(sessionId)
        const sessionStats = endSession ? {
          totalTranscriptLength: endSession.fullTranscript.length,
          chunksProcessed: endSession.recentChunks.length,
          hashesTracked: endSession.processedHashes.size,
          lastUpdate: endSession.lastUpdate
        } : null
        
        const ended = activeSessions.delete(sessionId)
        console.log(`ğŸ§¹ Enhanced session ${sessionId} cleanup (${ended ? 'removed' : 'not found'})`)
        if (sessionStats) {
          console.log(`ğŸ“Š Final session stats:`, sessionStats)
        }
        
        return NextResponse.json({ 
          success: true, 
          cleaned: ended,
          finalStats: sessionStats
        })

      default:
        return NextResponse.json({ error: "Invalid type. Use 'start', 'transcript', or 'end'" }, { status: 400 })
    }
  } catch (error) {
    console.error('âŒ STT Stream error:', error)
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 })
  }
}

// Enhanced GET endpoint with detailed session analytics
export async function GET(req: NextRequest) {
  try {
    const { searchParams } = new URL(req.url)
    const sessionId = searchParams.get('sessionId')
    const includeStats = searchParams.get('includeStats') === 'true'

    if (!sessionId) {
      return NextResponse.json({ error: 'Session ID is required' }, { status: 400 })
    }

    const session = activeSessions.get(sessionId)
    if (!session) {
      return NextResponse.json({ error: 'Session not found' }, { status: 404 })
    }

    const response: {
      transcript: string
      lastUpdate: Date
      length: number
      analytics?: {
        totalChunks: number
        hashesTracked: number
        recentActivity: number
        chunkSizes: Array<{
          length: number
          timestamp: number
          ageSeconds: number
        }>
        averageChunkSize: number
      }
    } = {
      transcript: session.fullTranscript,
      lastUpdate: session.lastUpdate,
      length: session.fullTranscript.length,
    }

    if (includeStats) {
      const now = Date.now()
      response.analytics = {
        totalChunks: session.recentChunks.length,
        hashesTracked: session.processedHashes.size,
        recentActivity: session.recentChunks
          .filter(chunk => now - chunk.timestamp < 60000) // Last minute
          .length,
        chunkSizes: session.recentChunks.map(chunk => ({
          length: chunk.text.length,
          timestamp: chunk.timestamp,
          ageSeconds: Math.round((now - chunk.timestamp) / 1000)
        })),
        averageChunkSize: session.recentChunks.length > 0 ? 
          Math.round(session.recentChunks.reduce((sum, chunk) => sum + chunk.text.length, 0) / session.recentChunks.length) : 0
      }
    }

    return NextResponse.json(response)
  } catch (error) {
    console.error('Enhanced STT Stream GET error:', error)
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 })
  }
}
