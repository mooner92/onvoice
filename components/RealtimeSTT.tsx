'use client'

import { useEffect, useRef, useState, useCallback } from 'react'
import { Button } from './ui/button'
import LanguageSelector from './LanguageSelector'

interface RealtimeSTTProps {
  sessionId: string
  onTranscriptUpdate: (transcript: string, isPartial: boolean) => void
  onError: (error: string) => void
  lang?: string
}

export function RealtimeSTT({ sessionId, onTranscriptUpdate, onError, lang = 'en-US' }: RealtimeSTTProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [hasPermission, setHasPermission] = useState(false)
  const [status, setStatus] = useState('Ready')
  const [chunkCount, setChunkCount] = useState(0)
  const [currentTranscript, setCurrentTranscript] = useState('')
  
  // ğŸŒ ì–¸ì–´ ì„ íƒ ìƒíƒœ (ì˜ì–´ ë°œí‘œ + í•œêµ­ì–´ íŠ¹ì´ì )
  const [primaryLanguage, setPrimaryLanguage] = useState('en-US')
  const [secondaryLanguage, setSecondaryLanguage] = useState('ko-KR')
  const [showLanguageSelector, setShowLanguageSelector] = useState(true)
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const isProcessingRef = useRef(false)
  const processingQueueRef = useRef<Blob[]>([])
  const currentTranscriptRef = useRef('') // ğŸ†• refë¡œ í˜„ì¬ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì¶”ì 
  const previousChunkRef = useRef<Blob | null>(null) // ğŸ†• ì´ì „ ì²­í¬ ë³´ì¡´
  const sessionIdRef = useRef(sessionId) // ğŸ†• sessionIdë¥¼ refë¡œ ê´€ë¦¬
  
  // ğŸ†• ì´ì¤‘ í ì‹œìŠ¤í…œ
  const primaryQueueRef = useRef<Blob[]>([])
  const secondaryQueueRef = useRef<Blob[]>([])
  const primaryTimerRef = useRef<NodeJS.Timeout | null>(null)
  const secondaryTimerRef = useRef<NodeJS.Timeout | null>(null)
  const lastProcessedTimeRef = useRef<number>(0)
  
  // ğŸ†• í ì„¤ì • - ë” ì•ˆì •ì ì¸ ê°’ìœ¼ë¡œ ì¡°ì •
  const CHUNK_INTERVAL = 2000 // 2ì´ˆ ì²­í¬ (ë” ì§§ê²Œ)
  const QUEUE_DELAY = 2000 // 2ì´ˆ ë”œë ˆì´
  const MIN_CHUNK_SIZE = 8000 // 8KB ìµœì†Œ (ë” í¬ê²Œ)
  
  // ğŸ†• sessionIdê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    sessionIdRef.current = sessionId
    console.log('ğŸ†” Session ID updated:', sessionId)
    
    // ğŸ†• sessionIdê°€ ì„¤ì •ë˜ë©´ ì¦‰ì‹œ ë¡œê·¸ ì¶œë ¥
    if (sessionId && sessionId.trim() !== '') {
      console.log('âœ… Session ID is ready for processing:', sessionId)
    }
  }, [sessionId])

  // ğŸŒ ì–¸ì–´ ì„ íƒ í•¸ë“¤ëŸ¬
  const handlePrimaryLanguageChange = (language: string) => {
    setPrimaryLanguage(language)
    // ë³´ì¡° ì–¸ì–´ê°€ ì£¼ ì–¸ì–´ì™€ ê°™ìœ¼ë©´ ì œê±°
    if (secondaryLanguage === language) {
      setSecondaryLanguage('none')
    }
  }

  const handleSecondaryLanguageChange = (language: string) => {
    setSecondaryLanguage(language)
  }

  const handleStartRecording = () => {
    setShowLanguageSelector(false)
    startRecording()
  }

  // ğŸ¤ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
  const requestPermission = useCallback(async () => {
    try {
      console.log('ğŸ¤ Requesting microphone permission...')
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
          channelCount: 1,
        },
      })
      
      stream.getTracks().forEach(track => track.stop())
      setHasPermission(true)
      setStatus('Permission granted')
      console.log('âœ… Microphone permission granted')
      return true
    } catch (error) {
      console.error('âŒ Microphone permission denied:', error)
      setStatus('Permission denied')
      onError('Microphone permission denied')
      return false
    }
  }, [onError])

  // ğŸ§¹ ì •ë¦¬ í•¨ìˆ˜
  const cleanup = useCallback(() => {
    console.log('ğŸ§¹ Cleaning up Realtime STT...')
    
    if (mediaRecorderRef.current) {
      if (mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
      }
      mediaRecorderRef.current = null
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    audioChunksRef.current = []
    processingQueueRef.current = []
    previousChunkRef.current = null
    
    // ğŸ†• ì´ì¤‘ í ì •ë¦¬
    primaryQueueRef.current = []
    secondaryQueueRef.current = []
    if (primaryTimerRef.current) {
      clearTimeout(primaryTimerRef.current)
      primaryTimerRef.current = null
    }
    if (secondaryTimerRef.current) {
      clearTimeout(secondaryTimerRef.current)
      secondaryTimerRef.current = null
    }
    
    setChunkCount(0)
    setIsRecording(false)
  }, [])

    // ğŸµ ì´ì¤‘ í ì‹œìŠ¤í…œ - 1ì°¨ í ì²˜ë¦¬
  const processPrimaryQueue = useCallback(async () => {
    if (primaryQueueRef.current.length === 0) return
    
    const audioBlob = primaryQueueRef.current.shift()!
    console.log('ğŸµ Processing primary queue chunk...')
    
    try {
      if (audioBlob.size < MIN_CHUNK_SIZE) {
        console.log('âš ï¸ Primary chunk too small, skipping...')
        return
      }

      // ğŸ†• sessionId ì§ì ‘ ê²€ì¦ (ë” ê°•ë ¥í•œ ê²€ì¦)
      const currentSessionId = sessionIdRef.current || sessionId
      if (!currentSessionId || currentSessionId.trim() === '') {
        console.log('âš ï¸ Session ID not ready yet, skipping chunk...')
        console.log('ğŸ” Debug - sessionId:', sessionId)
        console.log('ğŸ” Debug - sessionIdRef.current:', sessionIdRef.current)
        return
      }
      
      console.log('âœ… Session ID verified for processing:', currentSessionId)

      const result = await callWhisperAPI(audioBlob, currentSessionId)
      if (result.transcript && result.transcript.trim()) {
        console.log('ğŸ“ Primary queue result:', result.transcript)
        updateTranscript(result.transcript, true)
      }
    } catch (error) {
      console.error('âŒ Primary queue processing failed:', error)
    }
  }, [sessionId])

  // ğŸµ ì´ì¤‘ í ì‹œìŠ¤í…œ - 2ì°¨ í ì²˜ë¦¬ (ë”œë ˆì´ í›„)
  const processSecondaryQueue = useCallback(async () => {
    if (secondaryQueueRef.current.length === 0) return
    
    const audioBlob = secondaryQueueRef.current.shift()!
    console.log('ğŸµ Processing secondary queue chunk (delayed)...')
    
    try {
      if (audioBlob.size < MIN_CHUNK_SIZE) {
        console.log('âš ï¸ Secondary chunk too small, skipping...')
        return
      }

      const result = await callWhisperAPI(audioBlob, sessionId)
      if (result.transcript && result.transcript.trim()) {
        console.log('ğŸ“ Secondary queue result:', result.transcript)
        // 2ì°¨ íëŠ” ë” ì •í™•í•œ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
        updateTranscript(result.transcript, false)
      }
    } catch (error) {
      console.error('âŒ Secondary queue processing failed:', error)
    }
  }, [sessionId])

  // ğŸµ Whisper API í˜¸ì¶œ í•¨ìˆ˜
  const callWhisperAPI = useCallback(async (audioBlob: Blob, sessionIdParam?: string) => {
    // ğŸ†• sessionId ê²€ì¦ (ë§¤ê°œë³€ìˆ˜ ë˜ëŠ” ref ì‚¬ìš©)
    const currentSessionId = sessionIdParam || sessionIdRef.current
    if (!currentSessionId || currentSessionId.trim() === '') {
      throw new Error('Session ID is required')
    }
    
    console.log('ğŸ¯ Calling Whisper API with sessionId:', currentSessionId)
    console.log('ğŸµ Audio blob info:', {
      size: audioBlob.size,
      type: audioBlob.type
    })
    
    // ğŸ¯ ì˜¤ë””ì˜¤ ë¸”ë¡­ì„ Whisper API í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    const convertedBlob = await convertAudioBlob(audioBlob)
    
    const formData = new FormData()
    
    // ğŸ¯ ì •í™•í•œ íŒŒì¼ í™•ì¥ì ë§¤í•‘
    const getFileExtension = (mimeType: string) => {
      const mimeToExtension: { [key: string]: string } = {
        'audio/webm': 'webm',
        'audio/webm;codecs=opus': 'webm',
        'audio/mp4': 'm4a',
        'audio/mp4;codecs=aac': 'm4a',
        'audio/wav': 'wav',
        'audio/ogg': 'ogg',
        'audio/ogg;codecs=opus': 'ogg',
        'audio/mp3': 'mp3',
        'audio/mpeg': 'mp3',
        'audio/mpga': 'mp3'
      }
      
      const extension = mimeToExtension[mimeType] || 'wav' // ê¸°ë³¸ê°’ì„ wavë¡œ ë³€ê²½
      console.log(`ğŸµ MIME type: ${mimeType} â†’ extension: ${extension}`)
      return extension
    }
    
    const fileExtension = getFileExtension(convertedBlob.type)
    const fileName = `audio.${fileExtension}`
    
    formData.append('audio', convertedBlob, fileName)
    formData.append('sessionId', currentSessionId)
    formData.append('language', lang)
    formData.append('model', 'whisper-1')
    formData.append('response_format', 'verbose_json')
    formData.append('temperature', '0')
    formData.append('prompt', 'This is a conversation or presentation. Focus on clear speech, proper grammar, and complete sentences. Pay attention to context and avoid typos.')
    formData.append('enableGrammarCheck', 'true')

    console.log('ğŸ“¤ Sending to Whisper API:', {
      fileName,
      fileSize: convertedBlob.size,
      fileType: convertedBlob.type,
      sessionId: currentSessionId,
      language: lang
    })

    const response = await fetch('/api/stt', {
      method: 'POST',
      body: formData,
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error('âŒ STT API Error:', errorText)
      throw new Error(`STT API failed: ${response.status} - ${errorText}`)
    }

    const result = await response.json()
    console.log('âœ… Whisper API response:', result)
    return result
  }, [lang, onError])

  // ğŸµ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
  const updateTranscript = useCallback((newTranscript: string, isPartial: boolean) => {
    setCurrentTranscript(prev => {
      // ğŸ†• ì¤‘ë³µ ë°©ì§€ ë° ë¬¸ë§¥ ë³´ì¡´
      const cleanNewTranscript = newTranscript.trim()
      if (!cleanNewTranscript) return prev
      
      // ì´ì „ í…ìŠ¤íŠ¸ì™€ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
      const prevWords = prev.split(' ').slice(-3) // ë§ˆì§€ë§‰ 3ê°œ ë‹¨ì–´
      const newWords = cleanNewTranscript.split(' ').slice(0, 3) // ì²˜ìŒ 3ê°œ ë‹¨ì–´
      
      // ğŸ†• ë‹¨ìˆœí•œ ì¶”ê°€ ë°©ì‹
      const combined = prev ? `${prev} ${cleanNewTranscript}` : cleanNewTranscript
      
      currentTranscriptRef.current = combined
      
      setTimeout(() => {
        onTranscriptUpdate(combined, isPartial)
      }, 0)
      
      return combined
    })
    
    console.log(`ğŸ“ Updated transcript (${isPartial ? 'partial' : 'final'}):`, newTranscript)
  }, [onTranscriptUpdate])

  // ğŸ¤ ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    if (isRecording) return

    // ğŸ†• sessionId í™•ì¸
    if (!sessionId || sessionId.trim() === '') {
      onError('Session ID is required')
      return
    }

    try {
      // ê¶Œí•œ í™•ì¸
      if (!hasPermission) {
        const granted = await requestPermission()
        if (!granted) return
      }

      // ğŸ¤ Web Speech APIë§Œ ì‚¬ìš© (MediaRecorder ë¹„í™œì„±í™”)
      if (webSpeechSupported) {
        setupWebSpeech()
        if (recognitionRef.current) {
          recognitionRef.current.start()
          console.log('ğŸ¤ Web Speech API recording started')
          setIsRecording(true)
          setStatus('Recording with Web Speech API...')
          setChunkCount(0)
          setCurrentTranscript('')
        } else {
          throw new Error('Web Speech API not available')
        }
      } else {
        throw new Error('Web Speech API not supported')
      }
      
      console.log('âœ… Web Speech API recording started')

    } catch (error) {
      console.error('âŒ Failed to start realtime recording:', error)
      setStatus('Failed to start')
      onError('Failed to start realtime recording')
    }
  }, [hasPermission, isRecording, requestPermission, processPrimaryQueue, processSecondaryQueue, currentTranscript, onTranscriptUpdate, onError])

  // ğŸ›‘ ë…¹ìŒ ì¤‘ì§€
  const stopRecording = useCallback(() => {
    // ğŸ¤ Web Speech API ì¤‘ì§€
    if (recognitionRef.current) {
      recognitionRef.current.stop()
      console.log('ğŸ¤ Web Speech API stopped')
    }

    // ğŸ¤ MediaRecorder ì¤‘ì§€ (ë°±ì—…ìš©)
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
      console.log('ğŸ›‘ MediaRecorder stopped')
    }

    setIsRecording(false)
    setStatus('Ready')
    console.log('ğŸ›‘ Recording stopped')
  }, [isRecording])

  // ğŸ§¹ ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      cleanup()
    }
  }, [cleanup])

  // ğŸµ AudioBufferë¥¼ WAV Blobìœ¼ë¡œ ë³€í™˜
  const audioBufferToWav = useCallback(async (audioBuffer: AudioBuffer): Promise<Blob> => {
    const length = audioBuffer.length
    const sampleRate = audioBuffer.sampleRate
    const channelData = audioBuffer.getChannelData(0)
    
    // WAV í—¤ë” ìƒì„±
    const buffer = new ArrayBuffer(44 + length * 2)
    const view = new DataView(buffer)
    
    // WAV íŒŒì¼ í—¤ë” ì‘ì„±
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + length * 2, true)
    writeString(8, 'WAVE')
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true)
    view.setUint16(20, 1, true)
    view.setUint16(22, 1, true)
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, sampleRate * 2, true)
    view.setUint16(32, 2, true)
    view.setUint16(34, 16, true)
    writeString(36, 'data')
    view.setUint32(40, length * 2, true)
    
    // ì˜¤ë””ì˜¤ ë°ì´í„° ì‘ì„±
    let offset = 44
    for (let i = 0; i < length; i++) {
      const sample = Math.max(-1, Math.min(1, channelData[i]))
      view.setInt16(offset, sample * 0x7FFF, true)
      offset += 2
    }
    
    return new Blob([buffer], { type: 'audio/wav' })
  }, [])

  // ğŸ¯ ì˜¤ë””ì˜¤ ë¸”ë¡­ì„ Whisper API í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  const convertAudioBlob = useCallback(async (audioBlob: Blob): Promise<Blob> => {
    console.log('ğŸ”„ Converting audio blob to Whisper-compatible format...')
    console.log('ğŸµ Original blob:', {
      size: audioBlob.size,
      type: audioBlob.type
    })
    
    // ì´ë¯¸ í˜¸í™˜ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸
    if (audioBlob.type === 'audio/webm' && !audioBlob.type.includes('codecs=')) {
      console.log('âœ… Already in compatible format')
      return audioBlob
    }
    
    // ğŸš« opus codecì´ í¬í•¨ëœ ê²½ìš° ì‹¤ì œ ë³€í™˜ ìˆ˜í–‰
    if (audioBlob.type.includes('codecs=opus')) {
      console.log('ğŸš« Opus codec detected, converting to WAV...')
      
      try {
        // ğŸµ AudioContextë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì˜¤ë””ì˜¤ ë³€í™˜
        const audioContext = new AudioContext({ sampleRate: 16000 })
        const arrayBuffer = await audioBlob.arrayBuffer()
        
        // ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë”©
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
        
        // ğŸ¯ WAV í˜•ì‹ìœ¼ë¡œ ì¸ì½”ë”©
        const wavBlob = await audioBufferToWav(audioBuffer)
        
        console.log('âœ… Converted to WAV format:', {
          originalSize: audioBlob.size,
          newSize: wavBlob.size,
          originalType: audioBlob.type,
          newType: wavBlob.type
        })
        
        return wavBlob
      } catch (error) {
        console.error('âŒ Audio conversion failed:', error)
        
        // ğŸš« ë³€í™˜ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ: ìƒˆë¡œìš´ MediaRecorderë¡œ ì¬ì¸ì½”ë”©
        console.log('ğŸ”„ Trying alternative re-encoding method...')
        
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: 16000,
              channelCount: 1
            }
          })
          
          const mediaRecorder = new MediaRecorder(stream, { 
            mimeType: 'audio/webm',
            audioBitsPerSecond: 128000
          })
          
          const chunks: Blob[] = []
          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              chunks.push(event.data)
            }
          }
          
          return new Promise((resolve, reject) => {
            mediaRecorder.onstop = () => {
              const newBlob = new Blob(chunks, { type: 'audio/webm' })
              console.log('âœ… Re-encoded to webm:', {
                size: newBlob.size,
                type: newBlob.type
              })
              stream.getTracks().forEach(track => track.stop())
              resolve(newBlob)
            }
            
            mediaRecorder.onerror = () => {
              stream.getTracks().forEach(track => track.stop())
              reject(new Error('Re-encoding failed'))
            }
            
            // ì§§ì€ ë…¹ìŒìœ¼ë¡œ ì¬ì¸ì½”ë”©
            mediaRecorder.start()
            setTimeout(() => mediaRecorder.stop(), 200)
          })
        } catch (reencodeError) {
          console.error('âŒ Re-encoding also failed:', reencodeError)
          // ìµœì¢… fallback: ì›ë³¸ ë°˜í™˜
          return audioBlob
        }
      }
    }
    
    // ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ë°˜í™˜
    return audioBlob
  }, [audioBufferToWav])

  // ğŸ¤ Web Speech API ì„¤ì •
  const [webSpeechSupported, setWebSpeechSupported] = useState(false)
  const [webSpeechActive, setWebSpeechActive] = useState(false)
  const [webSpeechTranscript, setWebSpeechTranscript] = useState('')
  const recognitionRef = useRef<any>(null)

  // ğŸ¤ Web Speech API íƒ€ì… ì •ì˜
  interface SpeechRecognitionEvent {
    resultIndex: number
    results: {
      [key: number]: {
        [key: number]: {
          transcript: string
        }
        isFinal: boolean
      }
      length: number
    }
  }

  interface SpeechRecognitionErrorEvent {
    error: string
  }

  // ğŸ¤ Web Speech API ì´ˆê¸°í™”
  useEffect(() => {
    if (typeof window !== 'undefined' && ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      setWebSpeechSupported(true)
      console.log('âœ… Web Speech API supported')
    } else {
      console.log('âŒ Web Speech API not supported')
    }
  }, [])

  // ğŸ¤ Web Speech API ì„¤ì •
  const setupWebSpeech = useCallback(() => {
    if (!webSpeechSupported) return

    try {
      const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
      const recognition = new SpeechRecognition()
      
      recognition.continuous = true
      recognition.interimResults = true
      // ğŸŒ ì„ íƒëœ ì–¸ì–´ë¡œ ì„¤ì • (í•œêµ­ì–´ ìš°ì„ )
      const webSpeechLanguages = secondaryLanguage && secondaryLanguage !== 'none'
        ? `${primaryLanguage},${secondaryLanguage}`
        : primaryLanguage
      recognition.lang = webSpeechLanguages
      console.log('ğŸŒ Web Speech API language set to:', webSpeechLanguages)
      recognition.maxAlternatives = 1
      
      recognition.onstart = () => {
        console.log('ğŸ¤ Web Speech API started')
        setWebSpeechActive(true)
      }
      
      recognition.onresult = (event: SpeechRecognitionEvent) => {
        let interimTranscript = ''
        let finalTranscript = ''
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }
        
        // ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
        const currentTranscript = finalTranscript + interimTranscript
        setWebSpeechTranscript(currentTranscript)
        
        // ìµœì¢… ê²°ê³¼ê°€ ìˆìœ¼ë©´ Whisper APIë¡œ ë³´ë‚´ê¸°
        if (finalTranscript.trim()) {
          console.log('ğŸ¤ Web Speech final result:', finalTranscript)
          // Whisper APIë¡œ ë³´ë‚´ê¸° (í…ìŠ¤íŠ¸ ê¸°ë°˜)
          sendToWhisperAPI(finalTranscript.trim())
        }
      }
      
      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        console.error('ğŸ¤ Web Speech API error:', event.error)
        setWebSpeechActive(false)
      }
      
      recognition.onend = () => {
        console.log('ğŸ¤ Web Speech API ended')
        setWebSpeechActive(false)
        // ìë™ ì¬ì‹œì‘
        if (isRecording) {
          setTimeout(() => {
            recognition.start()
          }, 100)
        }
      }
      
      recognitionRef.current = recognition
    } catch (error) {
      console.error('âŒ Web Speech API setup failed:', error)
    }
  }, [webSpeechSupported, lang, isRecording])

  // ğŸ¯ Whisper APIë¡œ í…ìŠ¤íŠ¸ ì „ì†¡ (Web Speech ê²°ê³¼)
  const sendToWhisperAPI = useCallback(async (text: string) => {
    if (!sessionIdRef.current) {
      console.error('âŒ No session ID available')
      return
    }

    try {
      console.log('ğŸ¯ Sending text to Whisper API:', text)
      
      const formData = new FormData()
      formData.append('text', text)
      formData.append('sessionId', sessionIdRef.current)
      // ğŸŒ ì„ íƒëœ ì–¸ì–´ë¡œ ì„¤ì •
      const apiLanguage = secondaryLanguage && secondaryLanguage !== 'none' ? 'auto' : primaryLanguage
      formData.append('language', apiLanguage)
      formData.append('model', 'whisper-1')
      formData.append('response_format', 'verbose_json')
      formData.append('temperature', '0')
      formData.append('prompt', 'This is a presentation transcription (English primary + Korean for emphasis) from Web Speech API that needs to be improved for presentation delivery and audience engagement while preserving the mixed language content.')
      formData.append('enableGrammarCheck', 'true')
      
      const response = await fetch('/api/stt-text', {
        method: 'POST',
        body: formData,
      })
      
      if (response.ok) {
        const result = await response.json()
        console.log('âœ… Whisper API text processing result:', result)
        
        if (result.transcript) {
          // íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸
          updateTranscript(result.transcript, false)
        }
      } else {
        console.error('âŒ Whisper API text processing failed')
      }
    } catch (error) {
      console.error('âŒ Error sending text to Whisper API:', error)
    }
  }, [lang, updateTranscript])

  if (!hasPermission) {
    return (
      <div className="flex flex-col gap-4 p-4 border rounded-lg">
        <div className="text-sm text-gray-600">{status}</div>
        <Button onClick={requestPermission} className="bg-blue-500 hover:bg-blue-600">
          ğŸ¤ Grant Microphone Permission
        </Button>
      </div>
    )
  }

  // ğŸŒ ì–¸ì–´ ì„ íƒê¸°ê°€ í‘œì‹œë˜ì–´ì•¼ í•˜ëŠ” ê²½ìš°
  if (showLanguageSelector) {
    return (
      <LanguageSelector
        primaryLanguage={primaryLanguage}
        secondaryLanguage={secondaryLanguage}
        onPrimaryLanguageChange={handlePrimaryLanguageChange}
        onSecondaryLanguageChange={handleSecondaryLanguageChange}
        onStart={handleStartRecording}
        isStarted={isRecording}
      />
    )
  }

  return (
    <div className="flex flex-col gap-4 p-4 border rounded-lg">
      <div className="flex items-center gap-4">
        <Button
          onClick={isRecording ? stopRecording : () => setShowLanguageSelector(true)}
          className={isRecording ? 'bg-red-500 hover:bg-red-600' : 'bg-blue-500 hover:bg-blue-600'}
        >
          {isRecording ? 'ğŸ›‘ Stop' : 'âš™ï¸ Settings'} Recording
        </Button>
        
        <div className="text-sm text-gray-600">
          {isRecording ? 'ğŸ”´ Recording...' : 'âšª Ready'}
          </div>
        </div>
      
      <div className="text-xs text-gray-500">
        <div>Status: {status}</div>
        <div>Chunks: {chunkCount}</div>
        <div>Processing: {isProcessingRef.current ? 'Yes' : 'No'}</div>
        <div>Queue: {processingQueueRef.current.length}</div>
        <div>Session ID: {sessionId.substring(0, 8)}...</div>
        <div>Web Speech: {webSpeechSupported ? (webSpeechActive ? 'ğŸŸ¢ Active' : 'âšª Ready') : 'âŒ Not Supported'}</div>
        <div>ğŸŒ Primary: {primaryLanguage}</div>
        <div>ğŸŒ Secondary: {secondaryLanguage === 'none' ? 'None' : secondaryLanguage}</div>
        </div>
      
      {webSpeechTranscript && (
        <div className="p-3 bg-blue-50 border border-blue-200 rounded mb-2">
          <div className="text-sm font-medium text-blue-800 mb-2">Web Speech (Live):</div>
          <div className="text-blue-900 text-sm">{webSpeechTranscript}</div>
        </div>
      )}
      
      {currentTranscript && (
        <div className="p-3 bg-yellow-50 border border-yellow-200 rounded">
          <div className="text-sm font-medium text-yellow-800 mb-2">Whisper API (Enhanced):</div>
          <div className="text-yellow-900 text-sm">{currentTranscript}</div>
        </div>
      )}
    </div>
  )
}
