'use client'

import React, { useState, useEffect, useRef, useCallback } from 'react'

// Web Speech API íƒ€ì… ì •ì˜
declare global {
  interface Window {
    webkitSpeechRecognition: any
    SpeechRecognition: any
  }
}

interface RealtimeSTTProps {
  sessionId: string
  onTranscriptUpdate: (transcript: string, isPartial: boolean) => void
  onError: (error: string) => void
  primaryLanguage?: string
  secondaryLanguage?: string
}

export function RealtimeSTT({ 
  sessionId, 
  onTranscriptUpdate, 
  onError, 
  primaryLanguage = 'en-US', 
  secondaryLanguage = 'ko-KR' 
}: RealtimeSTTProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [status, setStatus] = useState('Ready')
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [chunks, setChunks] = useState(0)
  const [duplicatesBlocked] = useState(0)
  const [autoReconnect] = useState('Scheduled')

  // Web Speech API ê´€ë ¨ refs
  const recognitionRef = useRef<any>(null)
  const isListeningRef = useRef(false)
  const reconnectTimerRef = useRef<NodeJS.Timeout | null>(null)

  // ğŸ¯ ì ì‘í˜• ë²„í¼ë§ ì‹œìŠ¤í…œ (Otter AI ìŠ¤íƒ€ì¼)
  const adaptiveBufferRef = useRef('')
  const confidenceScoresRef = useRef<Array<{text: string, confidence: number, timestamp: number}>>([])
  const lastProcessedRef = useRef<string>('')
  const bufferStartTimeRef = useRef<number>(0)
  const semanticTimerRef = useRef<NodeJS.Timeout | null>(null)
  
  // ì ì‘í˜• ì„¤ì • (ì‹¤ì‹œê°„ì„±ê³¼ ì •í™•ë„ ê· í˜•)
  const MIN_BUFFER_SIZE = 15        // ìµœì†Œ 15ì (ì§§ì€ ë¬¸ì¥ë„ ì²˜ë¦¬í•˜ë©´ì„œ ì •í™•ë„ í–¥ìƒ)
  const MAX_BUFFER_SIZE = 60        // ìµœëŒ€ 60ì (ì˜¤íƒ€ ê°ì†Œë¥¼ ìœ„í•´ ë” ì‘ê²Œ ì¡°ì •)
  const SEMANTIC_TIMEOUT = 600      // 0.6ì´ˆ í›„ ì²˜ë¦¬ (ë¹ ë¥¸ ì‘ë‹µìœ¼ë¡œ ì‹¤ì‹œê°„ì„± í–¥ìƒ)
  const OVERLAP_SIZE = 20           // 20ì ì˜¤ë²„ë© (ì ì ˆí•œ ì¤‘ë³µ ì œê±°)
  
  // ì¹¨ë¬µ ê°ì§€ë¥¼ ìœ„í•œ ì˜¤ë””ì˜¤ ë¶„ì„
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const silenceDetectionRef = useRef<NodeJS.Timeout | null>(null)
  const lastSpeechTimeRef = useRef<number>(Date.now())
  const SILENCE_THRESHOLD = -50 // dB ê¸°ì¤€ (ì¡°ì • ê°€ëŠ¥)
  const SILENCE_DURATION = 2000 // 2ì´ˆ ì¹¨ë¬µ ê°ì§€
  
  // ğŸ”„ 4ë¶„ë§ˆë‹¤ ìë™ ì¬ì—°ê²° ì‹œìŠ¤í…œ
  const lastReconnectTimeRef = useRef<number>(Date.now())
  const RECONNECT_INTERVAL = 4 * 60 * 1000 // 4ë¶„ (240ì´ˆ)

  // Web Speech API ì§€ì› í™•ì¸
  const isSupported = typeof window !== 'undefined' && 'webkitSpeechRecognition' in window

  // ì–¸ì–´ ì„¤ì •
  const [currentPrimaryLanguage, setCurrentPrimaryLanguage] = useState(primaryLanguage)
  const [currentSecondaryLanguage, setCurrentSecondaryLanguage] = useState(secondaryLanguage)

  // ğŸ¯ ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë° ì¹¨ë¬µ ê°ì§€
  const analyzeAudioLevel = useCallback(() => {
    if (!analyserRef.current) return false
    
    const bufferLength = analyserRef.current.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    analyserRef.current.getByteFrequencyData(dataArray)
    
    // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
    let sum = 0
    for (let i = 0; i < bufferLength; i++) {
      sum += dataArray[i]
    }
    const average = sum / bufferLength
    
    // dBë¡œ ë³€í™˜
    const decibels = 20 * Math.log10(average / 255)
    
    const isSpeaking = decibels > SILENCE_THRESHOLD
    if (isSpeaking) {
      lastSpeechTimeRef.current = Date.now()
    }
    
    return isSpeaking
  }, [])
  
  // ğŸ”„ 4ë¶„ë§ˆë‹¤ ë¶€ë“œëŸ¬ìš´ ìë™ ì¬ì—°ê²° íƒ€ì´ë¨¸
  // 4ë¶„ë§ˆë‹¤ ì˜ˆë°©ì  ì¬ì—°ê²° (5ë¶„ ì œí•œ ë°©ì§€) - ì¤‘ë³µ DB ì €ì¥ ë°©ì§€
  const startAutoReconnectTimer = useCallback(() => {
    if (reconnectTimerRef.current) {
      clearTimeout(reconnectTimerRef.current)
    }
    
    console.log(`ğŸ”„ Setting up 4-minute preventive reconnection timer (${RECONNECT_INTERVAL/1000}s)`)
    
    reconnectTimerRef.current = setTimeout(() => {
      console.log('ğŸ”„ 4ë¶„ ê²½ê³¼ - ì˜ˆë°©ì  ì¬ì—°ê²° ì‹œì‘ (5ë¶„ ì œí•œ ë°©ì§€)')
      
      // í˜„ì¬ ë²„í¼ì˜ ë‚´ìš©ì„ ë¨¼ì € ì²˜ë¦¬ (ì¤‘ë³µ DB ì €ì¥ ë°©ì§€)
      if (adaptiveBufferRef.current.trim()) {
        console.log('ğŸ“¦ Processing remaining buffer before preventive reconnection')
        // ê°•ì œ ì²˜ë¦¬í•˜ë˜ ìƒˆë¡œìš´ ì„¸ì…˜ì„ ì‹œì‘í•˜ì§€ ì•ŠìŒ
        processChunk(adaptiveBufferRef.current, true)
        adaptiveBufferRef.current = ''
      }
      
      // í˜„ì¬ ì¸ì‹ ì¤‘ì§€í•˜ê³  ì¦‰ì‹œ ì¬ì‹œì‘ (ì„¸ì…˜ ìœ ì§€)
      if (recognitionRef.current) {
        try {
          console.log('ğŸ”„ Stopping current recognition for preventive restart')
          recognitionRef.current.stop()
          // onend ì´ë²¤íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ ì¬ì‹œì‘ë¨ (ì„¸ì…˜ ìœ ì§€)
        } catch (error) {
          console.warn('ğŸ”„ Error stopping recognition for preventive restart:', error)
        }
      }

      // ë‹¤ìŒ ì¬ì—°ê²° íƒ€ì´ë¨¸ ì„¤ì •
      console.log('ğŸ”„ Setting up next 4-minute preventive reconnection timer')
      startAutoReconnectTimer()
    }, RECONNECT_INTERVAL)
    
    lastReconnectTimeRef.current = Date.now()
  }, [isRecording])

  // ğŸ¯ ì¹¨ë¬µ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ë” ì ê·¹ì ìœ¼ë¡œ)
  const startSilenceDetection = useCallback(() => {
    const checkSilence = () => {
      const isSpeaking = analyzeAudioLevel()
      const timeSinceLastSpeech = Date.now() - lastSpeechTimeRef.current
      
      // 2ì´ˆ ì´ìƒ ì¹¨ë¬µì´ë©´ í˜„ì¬ ë²„í¼ ê°•ì œ ì²˜ë¦¬
      if (!isSpeaking && timeSinceLastSpeech > SILENCE_DURATION) {
        console.log('ğŸ”‡ Silence detected, forcing chunk processing')
        if (adaptiveBufferRef.current.trim()) {
          processChunk(adaptiveBufferRef.current, true) // ê°•ì œ ì²˜ë¦¬ í”Œë˜ê·¸
          adaptiveBufferRef.current = ''
        }
        lastSpeechTimeRef.current = Date.now() // ë¦¬ì…‹í•˜ì—¬ ë°˜ë³µ ë°©ì§€
      }
      
      // ê³„ì† ëª¨ë‹ˆí„°ë§ (ë” ìì£¼ ì²´í¬)
      if (isRecording) {
        silenceDetectionRef.current = setTimeout(checkSilence, 50) // 50msë§ˆë‹¤ ì²´í¬ (100ms â†’ 50ms)
      }
    }
    
    checkSilence()
  }, [isRecording])
  
  // ğŸ¯ ë¬¸ì¥ ì™„ì„± ì²´í¬ (ê· í˜•ì¡íŒ ë²„ì „)
  const isCompleteSentence = (text: string): boolean => {
    const trimmed = text.trim()
    if (!trimmed) return false
    
    // ë¬¸ì¥ ë ë¶€í˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (ê°€ì¥ í™•ì‹¤í•œ ì‹ í˜¸)
    const hasEndPunctuation = /[.!?ã€‚ï¼ï¼Ÿ]/.test(trimmed.slice(-1))
    
    // ìµœì†Œ ê¸¸ì´ í™•ì¸ (ë” ë‚®ì¶¤)
    const hasMinimumLength = trimmed.length > 5
    
    // ìì—°ì–´ íŒ¨í„´ í™•ì¸
    const hasWordPattern = /\w/.test(trimmed)
    
    // ì‰¼í‘œë‚˜ ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹ì´ ìˆëŠ”ì§€ í™•ì¸
    const hasNaturalBreak = /[,ï¼Œã€]/.test(trimmed)
    
    // ê¸¸ì´ ê¸°ë°˜ ë¶„í•  (50ì ì´ìƒì´ë©´ ê°•ì œë¡œ ì™„ì„±ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼) - ë” ì ê·¹ì 
    const isLengthForced = trimmed.length > 50
    
    // ì‹œê°„ ê¸°ë°˜ ê°•ì œ ë¶„í•  (8ì´ˆ ì´ìƒì´ë©´ ê°•ì œë¡œ ì™„ì„±ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼) - ë” ì ê·¹ì 
    const timeSinceLastSpeech = Date.now() - lastSpeechTimeRef.current
    const isTimeForced = timeSinceLastSpeech > 8000 && trimmed.length > 20
    
    // ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ë¶„í•  (8ê°œ ë‹¨ì–´ ì´ìƒì´ë©´ ì™„ì„±ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼) - ë” ì ê·¹ì 
    const wordCount = trimmed.split(/\s+/).length
    const isWordCountForced = wordCount > 8
    
    // ë” ì ê·¹ì ì¸ ì¡°ê±´ë“¤
    return (hasEndPunctuation && hasMinimumLength) || 
           (hasNaturalBreak && trimmed.length > 15) || // ì‰¼í‘œ ê¸°ì¤€ë„ ë‚®ì¶¤
           isLengthForced ||
           isTimeForced ||
           isWordCountForced
  }

  // ğŸ¯ ë¬¸ì¥ ë¶„í•  ë° ì „ì†¡ (ë” ìœ ì—°í•œ ë²„ì „)
  const processCompleteSentences = (text: string): string => {
    const trimmed = text.trim()
    if (!trimmed) return ''
    
    // ë¬¸ì¥ ë ë¶€í˜¸ë¡œ ë¶„í• 
    const sentences = trimmed.split(/(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+/)
    
    if (sentences.length > 1) {
      // ì™„ì „í•œ ë¬¸ì¥ë“¤ì„ ì „ì†¡
      const completeSentences = sentences.slice(0, -1)
      completeSentences.forEach(sentence => {
        if (sentence.trim()) {
          console.log('âœ… Complete sentence detected:', sentence.trim())
          sendToSTTStream(sentence.trim())
        }
      })
      
      // ë‚¨ì€ ë¶€ë¶„ ë°˜í™˜
      return sentences[sentences.length - 1] || ''
    }
    
    // ê¸¸ì´ ê¸°ë°˜ ê°•ì œ ë¶„í•  (50ì ì´ìƒ) - ë” ì ê·¹ì 
    if (trimmed.length > 50) {
      // ì¤‘ê°„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
      const midPoint = Math.floor(trimmed.length / 2)
      const splitPoint = trimmed.lastIndexOf(' ', midPoint) || 
                        trimmed.lastIndexOf(',', midPoint) ||
                        trimmed.lastIndexOf('ï¼Œ', midPoint) ||
                        midPoint
      
      if (splitPoint > trimmed.length * 0.3) { // 30% ì´ìƒì—ì„œ ë¶„í•  - ë” ì ê·¹ì 
        const firstPart = trimmed.substring(0, splitPoint).trim()
        const secondPart = trimmed.substring(splitPoint).trim()
        
        if (firstPart) {
          console.log('ğŸ“ Length-based sentence split:', firstPart)
          sendToSTTStream(firstPart)
        }
        return secondPart
      }
    }
    
    // ì‹œê°„ ê¸°ë°˜ ê°•ì œ ë¶„í•  (8ì´ˆ ì´ìƒ) - ë” ì ê·¹ì 
    const timeSinceLastSpeech = Date.now() - lastSpeechTimeRef.current
    if (timeSinceLastSpeech > 8000 && trimmed.length > 20) {
      // ì¤‘ê°„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
      const midPoint = Math.floor(trimmed.length / 2)
      const splitPoint = trimmed.lastIndexOf(' ', midPoint) || 
                        trimmed.lastIndexOf(',', midPoint) ||
                        midPoint
      
      if (splitPoint > trimmed.length * 0.3) { // 30% ì´ìƒì—ì„œ ë¶„í• 
        const firstPart = trimmed.substring(0, splitPoint).trim()
        const secondPart = trimmed.substring(splitPoint).trim()
        
        if (firstPart) {
          console.log('â° Time-based sentence split:', firstPart)
          sendToSTTStream(firstPart)
        }
        return secondPart
      }
    }
    
    // ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ë¶„í•  (8ê°œ ë‹¨ì–´ ì´ìƒ) - ë” ì ê·¹ì 
    const wordCount = trimmed.split(/\s+/).length
    if (wordCount > 8) {
      const words = trimmed.split(/\s+/)
      const firstPart = words.slice(0, Math.floor(wordCount / 2)).join(' ')
      const secondPart = words.slice(Math.floor(wordCount / 2)).join(' ')
      
      if (firstPart) {
        console.log('ğŸ“ Word-count-based sentence split:', firstPart)
        sendToSTTStream(firstPart)
      }
      return secondPart
    }
    
    return trimmed
  }

  // ğŸ¯ 5ì´ˆ ì²­í¬ ì²˜ë¦¬ (ë©”ì¸ í) - ê°œì„ ëœ ë²„ì „
  const processChunk = async (text: string, forced = false) => {
    const cleanText = text.trim()
    if (!cleanText) return
    
    const timestamp = Date.now()
    console.log(`ğŸ“¦ Processing ${forced ? 'FORCED' : 'SCHEDULED'} chunk: "${cleanText.substring(0, 30)}..."`)
    
    // ë¬¸ì¥ ë¶„í•  ë° ì™„ì „í•œ ë¬¸ì¥ë“¤ ì „ì†¡
    const remainingText = processCompleteSentences(cleanText)
    
    // ë©”ì¸ íì— ì¶”ê°€
    // adaptiveBufferRef.current += ' ' + cleanText // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    
    // ì™„ì „í•œ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì „ì†¡, ì—†ìœ¼ë©´ ë²„í¼ì— ìœ ì§€
    if (remainingText !== cleanText) {
      // ì¼ë¶€ê°€ ì „ì†¡ë˜ì—ˆìœ¼ë¯€ë¡œ ë‚¨ì€ ë¶€ë¶„ë§Œ ë²„í¼ì— ìœ ì§€
      adaptiveBufferRef.current = remainingText
      lastProcessedRef.current = remainingText // ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
    }
    
    // í í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    // adaptiveBufferRef.current.length > 1000 // ë²„í¼ í¬ê¸° ì œí•œ ì œê±°
    
    // íƒ€ì´ë¨¸ ì‹œì‘
    if (bufferStartTimeRef.current === 0) {
      bufferStartTimeRef.current = timestamp
    }
  }
  
  // ğŸ¯ ì§€ì—° í ì²˜ë¦¬ (ë°±ì—… ë° í’ˆì§ˆ ê²€ì¦)
  const processDelayedChunk = async (text: string) => {
    const cleanText = text.trim()
    if (!cleanText) return
    
    const timestamp = Date.now()
    console.log(`â° Processing delayed chunk: "${cleanText.substring(0, 30)}..."`)
    
    // ì§€ì—° íì— ì¶”ê°€
    // delayedQueueRef.current.push({ text: cleanText, timestamp }) // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    
    // ë©”ì¸ íì™€ ë¹„êµí•˜ì—¬ ëˆ„ë½ëœ ë‚´ìš© í™•ì¸
    // const recentMainChunks = chunkQueueRef.current
    //   .filter(chunk => timestamp - chunk.timestamp < CHUNK_INTERVAL * 2)
    //   .map(chunk => chunk.text)
    
    // ë©”ì¸ íì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ ë°±ì—…ìœ¼ë¡œ ì „ì†¡
    // const isNewContent = !recentMainChunks.some(mainText => 
    //   mainText.includes(cleanText.substring(0, 20)) || 
    //   cleanText.includes(mainText.substring(0, 20))
    // )
    
    // if (isNewContent) {
    //   console.log('ğŸ”„ Delayed queue found missing content, sending as backup')
    //   await sendToSTTStream(cleanText)
    // }
    
    // í í¬ê¸° ì œí•œ
    // if (delayedQueueRef.current.length > 5) {
    //   delayedQueueRef.current = delayedQueueRef.current.slice(-3)
    // }
  }
  
  // ğŸ¯ STT ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
  const sendToSTTStream = async (text: string) => {
    try {
      console.log('ğŸ“¡ Sending to STT stream:', text.substring(0, 50) + '...')
      
      const response = await fetch('/api/stt-stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          type: 'transcript',
          sessionId: sessionId,
          transcript: text,
          isPartial: false
        }),
      })

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const result = await response.json()
      console.log('âœ… STT stream response:', result)
      
      // ìƒíƒœ ì—…ë°ì´íŠ¸
      setChunks(prev => prev + 1)
      if (result.success) {
        onTranscriptUpdate(text, false)
        setCurrentTranscript(text)
      }
      
    } catch (error) {
      console.error('âŒ STT stream failed:', error)
      onError(`STT processing failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
    }
  }

  // ğŸ¯ ì ì‘í˜• ë²„í¼ë§ íƒ€ì´ë¨¸ ì‹œì‘
  const startAdaptiveTimer = useCallback(() => {
    // ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  íƒ€ì´ë¨¸
    const scheduleSemanticChunk = () => {
      // ë²„í¼ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì²˜ë¦¬
      if (adaptiveBufferRef.current.trim()) {
        processChunk(adaptiveBufferRef.current)
        adaptiveBufferRef.current = ''
        lastProcessedRef.current = '' // ë²„í¼ê°€ ë¹„ì›Œì§€ë©´ ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë„ ì´ˆê¸°í™”
        bufferStartTimeRef.current = 0 // íƒ€ì´ë¨¸ ì‹œì‘ ì‹œê°„ ì´ˆê¸°í™”
      }
      
      // íƒ€ì´ë¨¸ ì¬ì„¤ì •
      if (isRecording) {
        const currentTime = Date.now()
        const elapsed = currentTime - bufferStartTimeRef.current
        const remainingTime = SEMANTIC_TIMEOUT - elapsed
        
        if (remainingTime > 0) {
          semanticTimerRef.current = setTimeout(scheduleSemanticChunk, remainingTime)
        } else {
          semanticTimerRef.current = setTimeout(scheduleSemanticChunk, SEMANTIC_TIMEOUT)
        }
      }
    }
    
    // íƒ€ì´ë¨¸ ì‹œì‘
    semanticTimerRef.current = setTimeout(scheduleSemanticChunk, SEMANTIC_TIMEOUT)
    
    console.log('â° Started adaptive semantic timer: 3s semantic chunks')
  }, [isRecording])

  // ğŸ¯ ì ì‘í˜• ë²„í¼ë§ ë¡œì§ (ë¬¸ë§¥ ë³´ì¡´ ìš°ì„ )
  const shouldProcessBuffer = useCallback((text: string): boolean => {
    const bufferLength = adaptiveBufferRef.current.length
    const timeSinceStart = Date.now() - bufferStartTimeRef.current
    
    // 1. ìµœì†Œ ë²„í¼ í¬ê¸° í™•ì¸
    if (bufferLength < MIN_BUFFER_SIZE) {
      return false
    }
    
    // 2. ë¬¸ì¥ ì™„ì„±ë„ í™•ì¸ (ê°€ì¥ ìš°ì„  - ë¬¸ë§¥ ë³´ì¡´)
    if (isCompleteSentence(adaptiveBufferRef.current)) {
      console.log('âœ… Complete sentence detected - processing buffer')
      return true
    }
    
    // 3. ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹ ê°ì§€ (ë¬¸ë§¥ ë³´ì¡´)
    if (hasNaturalBreak(adaptiveBufferRef.current)) {
      console.log('âœ… Natural break detected - processing buffer')
      return true
    }
    
    // 4. ìµœëŒ€ ë²„í¼ í¬ê¸° í™•ì¸ (ê°•ì œ ë¶„í•  ë°©ì§€)
    if (bufferLength > MAX_BUFFER_SIZE) {
      console.log('ğŸ“¦ Buffer full - processing buffer')
      return true
    }
    
    // 5. ì‹œê°„ ê¸°ë°˜ ì²˜ë¦¬ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    if (timeSinceStart > SEMANTIC_TIMEOUT && bufferLength > 20) {
      console.log('â° Semantic timeout - processing buffer')
      return true
    }
    
    return false
  }, [])

  // ğŸ¯ ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹ ê°ì§€ (ë¬¸ë§¥ ë³´ì¡´)
  const hasNaturalBreak = useCallback((text: string): boolean => {
    const trimmed = text.trim()
    
    // 1. ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ëŠ” ê²½ìš°
    if (/[.!?ã€‚ï¼ï¼Ÿ]/.test(trimmed.slice(-1))) {
      return true
    }
    
    // 2. ì‰¼í‘œ + ì¶©ë¶„í•œ ê¸¸ì´ (ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹)
    if (trimmed.includes(',') && trimmed.length > 25) {
      return true
    }
    
    // 3. ì—°ê²°ì‚¬ë‚˜ ì „ì¹˜ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš° (ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸°)
    const endingWords = ['and', 'or', 'but', 'so', 'because', 'when', 'if', 'that', 'which', 'who', 'what', 'where', 'why', 'how']
    const lastWord = trimmed.split(/\s+/).pop()?.toLowerCase()
    if (endingWords.includes(lastWord || '')) {
      return false // ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸°
    }
    
    // 4. ê´€ì‚¬ë‚˜ ì „ì¹˜ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° (ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸°)
    const startingWords = ['a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below']
    const firstWord = trimmed.split(/\s+/)[0]?.toLowerCase()
    if (startingWords.includes(firstWord || '')) {
      return false // ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸°
    }
    
    // 5. ì¶©ë¶„í•œ ê¸¸ì´ + ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ ê²½ê³„
    if (trimmed.length > 30 && !trimmed.endsWith(' ')) {
      return true
    }
    
    return false
  }, [])

  // ğŸ¯ ì ì‘í˜• í…ìŠ¤íŠ¸ ì¶”ê°€
  const addToAdaptiveBuffer = useCallback((text: string) => {
    // ë²„í¼ ì‹œì‘ ì‹œê°„ ì„¤ì •
    if (bufferStartTimeRef.current === 0) {
      bufferStartTimeRef.current = Date.now()
    }
    
    // í…ìŠ¤íŠ¸ ì¶”ê°€
    adaptiveBufferRef.current += ' ' + text
    adaptiveBufferRef.current = adaptiveBufferRef.current.trim()
    
    // ğŸš¨ ê°•ì œ ì²­í¬ ë¶„í• : ë„ˆë¬´ í° ë©ì–´ë¦¬ ë°©ì§€
    if (adaptiveBufferRef.current.length > MAX_BUFFER_SIZE * 1.5) {
      console.log(`ğŸš¨ Buffer too large (${adaptiveBufferRef.current.length} chars) - forcing chunk split`)
      
      // ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
      const sentences = adaptiveBufferRef.current.split(/[.!?ã€‚ï¼ï¼Ÿ]/)
      if (sentences.length > 1) {
        // ì²« ë²ˆì§¸ ì™„ì „í•œ ë¬¸ì¥ë§Œ ì²˜ë¦¬
        const firstSentence = sentences[0].trim() + '.'
        if (firstSentence.length > MIN_BUFFER_SIZE) {
          adaptiveBufferRef.current = firstSentence
          processAdaptiveChunk()
          
          // ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ë¥¼ ìƒˆ ë²„í¼ì— ì¶”ê°€
          const remainingText = sentences.slice(1).join('. ').trim()
          if (remainingText.length > MIN_BUFFER_SIZE) {
            adaptiveBufferRef.current = remainingText
            bufferStartTimeRef.current = Date.now()
          }
          return
        }
      }
      
      // ë¬¸ì¥ ë¶„í• ì´ ì•ˆë˜ë©´ ê°•ì œë¡œ MAX_BUFFER_SIZEë¡œ ìë¥´ê¸°
      const forcedChunk = adaptiveBufferRef.current.substring(0, MAX_BUFFER_SIZE)
      adaptiveBufferRef.current = forcedChunk
      processAdaptiveChunk()
      
      // ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ë¥¼ ìƒˆ ë²„í¼ì— ì¶”ê°€
      const remainingText = adaptiveBufferRef.current.substring(MAX_BUFFER_SIZE).trim()
      if (remainingText.length > MIN_BUFFER_SIZE) {
        adaptiveBufferRef.current = remainingText
        bufferStartTimeRef.current = Date.now()
      }
      return
    }
    
    // í…ìŠ¤íŠ¸ ê¸°ë¡
    confidenceScoresRef.current.push({
      text: text,
      confidence: 0.8, // ê¸°ë³¸ê°’
      timestamp: Date.now()
    })
    
    console.log(`ğŸ“¦ Added to adaptive buffer: "${text}"`)
    console.log(`ğŸ“¦ Buffer length: ${adaptiveBufferRef.current.length} chars`)
    
    // ìŒì„± í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
    lastSpeechTimeRef.current = Date.now()
    
    // ì‹¤ì‹œê°„ í”¼ë“œë°± ì—…ë°ì´íŠ¸
    onTranscriptUpdate(adaptiveBufferRef.current, true)
    
    // ì ì‘í˜• ì²˜ë¦¬ ì¡°ê±´ í™•ì¸
    if (shouldProcessBuffer(text)) {
      processAdaptiveChunk()
    }
  }, [shouldProcessBuffer])

  // ğŸ¯ ì ì‘í˜• ì²­í¬ ì²˜ë¦¬
  const processAdaptiveChunk = useCallback(async () => {
    const text = adaptiveBufferRef.current.trim()
    if (!text) return
    
    console.log(`ğŸ¯ Processing adaptive chunk: "${text.substring(0, 50)}..."`)
    
    try {
      // STT ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
      await sendToSTTStream(text)
      
      // ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ë¶€ë¶„ ìœ ì§€
      const overlapText = text.slice(-OVERLAP_SIZE)
      adaptiveBufferRef.current = overlapText
      
      // ë²„í¼ ìƒíƒœ ì´ˆê¸°í™”
      bufferStartTimeRef.current = Date.now()
      confidenceScoresRef.current = []
      lastProcessedRef.current = text
      
      console.log(`âœ… Adaptive chunk processed, keeping overlap: "${overlapText}"`)
      
    } catch (error) {
      console.error('âŒ Adaptive chunk processing failed:', error)
      // ì—ëŸ¬ ì‹œ ë²„í¼ ìœ ì§€
    }
  }, [])

  // ğŸ¯ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
  const initializeAudioContext = useCallback(async () => {
    try {
      // ë§ˆì´í¬ ì ‘ê·¼
      mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true })
      
      // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      analyserRef.current = audioContextRef.current.createAnalyser()
      
      const source = audioContextRef.current.createMediaStreamSource(mediaStreamRef.current)
      source.connect(analyserRef.current)
      
      analyserRef.current.fftSize = 256
      analyserRef.current.smoothingTimeConstant = 0.8
      
      console.log('ğŸ”Š Audio context initialized for silence detection')
      return true
    } catch (error) {
      console.error('âŒ Audio context initialization failed:', error)
      return false
    }
  }, [])

  // Web Speech API ì´ˆê¸°í™”
  const initializeSpeechRecognition = useCallback(() => {
    if (!isSupported) {
      onError('Speech recognition is not supported in this browser')
      return null
    }

    const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition
    const recognition = new SpeechRecognition()

    recognition.continuous = true
    recognition.interimResults = true
    recognition.lang = currentPrimaryLanguage

    recognition.onstart = () => {
      console.log('ğŸ¤ Speech recognition started')
      setIsRecording(true)
      setStatus('Recording with 5s Chunk System...')
      isListeningRef.current = true
      
      // ğŸ”„ 4ë¶„ë§ˆë‹¤ ìë™ ì¬ì—°ê²° íƒ€ì´ë¨¸ ì‹œì‘
      startAutoReconnectTimer()
    }

    recognition.onresult = (event: any) => {
      let finalTranscript = ''
      let interimTranscript = ''

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          finalTranscript += transcript
        } else {
          interimTranscript += transcript
        }
      }

      // ìµœì¢… ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì²­í¬ ë²„í¼ì— ì¶”ê°€
      if (finalTranscript.trim()) {
        addToAdaptiveBuffer(finalTranscript)
      }

      // ì„ì‹œ ê²°ê³¼ë„ í‘œì‹œ (ì‹¤ì‹œê°„ í”¼ë“œë°±)
      if (interimTranscript.trim()) {
        const fullBuffer = (adaptiveBufferRef.current + ' ' + interimTranscript).trim()
        console.log('ï¿½ï¿½ Interim transcript:', interimTranscript)
        onTranscriptUpdate(fullBuffer, true)
      }
    }

    recognition.onerror = (event: any) => {
      console.error('âŒ Speech recognition error:', event.error)
      
      // no-speech ì—ëŸ¬ëŠ” ì™„ì „íˆ ë¬´ì‹œí•˜ê³  ì¡°ìš©íˆ ì¬ì‹œì‘ (ì´ì „ ì½”ë“œì™€ ë™ì¼)
      if (event.error === 'no-speech') {
        console.log('ğŸ”‡ No speech detected, silently restarting...')
        setTimeout(() => {
          if (isRecording) {
            try {
              recognition.start()
              isListeningRef.current = true
              console.log('âœ… Speech recognition silently restarted')
            } catch (error) {
              console.log('ğŸ”„ Silent restart failed, retrying...')
              // ì¬ì‹œë„
              setTimeout(() => {
                if (isRecording) {
                  try {
                    recognition.start()
                    isListeningRef.current = true
                    console.log('âœ… Speech recognition restarted on retry')
                  } catch (retryError) {
                    console.log('ğŸ”„ Silent restart retry failed, continuing...')
                  }
                }
              }, 50) // 50ms í›„ ì¬ì‹œë„ (ë§¤ìš° ë¹ ë¥´ê²Œ)
            }
          }
        }, 50) // 50ms í›„ ì¬ì‹œì‘ (ë§¤ìš° ë¹ ë¥´ê²Œ)
        return // ì—ëŸ¬ë¥¼ í‘œì‹œí•˜ì§€ ì•Šê³  ì¡°ìš©íˆ ì²˜ë¦¬
      } else if (event.error === 'network') {
        // ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ëŠ” 4ë¶„ íƒ€ì„ì•„ì›ƒì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        console.log('ğŸŒ Network error detected, attempting restart...')
        setTimeout(() => {
          try {
            recognition.start()
            isListeningRef.current = true
            console.log('âœ… Speech recognition restarted after network error')
          } catch (error) {
            console.log('ğŸ”„ Network error restart failed, will retry...')
            // ì¬ì‹œë„
            setTimeout(() => {
              try {
                recognition.start()
                isListeningRef.current = true
                console.log('âœ… Speech recognition restarted on network error retry')
              } catch (retryError) {
                console.log('ğŸ”„ Network error restart retry failed, continuing...')
              }
            }, 1000)
          }
        }, 1000)
        return // ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ë„ ì¡°ìš©íˆ ì²˜ë¦¬
      } else {
        // ë‹¤ë¥¸ ì—ëŸ¬ë“¤ë§Œ ì‚¬ìš©ìì—ê²Œ í‘œì‹œ
        console.log(`âŒ Setting isRecording to false due to error: ${event.error}`)
        onError(`Speech recognition error: ${event.error}`)
        setIsRecording(false)
        setStatus('Speech recognition error')
      }
    }

    recognition.onend = () => {
      console.log('ğŸ›‘ Speech recognition ended')
      isListeningRef.current = false
      
      // ì˜ˆë°©ì  ì¬ì—°ê²°ê³¼ ì¼ì¹˜í•˜ëŠ” ìë™ ì¬ì‹œì‘
      if (isRecording) {
        console.log('ğŸ”„ Restarting speech recognition after end event...')
        setTimeout(() => {
          if (isRecording) {
            try {
              recognition.start()
              isListeningRef.current = true
              console.log('âœ… Speech recognition restarted successfully after end event')
            } catch (error) {
              console.log('ğŸ”„ Speech recognition restart failed after end event, will retry...')
              // ì¬ì‹œì‘ ì‹¤íŒ¨ì‹œ ë‹¤ì‹œ ì‹œë„
              setTimeout(() => {
                if (isRecording) {
                  try {
                    recognition.start()
                    isListeningRef.current = true
                    console.log('âœ… Speech recognition restarted on retry after end event')
                  } catch (retryError) {
                    console.log('ğŸ”„ Second retry failed after end event, trying again...')
                    // ë‘ ë²ˆì§¸ ì¬ì‹œë„ë„ ì‹¤íŒ¨í•˜ë©´ ë‹¤ì‹œ ì‹œë„
                    setTimeout(() => {
                      if (isRecording) {
                        try {
                          recognition.start()
                          isListeningRef.current = true
                          console.log('âœ… Speech recognition restarted on third try after end event')
                        } catch (thirdRetryError) {
                          console.error('âŒ Speech recognition restart failed after third retry')
                          console.log('âŒ Setting isRecording to false due to restart failure')
                          setIsRecording(false)
                          setStatus('Speech recognition restart failed')
                        }
                      }
                    }, 500) // 0.5ì´ˆ í›„ ì„¸ ë²ˆì§¸ ì‹œë„
                  }
                }
              }, 500) // 0.5ì´ˆ í›„ ë‘ ë²ˆì§¸ ì‹œë„
            }
          }
        }, 200) // 0.2ì´ˆ í›„ ì¬ì‹œì‘ (ì˜ˆë°©ì  ì¬ì—°ê²°ê³¼ ì¼ì¹˜)
      } else {
        console.log('ğŸ›‘ Speech recognition ended and isRecording is false - not restarting')
        setStatus('Ready')
      }
    }

    return recognition
  }, [isSupported, currentPrimaryLanguage, onError, onTranscriptUpdate, isRecording])

  // ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    console.log('ğŸ¤ Starting 5-second chunk-based speech recognition...')
    
    if (!isSupported) {
      onError('Speech recognition is not supported in this browser')
      return
    }

    // ë²„í¼ ë° í ì´ˆê¸°í™”
    adaptiveBufferRef.current = ''
    confidenceScoresRef.current = []
    lastProcessedRef.current = ''
    bufferStartTimeRef.current = 0
    // sentenceBufferRef.current = '' // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    // chunkBufferRef.current = '' // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    // processedSentencesRef.current.clear() // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    // chunkQueueRef.current = [] // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    // delayedQueueRef.current = [] // ì´ì œ ë²„í¼ì— ì§ì ‘ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    lastSpeechTimeRef.current = Date.now()

    // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
    const audioInitialized = await initializeAudioContext()
    if (!audioInitialized) {
      console.warn('âš ï¸ Audio context initialization failed, continuing without silence detection')
    }

    const recognition = initializeSpeechRecognition()
    if (recognition) {
      recognitionRef.current = recognition
      recognition.start()
      
      // ì ì‘í˜• íƒ€ì´ë¨¸ ì‹œì‘
      startAdaptiveTimer()
      
      // ì¹¨ë¬µ ê°ì§€ ì‹œì‘ (ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ê°€ ì´ˆê¸°í™”ëœ ê²½ìš°ë§Œ)
      if (audioInitialized) {
        startSilenceDetection()
      }
      
      // STT ìŠ¤íŠ¸ë¦¼ ì„¸ì…˜ ì‹œì‘ ì•Œë¦¼
      try {
        await fetch('/api/stt-stream', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ type: 'start', sessionId: sessionId })
        })
        console.log('âœ… STT stream session started')
      } catch (error) {
        console.error('âŒ Failed to start STT stream session:', error)
      }
    }
  }, [isSupported, initializeSpeechRecognition, initializeAudioContext, startAdaptiveTimer, startSilenceDetection, onError, sessionId])

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = useCallback(async () => {
    console.log('ğŸ›‘ Stopping chunk-based speech recognition...')
    
    if (recognitionRef.current) {
      recognitionRef.current.stop()
      recognitionRef.current = null
    }
    
    isListeningRef.current = false
    console.log('ğŸ›‘ Setting isRecording to false in stopRecording')
    setIsRecording(false)
    setStatus('Processing remaining chunks...')
    
    // ëª¨ë“  íƒ€ì´ë¨¸ ì •ë¦¬
    if (reconnectTimerRef.current) {
      clearTimeout(reconnectTimerRef.current)
      reconnectTimerRef.current = null
    }
    
    if (semanticTimerRef.current) {
      clearTimeout(semanticTimerRef.current)
      semanticTimerRef.current = null
    }
    
    if (silenceDetectionRef.current) {
      clearTimeout(silenceDetectionRef.current)
      silenceDetectionRef.current = null
    }
    
    // ë‚¨ì€ ì²­í¬ ë²„í¼ ì²˜ë¦¬
    if (adaptiveBufferRef.current.trim()) {
      console.log('ğŸ“¦ Processing final chunk buffer')
      await processChunk(adaptiveBufferRef.current, true)
      adaptiveBufferRef.current = ''
    }
    
    // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop())
      mediaStreamRef.current = null
    }
    
    // STT ìŠ¤íŠ¸ë¦¼ ì„¸ì…˜ ì¢…ë£Œ ì•Œë¦¼
    try {
      await fetch('/api/stt-stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ type: 'end', sessionId: sessionId })
      })
      console.log('âœ… STT stream session ended')
    } catch (error) {
      console.error('âŒ Failed to end STT stream session:', error)
    }
    
    setStatus('Ready')
    setCurrentTranscript('')
  }, [sessionId])

  // ì–¸ì–´ ë³€ê²½ ì²˜ë¦¬
  const handlePrimaryLanguageChange = (language: string) => {
    setCurrentPrimaryLanguage(language)
    if (isRecording && recognitionRef.current) {
      recognitionRef.current.stop()
    }
  }

  const handleSecondaryLanguageChange = (language: string) => {
    setCurrentSecondaryLanguage(language)
  }

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      
      // ëª¨ë“  íƒ€ì´ë¨¸ ì •ë¦¬
      const timers = [
        reconnectTimerRef.current,
        semanticTimerRef.current,
        silenceDetectionRef.current
      ]
      
      timers.forEach(timer => {
        if (timer) clearTimeout(timer)
      })
      
      // ì˜¤ë””ì˜¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close()
      }
      
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop())
      }
    }
  }, [])

  // ìƒíƒœ í…ìŠ¤íŠ¸ ìƒì„±
  const getStatusText = () => {
    if (!isSupported) return 'Speech recognition not supported'
    return status
  }

  return (
    <div className="space-y-4">
      {/* ìƒíƒœ ì •ë³´ */}
      <div className="text-sm text-gray-600 space-y-1">
        <div>Status: {getStatusText()}</div>
        <div>Chunks Sent: {chunks}</div>
        <div>Adaptive Buffer: {adaptiveBufferRef.current.length} chars</div>
        <div>Confidence Scores: {confidenceScoresRef.current.length}</div>
        <div>Session ID: {sessionId}</div>
        <div>Primary: {currentPrimaryLanguage}</div>
        <div>Secondary: {currentSecondaryLanguage}</div>
        <div>Duplicates Blocked: {duplicatesBlocked}</div>
        <div>Auto-reconnect: {autoReconnect}</div>
        <div>Chunk Buffer: &quot;{adaptiveBufferRef.current.substring(0, 50)}...&quot;</div>
        <div>Last Processed: &quot;{lastProcessedRef.current.substring(0, 30)}...&quot;</div>
        <div>Last Speech: {Math.round((Date.now() - lastSpeechTimeRef.current) / 1000)}s ago</div>
      </div>

      {/* ì–¸ì–´ ì„¤ì • */}
      <div className="space-y-4">
        <h3 className="text-lg font-semibold">Presentation Language Setup</h3>
        
        <div className="space-y-2">
          <label className="block text-sm font-medium">
            Main Presentation Language
          </label>
          <select
            value={currentPrimaryLanguage}
            onChange={(e) => handlePrimaryLanguageChange(e.target.value)}
            className="w-full p-2 border rounded-md"
            disabled={isRecording}
          >
            <option value="en-US">English (English)</option>
            <option value="ko-KR">Korean (í•œêµ­ì–´)</option>
            <option value="zh-CN">Chinese (ä¸­æ–‡)</option>
            <option value="ja-JP">Japanese (æ—¥æœ¬èª)</option>
            <option value="es-ES">Spanish (EspaÃ±ol)</option>
            <option value="fr-FR">French (FranÃ§ais)</option>
            <option value="de-DE">German (Deutsch)</option>
            <option value="it-IT">Italian (Italiano)</option>
            <option value="pt-BR">Portuguese (PortuguÃªs)</option>
            <option value="ru-RU">Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)</option>
            <option value="hi-IN">Hindi (à¤¹à¤¿à¤¨à¥à¤¦à¥€)</option>
            <option value="ar-SA">Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)</option>
          </select>
        </div>

        <div className="space-y-2">
          <label className="block text-sm font-medium">
            Emphasis Language (for Special Points)
          </label>
          <select
            value={currentSecondaryLanguage}
            onChange={(e) => handleSecondaryLanguageChange(e.target.value)}
            className="w-full p-2 border rounded-md"
            disabled={isRecording}
          >
            <option value="ko-KR">Korean (í•œêµ­ì–´)</option>
            <option value="en-US">English (English)</option>
            <option value="zh-CN">Chinese (ä¸­æ–‡)</option>
            <option value="ja-JP">Japanese (æ—¥æœ¬èª)</option>
            <option value="es-ES">Spanish (EspaÃ±ol)</option>
            <option value="fr-FR">French (FranÃ§ais)</option>
            <option value="de-DE">German (Deutsch)</option>
            <option value="it-IT">Italian (Italiano)</option>
            <option value="pt-BR">Portuguese (PortuguÃªs)</option>
            <option value="ru-RU">Russian (Ğ ÑƒÑÑĞºĞ¸Ğ¹)</option>
            <option value="hi-IN">Hindi (à¤¹à¤¿à¤¨à¥à¤¦à¥€)</option>
            <option value="ar-SA">Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)</option>
          </select>
        </div>

        <div className="text-sm text-gray-600">
          <div>Main: {currentPrimaryLanguage}</div>
          <div>Emphasis: {currentSecondaryLanguage}</div>
          <div>Speech Recognition: {currentPrimaryLanguage}</div>
        </div>
      </div>

      {/* ì œì–´ ë²„íŠ¼ */}
      <div className="flex space-x-4">
        {!isRecording ? (
          <button
            onClick={startRecording}
            disabled={!isSupported}
            className="px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 disabled:bg-gray-400"
          >
            {!isSupported ? 'Speech Recognition Not Supported' : 'Start Presenting'}
          </button>
        ) : (
          <button
            onClick={stopRecording}
            className="px-4 py-2 bg-red-500 text-white rounded-md hover:bg-red-600 flex items-center space-x-2"
          >
            <div className="w-3 h-3 bg-green-500 rounded-full animate-pulse"></div>
            <span>Adaptive Recording...</span>
          </button>
        )}
      </div>

      {/* í˜„ì¬ ì „ì‚¬ ê²°ê³¼ */}
      {currentTranscript && (
        <div className="mt-4 p-4 bg-gray-100 rounded-md">
          <div className="text-sm text-gray-500">
            {new Date().toLocaleTimeString()}
          </div>
          <div className="mt-1">{currentTranscript}</div>
        </div>
      )}
    </div>
  )
}
