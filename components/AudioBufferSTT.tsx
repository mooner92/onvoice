import React, { useState, useRef, useCallback, useEffect } from 'react'
import { Button } from './ui/button'

interface AudioBufferSTTProps {
  sessionId: string
  onTranscriptUpdate: (transcript: string, isPartial: boolean) => void
  language?: string
}

interface AudioChunk {
  id: string
  audioBlob: Blob
  timestamp: number
  duration: number
  isPartial: boolean
}

interface ProcessedTranscript {
  id: string
  text: string
  confidence: number
  startTime: number
  endTime: number
  isComplete: boolean
}

export default function AudioBufferSTT({ sessionId, onTranscriptUpdate, language = 'en-US' }: AudioBufferSTTProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [currentTranscript, setCurrentTranscript] = useState('')
  
  // Audio Buffer ê´€ë ¨ ìƒíƒœ
  const audioBuffer = useRef<AudioChunk[]>([])
  const processedTranscripts = useRef<ProcessedTranscript[]>([])
  const mediaRecorder = useRef<MediaRecorder | null>(null)
  const audioContext = useRef<AudioContext | null>(null)
  const analyser = useRef<AnalyserNode | null>(null)
  const microphone = useRef<MediaStreamAudioSourceNode | null>(null)
  
  // ì²­í‚¹ ì„¤ì •
  const CHUNK_INTERVAL = 2000 // 2ì´ˆë§ˆë‹¤ ì²­í¬ ìƒì„±
  const BUFFER_DURATION = 8000 // 8ì´ˆ ë²„í¼ ìœ ì§€
  const SILENCE_THRESHOLD = -50 // dB
  const SILENCE_DURATION = 1500 // 1.5ì´ˆ ì¹¨ë¬µ ê°ì§€
  const OVERLAP_DURATION = 1000 // 1ì´ˆ ì˜¤ë²„ë©
  
  // ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„
  const silenceStartTime = useRef<number | null>(null)
  const lastSpeechTime = useRef<number>(Date.now())
  const isSpeaking = useRef<boolean>(false)

  // ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: ë¬¸ë§¥ ê¸°ë°˜ ì²­í¬ ë¶„í• 
  const shouldCreateChunk = useCallback((currentTime: number): boolean => {
    const timeSinceLastSpeech = currentTime - lastSpeechTime.current
    const bufferSize = audioBuffer.current.reduce((total, chunk) => total + chunk.duration, 0)
    
    // 1. ì¹¨ë¬µ ê°ì§€ (ë¬¸ì¥ ë)
    if (timeSinceLastSpeech > SILENCE_DURATION && isSpeaking.current) {
      console.log('ğŸ”‡ Silence detected - creating chunk')
      return true
    }
    
    // 2. ë²„í¼ í¬ê¸° ì œí•œ
    if (bufferSize > BUFFER_DURATION) {
      console.log('ğŸ“¦ Buffer full - creating chunk')
      return true
    }
    
    // 3. ì •ê¸°ì  ì²­í‚¹ (ê¸´ ë¬¸ì¥ ëŒ€ë¹„)
    if (timeSinceLastSpeech > CHUNK_INTERVAL && bufferSize > 3000) {
      console.log('â° Regular chunking - creating chunk')
      return true
    }
    
    return false
  }, [])

  // ğŸµ ì‹¤ì‹œê°„ ìŒì„± ë ˆë²¨ ë¶„ì„
  const analyzeAudioLevel = useCallback(() => {
    if (!analyser.current) return

    const dataArray = new Uint8Array(analyser.current.frequencyBinCount)
    analyser.current.getByteFrequencyData(dataArray)
    
    // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
    const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length
    const db = 20 * Math.log10(average / 255)
    
    const currentTime = Date.now()
    
    if (db > SILENCE_THRESHOLD) {
      // ìŒì„± ê°ì§€
      if (!isSpeaking.current) {
        console.log('ğŸ¤ Speech started')
        isSpeaking.current = true
      }
      lastSpeechTime.current = currentTime
      silenceStartTime.current = null
    } else {
      // ì¹¨ë¬µ ê°ì§€
      if (isSpeaking.current && !silenceStartTime.current) {
        silenceStartTime.current = currentTime
      }
      
      // ì¹¨ë¬µ ì§€ì† ì‹œê°„ ì²´í¬
      if (silenceStartTime.current && currentTime - silenceStartTime.current > SILENCE_DURATION) {
        isSpeaking.current = false
        console.log('ğŸ”‡ Speech ended')
      }
    }
  }, [])

  // ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì²­í¬ ìƒì„±
  const createSmartChunk = useCallback(async (): Promise<void> => {
    if (audioBuffer.current.length === 0) return

    setIsProcessing(true)
    
    try {
      // ì˜¤ë²„ë©ì„ ê³ ë ¤í•œ ì²­í¬ ìƒì„±
      const chunks = audioBuffer.current
      const totalDuration = chunks.reduce((sum, chunk) => sum + chunk.duration, 0)
      
      // ë§ˆì§€ë§‰ ì²­í¬ì˜ ë ë¶€ë¶„ì„ ë‹¤ìŒ ì²­í¬ì— í¬í•¨ (ì˜¤ë²„ë©)
      const overlapStart = Math.max(0, totalDuration - OVERLAP_DURATION)
      
      // ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©
      const mergedBlob = new Blob(chunks.map(chunk => chunk.audioBlob), { type: 'audio/webm' })
      
      console.log(`ğŸµ Creating smart chunk: ${chunks.length} chunks, ${totalDuration}ms duration`)
      
      // Whisper API í˜¸ì¶œ
      const transcript = await sendToWhisper(mergedBlob, language)
      
      if (transcript) {
        // ë¬¸ë§¥ ê¸°ë°˜ í›„ì²˜ë¦¬
        const processedText = await processTranscriptWithContext(transcript)
        
        // ì™„ì „í•œ ë¬¸ì¥ì¸ì§€ íŒë‹¨
        const isComplete = isCompleteSentence(processedText)
        
        const processedTranscript: ProcessedTranscript = {
          id: `transcript_${Date.now()}`,
          text: processedText,
          confidence: transcript.confidence || 0.8,
          startTime: chunks[0].timestamp,
          endTime: chunks[chunks.length - 1].timestamp + chunks[chunks.length - 1].duration,
          isComplete
        }
        
        processedTranscripts.current.push(processedTranscript)
        
        // í´ë¼ì´ì–¸íŠ¸ì— ì—…ë°ì´íŠ¸
        const fullTranscript = processedTranscripts.current
          .filter(t => t.isComplete)
          .map(t => t.text)
          .join(' ')
        
        setCurrentTranscript(fullTranscript)
        onTranscriptUpdate(fullTranscript, !isComplete)
        
        console.log(`âœ… Smart chunk processed: "${processedText}" (complete: ${isComplete})`)
      }
      
      // ì˜¤ë²„ë© ë¶€ë¶„ë§Œ ë²„í¼ì— ìœ ì§€
      if (chunks.length > 1) {
        const lastChunk = chunks[chunks.length - 1]
        audioBuffer.current = [lastChunk]
      } else {
        audioBuffer.current = []
      }
      
    } catch (error) {
      console.error('âŒ Smart chunk processing failed:', error)
    } finally {
      setIsProcessing(false)
    }
  }, [language, onTranscriptUpdate])

  // ğŸ§  ë¬¸ë§¥ ê¸°ë°˜ í›„ì²˜ë¦¬
  const processTranscriptWithContext = useCallback(async (transcript: any): Promise<string> => {
    const currentText = transcript.text.trim()
    
    // ì´ì „ ë¬¸ë§¥ê³¼ ê²°í•©í•˜ì—¬ ë¬¸ì¥ ì™„ì„±ë„ í–¥ìƒ
    const previousTranscripts = processedTranscripts.current
      .filter(t => t.isComplete)
      .slice(-3) // ìµœê·¼ 3ê°œ ë¬¸ì¥ë§Œ ì°¸ì¡°
    
    const context = previousTranscripts.map(t => t.text).join(' ')
    
    if (context && !isCompleteSentence(currentText)) {
      // Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ ê¸°ë°˜ ë¬¸ì¥ ì™„ì„±
      try {
        const prompt = `ë‹¤ìŒì€ ìŒì„± ì¸ì‹ ê²°ê³¼ì…ë‹ˆë‹¤. ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ì„ ì™„ì„±í•´ì£¼ì„¸ìš”:

ì´ì „ ë¬¸ë§¥: "${context}"
í˜„ì¬ ì¸ì‹: "${currentText}"

ì™„ì„±ëœ ë¬¸ì¥ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.`

        const response = await fetch('/api/complete-sentence', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ prompt })
        })
        
        if (response.ok) {
          const result = await response.json()
          return result.completedText || currentText
        }
      } catch (error) {
        console.error('Sentence completion failed:', error)
      }
    }
    
    return currentText
  }, [])

  // ğŸ“ ì™„ì „í•œ ë¬¸ì¥ íŒë‹¨
  const isCompleteSentence = useCallback((text: string): boolean => {
    const trimmed = text.trim()
    
    // ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
    if (/[.!?]$/.test(trimmed)) return true
    
    // ì¼ë°˜ì ì¸ ë¬¸ì¥ ë íŒ¨í„´ í™•ì¸
    const endPatterns = [
      /(thank you|thanks)$/i,
      /(goodbye|bye)$/i,
      /(that's all|that is all)$/i,
      /(the end)$/i
    ]
    
    if (endPatterns.some(pattern => pattern.test(trimmed))) return true
    
    // ë¬¸ì¥ ê¸¸ì´ì™€ êµ¬ì¡° ë¶„ì„
    const words = trimmed.split(' ')
    if (words.length >= 5 && words.length <= 50) {
      // ì£¼ì–´-ë™ì‚¬ êµ¬ì¡° í™•ì¸ (ê°„ë‹¨í•œ ë²„ì „)
      const hasSubject = /^(I|you|he|she|it|we|they|this|that|there)/i.test(trimmed)
      const hasVerb = /\b(am|is|are|was|were|have|has|had|do|does|did|can|could|will|would|should|may|might)\b/i.test(trimmed)
      
      if (hasSubject && hasVerb) return true
    }
    
    return false
  }, [])

  // ğŸ¤ Whisper API í˜¸ì¶œ
  const sendToWhisper = useCallback(async (audioBlob: Blob, targetLanguage: string): Promise<any> => {
    const formData = new FormData()
    formData.append('audio', audioBlob, 'audio.webm')
    formData.append('language', targetLanguage)
    formData.append('sessionId', sessionId)
    
    try {
      const response = await fetch('/api/stt', {
        method: 'POST',
        body: formData
      })
      
      if (response.ok) {
        const result = await response.json()
        return result
      } else {
        throw new Error(`STT API failed: ${response.status}`)
      }
    } catch (error) {
      console.error('Whisper API error:', error)
      throw error
    }
  }, [sessionId])

  // ğŸµ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
  const handleAudioData = useCallback((event: BlobEvent) => {
    const audioBlob = event.data
    const currentTime = Date.now()
    
    // ì²­í¬ ì •ë³´ ìƒì„±
    const chunk: AudioChunk = {
      id: `chunk_${currentTime}`,
      audioBlob,
      timestamp: currentTime,
      duration: CHUNK_INTERVAL,
      isPartial: true
    }
    
    audioBuffer.current.push(chunk)
    
    // ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì¡°ê±´ í™•ì¸
    if (shouldCreateChunk(currentTime)) {
      createSmartChunk()
    }
  }, [shouldCreateChunk, createSmartChunk])

  // ğŸ¤ ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      
      // Audio Context ì„¤ì •
      audioContext.current = new AudioContext()
      analyser.current = audioContext.current.createAnalyser()
      microphone.current = audioContext.current.createMediaStreamSource(stream)
      
      microphone.current.connect(analyser.current)
      analyser.current.fftSize = 256
      
      // ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ ì‹œì‘
      const analyzeInterval = setInterval(analyzeAudioLevel, 100)
      
      // MediaRecorder ì„¤ì •
      const options = { mimeType: 'audio/webm;codecs=opus' }
      mediaRecorder.current = new MediaRecorder(stream, options)
      
      mediaRecorder.current.ondataavailable = handleAudioData
      mediaRecorder.current.start(CHUNK_INTERVAL)
      
      setIsRecording(true)
      console.log('ğŸ¤ Audio Buffer STT started')
      
      // ì •ë¦¬ í•¨ìˆ˜ ì €ì¥
      return () => {
        clearInterval(analyzeInterval)
        stream.getTracks().forEach(track => track.stop())
      }
      
    } catch (error) {
      console.error('Failed to start recording:', error)
    }
  }, [handleAudioData, analyzeAudioLevel])

  // ğŸ›‘ ë…¹ìŒ ì¤‘ì§€
  const stopRecording = useCallback(() => {
    if (mediaRecorder.current && isRecording) {
      mediaRecorder.current.stop()
      setIsRecording(false)
      
      // ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
      if (audioBuffer.current.length > 0) {
        createSmartChunk()
      }
      
      console.log('ğŸ›‘ Audio Buffer STT stopped')
    }
  }, [isRecording, createSmartChunk])

  // ğŸ§¹ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (audioContext.current) {
        audioContext.current.close()
      }
    }
  }, [])

  return (
    <div className="flex flex-col gap-4 p-4 border rounded-lg">
      <div className="flex items-center gap-4">
        <Button
          onClick={isRecording ? stopRecording : startRecording}
          disabled={isProcessing}
          className={isRecording ? 'bg-red-500 hover:bg-red-600' : 'bg-blue-500 hover:bg-blue-600'}
        >
          {isRecording ? 'ğŸ›‘ Stop' : 'ğŸ¤ Start'} Audio Buffer STT
        </Button>
        
        {isProcessing && (
          <div className="flex items-center gap-2 text-sm text-gray-600">
            <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-blue-500"></div>
            Processing...
          </div>
        )}
      </div>
      
      <div className="text-sm text-gray-600">
        <div>ğŸµ Buffer: {audioBuffer.current.length} chunks</div>
        <div>ğŸ“ Processed: {processedTranscripts.current.length} transcripts</div>
        <div>ğŸ¤ Speaking: {isSpeaking.current ? 'Yes' : 'No'}</div>
      </div>
      
      {currentTranscript && (
        <div className="p-3 bg-gray-50 rounded border">
          <div className="text-sm font-medium text-gray-700 mb-2">Current Transcript:</div>
          <div className="text-gray-900">{currentTranscript}</div>
        </div>
      )}
    </div>
  )
} 