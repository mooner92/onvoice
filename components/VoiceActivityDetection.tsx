import { useEffect, useRef, useState } from 'react'

interface VoiceActivityDetectionProps {
  onSpeechStart: () => void
  onSpeechEnd: (audioBlob: Blob) => void
  onSpeechActivity: (isActive: boolean) => void
  threshold?: number
  silenceDuration?: number
}

export function VoiceActivityDetection({
  onSpeechStart,
  onSpeechEnd,
  onSpeechActivity,
  threshold = 0.01,
  silenceDuration = 1000 // 1ì´ˆ ì¹¨ë¬µ í›„ êµ¬ê°„ ì¢…ë£Œ
}: VoiceActivityDetectionProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const animationFrameRef = useRef<number | null>(null)

  // ìŒì„± í™œë™ ê°ì§€ í•¨ìˆ˜
  const detectVoiceActivity = () => {
    if (!analyserRef.current) return

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)

    // RMS (Root Mean Square) ê³„ì‚°
    const rms = Math.sqrt(
      dataArray.reduce((sum, value) => sum + value * value, 0) / dataArray.length
    ) / 255

    const isCurrentlySpeaking = rms > threshold

    // ìŒì„± í™œë™ ìƒíƒœ ë³€í™” ê°ì§€
    if (isCurrentlySpeaking && !isSpeaking) {
      // ìŒì„± ì‹œìž‘
      console.log('ðŸŽ¤ Speech started (RMS:', rms.toFixed(3), ')')
      setIsSpeaking(true)
      onSpeechActivity(true)
      onSpeechStart()
      
      // ì¹¨ë¬µ íƒ€ì´ë¨¸ ì·¨ì†Œ
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
        silenceTimerRef.current = null
      }
      
      // ìƒˆë¡œìš´ ë…¹ìŒ ì‹œìž‘
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'inactive') {
        audioChunksRef.current = []
        mediaRecorderRef.current.start()
      }
    } else if (!isCurrentlySpeaking && isSpeaking) {
      // ì¹¨ë¬µ ê°ì§€ - íƒ€ì´ë¨¸ ì‹œìž‘
      if (!silenceTimerRef.current) {
        silenceTimerRef.current = setTimeout(() => {
          console.log('ðŸ”‡ Speech ended (silence detected)')
          setIsSpeaking(false)
          onSpeechActivity(false)
          
          // ë…¹ìŒ ì¤‘ì§€ ë° ì˜¤ë””ì˜¤ ì „ì†¡
          if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            mediaRecorderRef.current.stop()
          }
          
          silenceTimerRef.current = null
        }, silenceDuration)
      }
    } else if (isCurrentlySpeaking && isSpeaking) {
      // ê³„ì† ë§í•˜ê³  ìžˆìŒ - ì¹¨ë¬µ íƒ€ì´ë¨¸ ì·¨ì†Œ
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
        silenceTimerRef.current = null
      }
    }

    // ë‹¤ìŒ í”„ë ˆìž„ì—ì„œ ë‹¤ì‹œ ê²€ì‚¬
    animationFrameRef.current = requestAnimationFrame(detectVoiceActivity)
  }

  // ë§ˆì´í¬ ì´ˆê¸°í™”
  const initializeMicrophone = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })

      streamRef.current = stream

      // AudioContext ì„¤ì •
      audioContextRef.current = new AudioContext()
      const source = audioContextRef.current.createMediaStreamSource(stream)
      analyserRef.current = audioContextRef.current.createAnalyser()
      
      analyserRef.current.fftSize = 2048
      analyserRef.current.smoothingTimeConstant = 0.8
      source.connect(analyserRef.current)

      // MediaRecorder ì„¤ì •
      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
        if (audioBlob.size > 0) {
          onSpeechEnd(audioBlob)
        }
        audioChunksRef.current = []
      }

      setIsRecording(true)
      
      // ìŒì„± í™œë™ ê°ì§€ ì‹œìž‘
      detectVoiceActivity()

    } catch (error) {
      console.error('Microphone initialization failed:', error)
    }
  }

  // ì •ë¦¬ í•¨ìˆ˜
  const cleanup = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
    }
    
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
    }

    if (audioContextRef.current) {
      audioContextRef.current.close()
    }

    setIsRecording(false)
    setIsSpeaking(false)
  }

  useEffect(() => {
    initializeMicrophone()
    return cleanup
  }, [])

  return (
    <div className="flex items-center space-x-2">
      <div className={`w-3 h-3 rounded-full ${isSpeaking ? 'bg-red-500 animate-pulse' : 'bg-gray-300'}`} />
      <span className="text-sm text-gray-600">
        {isSpeaking ? 'Speaking...' : 'Listening...'}
      </span>
    </div>
  )
} 