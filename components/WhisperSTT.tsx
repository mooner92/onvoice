'use client'

import { useEffect, useRef, useState, useCallback } from 'react'

interface WhisperSTTProps {
  sessionId: string
  isRecording: boolean
  onTranscriptUpdate: (transcript: string, isPartial: boolean) => void
  onError: (error: string) => void
  lang?: string
}

// ğŸ¯ Audio Buffer ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì„¤ì •
const CHUNK_INTERVAL = 2000 // 2ì´ˆë§ˆë‹¤ ì²­í¬ ìƒì„± (ë” ì•ˆì •ì ì¸ ì²­í‚¹)
const MAX_CHUNK_DURATION = 8000 // ìµœëŒ€ 8ì´ˆ ì²­í¬ (ë¬¸ì¥ ì™„ì„±ë„ í–¥ìƒ)
const SILENCE_THRESHOLD = 0.015 // ë¬´ìŒ ê°ì§€ ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
const SILENCE_DURATION = 1500 // 1.5ì´ˆ ë¬´ìŒ í›„ ì²­í¬ ì²˜ë¦¬ (ë¬¸ì¥ ë ê°ì§€)
// const MIN_SPEECH_DURATION = 500 // ìµœì†Œ 0.5ì´ˆ ìŒì„± (ë…¸ì´ì¦ˆ í•„í„°ë§)
const OVERLAP_DURATION = 1000 // 1ì´ˆ ì˜¤ë²„ë© (ë¬¸ì¥ ì—°ê²°)
const BUFFER_MAX_SIZE = 12000 // 12ì´ˆ ë²„í¼ ìµœëŒ€ í¬ê¸°

export function WhisperSTT({ sessionId, isRecording, onTranscriptUpdate, onError, lang = 'en-US' }: WhisperSTTProps) {
  const [hasPermission, setHasPermission] = useState(false)
  const [isListening, setIsListening] = useState(false)
  const [status, setStatus] = useState('Initializing...')

  // ğŸµ Audio Buffer ê´€ë ¨ ìƒíƒœ
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioBufferRef = useRef<Blob[]>([]) // ğŸ†• Audio Buffer
  const processedTranscriptsRef = useRef<string[]>([]) // ğŸ†• ì²˜ë¦¬ëœ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸
  const silenceStartRef = useRef<number | null>(null)
  const isSpeakingRef = useRef(false)
  const lastSpeechTimeRef = useRef(0)
  const isStartingRef = useRef(false)
  const isProcessingRef = useRef(false)
  const bufferStartTimeRef = useRef(0) // ğŸ†• ë²„í¼ ì‹œì‘ ì‹œê°„

  // Request microphone permission
  const requestMicrophonePermission = useCallback(async (): Promise<boolean> => {
    try {
      console.log('ğŸ¤ Requesting microphone permission...')
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
          channelCount: 1,
        },
      })
      
      stream.getTracks().forEach(track => track.stop())
      setHasPermission(true)
      setStatus('Permission granted')
      console.log('âœ… Microphone permission granted')
      return true
    } catch (error) {
      console.error('âŒ Microphone permission denied:', error)
      setStatus('Permission denied')
      onError('Microphone permission denied')
      return false
    }
  }, [onError])

  // Cleanup function
  const cleanup = useCallback(() => {
    console.log('ğŸ§¹ Cleaning up Whisper STT...')
    
    if (mediaRecorderRef.current) {
      if (mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
      }
      mediaRecorderRef.current = null
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    analyserRef.current = null
    audioChunksRef.current = []
    audioBufferRef.current = [] // ğŸ†• Audio Buffer ì •ë¦¬
    processedTranscriptsRef.current = [] // ğŸ†• ì²˜ë¦¬ëœ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì •ë¦¬
    bufferStartTimeRef.current = 0 // ğŸ†• ë²„í¼ ì‹œì‘ ì‹œê°„ ì •ë¦¬
    silenceStartRef.current = null
    isSpeakingRef.current = false
    lastSpeechTimeRef.current = 0
    setIsListening(false)
  }, [])

  // Calculate RMS for volume detection
  const calculateRMS = (dataArray: Uint8Array): number => {
    let sum = 0
    for (let i = 0; i < dataArray.length; i++) {
      sum += dataArray[i] * dataArray[i]
    }
    return Math.sqrt(sum / dataArray.length) / 255
  }

  // Detect speech activity
  const detectSpeechActivity = useCallback(() => {
    if (!analyserRef.current || !isListening) return

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)
    
    const volume = calculateRMS(dataArray)
    const now = Date.now()

    if (volume > SILENCE_THRESHOLD) {
      isSpeakingRef.current = true
      lastSpeechTimeRef.current = now
      silenceStartRef.current = null
    } else if (isSpeakingRef.current) {
      if (!silenceStartRef.current) {
        silenceStartRef.current = now
      } else if (now - silenceStartRef.current > SILENCE_DURATION) {
        isSpeakingRef.current = false
        processAudioChunk()
      }
    }
  }, [isListening])

  // Start audio analysis
  const startAudioAnalysis = useCallback(() => {
    if (!analyserRef.current) return

    const analyzeAudio = () => {
      if (isListening) {
        detectSpeechActivity()
        requestAnimationFrame(analyzeAudio)
      }
    }
    analyzeAudio()
  }, [isListening, detectSpeechActivity])

  // ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: ë¬¸ë§¥ ê¸°ë°˜ ì²­í¬ ë¶„í• 
  const shouldProcessBuffer = useCallback((currentTime: number): boolean => {
    const bufferDuration = currentTime - bufferStartTimeRef.current
    const timeSinceLastSpeech = currentTime - lastSpeechTimeRef.current
    
    // 1. ì¹¨ë¬µ ê°ì§€ (ë¬¸ì¥ ë)
    if (timeSinceLastSpeech > SILENCE_DURATION && isSpeakingRef.current) {
      console.log('ğŸ”‡ Silence detected - processing buffer')
      return true
    }
    
    // 2. ë²„í¼ í¬ê¸° ì œí•œ
    if (bufferDuration > BUFFER_MAX_SIZE) {
      console.log('ğŸ“¦ Buffer full - processing buffer')
      return true
    }
    
    // 3. ì •ê¸°ì  ì²˜ë¦¬ (ê¸´ ë¬¸ì¥ ëŒ€ë¹„)
    if (bufferDuration > MAX_CHUNK_DURATION && audioBufferRef.current.length > 0) {
      console.log('â° Regular processing - processing buffer')
      return true
    }
    
    return false
  }, [])

  // ğŸµ Audio Buffer ì²˜ë¦¬
  const processAudioBuffer = useCallback(() => {
    if (audioBufferRef.current.length === 0) return

    // Use the MIME type from MediaRecorder
    const mimeType = mediaRecorderRef.current?.mimeType || 'audio/webm'
    const audioBlob = new Blob(audioBufferRef.current, { type: mimeType })
    
    console.log(`ğŸµ Processing audio buffer: ${audioBufferRef.current.length} chunks, ${audioBlob.size} bytes`)
    
    if (audioBlob.size > 0) {
      sendToWhisper(audioBlob)
    }
    
    // ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ì²­í¬ ìœ ì§€
    if (audioBufferRef.current.length > 1) {
      audioBufferRef.current = audioBufferRef.current.slice(-1)
      bufferStartTimeRef.current = Date.now() - OVERLAP_DURATION
    } else {
      audioBufferRef.current = []
      bufferStartTimeRef.current = 0
    }
  }, [])

  // ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ê¸°ì¡´ processAudioChunk ëŒ€ì²´)
  const processAudioChunk = useCallback(() => {
    if (audioChunksRef.current.length === 0) return

    // ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€
    audioBufferRef.current.push(...audioChunksRef.current)
    audioChunksRef.current = []
    
    // ë²„í¼ ì‹œì‘ ì‹œê°„ ì„¤ì •
    if (bufferStartTimeRef.current === 0) {
      bufferStartTimeRef.current = Date.now()
    }
    
    // ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì¡°ê±´ í™•ì¸
    if (shouldProcessBuffer(Date.now())) {
      processAudioBuffer()
    }
  }, [shouldProcessBuffer, processAudioBuffer])

  // Send audio to Whisper API
  const sendToWhisper = useCallback(async (audioBlob: Blob) => {
    console.log('ğŸš€ Sending audio chunk to Whisper API...')

    // ì¤‘ë³µ ì „ì†¡ ë°©ì§€ë¥¼ ìœ„í•œ ë””ë°”ìš´ì‹±
    if (isProcessingRef.current) {
      console.log('âš ï¸ Skipping chunk - already processing')
      return
    }
    
    isProcessingRef.current = true

    const formData = new FormData()
    // Use the actual MIME type from the blob
    const fileExtension = audioBlob.type.includes('webm') ? 'webm' : 
                         audioBlob.type.includes('mp4') ? 'm4a' : 
                         audioBlob.type.includes('wav') ? 'wav' : 'webm'
    formData.append('audio', audioBlob, `audio.${fileExtension}`)
    formData.append('sessionId', sessionId)
    formData.append('model', 'whisper-1')
    formData.append('language', lang)
    formData.append('response_format', 'verbose_json')
    formData.append('temperature', '0')
    formData.append('prompt', 'This is a lecture or presentation. Focus on clear speech and ignore background noise.')
    formData.append('enableGrammarCheck', 'true')
    formData.append('useGemini', 'true')

    try {
      const response = await fetch('/api/stt', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        throw new Error(`Whisper API error: ${response.status}`)
      }

      const result = await response.json()
      console.log('âœ… Whisper API response:', result)

      if (result.transcript && result.transcript.trim()) {
        const processedText = await processTranscriptWithContext(result.transcript.trim())
        processedTranscriptsRef.current.push(processedText)
        
        // ì „ì²´ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì¡°í•©
        const fullTranscript = processedTranscriptsRef.current.join(' ')
        onTranscriptUpdate(fullTranscript, false)
        
        console.log(`âœ… Processed transcript: "${processedText}"`)
        console.log(`ğŸ“ Full transcript: "${fullTranscript}"`)
      }
    } catch (error) {
      console.error('âŒ Whisper API error:', error)
      onError('Whisper transcription failed')
    } finally {
      isProcessingRef.current = false
    }
  }, [sessionId, lang, onTranscriptUpdate, onError])

  // ğŸ§  ë¬¸ë§¥ ê¸°ë°˜ í›„ì²˜ë¦¬
  const processTranscriptWithContext = useCallback(async (transcript: string): Promise<string> => {
    const currentText = transcript.trim()
    
    // ì´ì „ ë¬¸ë§¥ê³¼ ê²°í•©í•˜ì—¬ ë¬¸ì¥ ì™„ì„±ë„ í–¥ìƒ
    const previousTranscripts = processedTranscriptsRef.current.slice(-3) // ìµœê·¼ 3ê°œ ë¬¸ì¥ë§Œ ì°¸ì¡°
    const context = previousTranscripts.join(' ')
    
    if (context && !isCompleteSentence(currentText)) {
      // Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ ê¸°ë°˜ ë¬¸ì¥ ì™„ì„±
      try {
        const prompt = `ë‹¤ìŒì€ ìŒì„± ì¸ì‹ ê²°ê³¼ì…ë‹ˆë‹¤. ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ì„ ì™„ì„±í•´ì£¼ì„¸ìš”:

ì´ì „ ë¬¸ë§¥: "${context}"
í˜„ì¬ ì¸ì‹: "${currentText}"

ì™„ì„±ëœ ë¬¸ì¥ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.`

        const response = await fetch('/api/complete-sentence', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ prompt })
        })
        
        if (response.ok) {
          const result = await response.json()
          return result.completedText || currentText
        }
      } catch (error) {
        console.error('Sentence completion failed:', error)
      }
    }
    
    return currentText
  }, [])

  // ğŸ“ ì™„ì „í•œ ë¬¸ì¥ íŒë‹¨
  const isCompleteSentence = useCallback((text: string): boolean => {
    const trimmed = text.trim()
    
    // ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
    if (/[.!?]$/.test(trimmed)) return true
    
    // ì¼ë°˜ì ì¸ ë¬¸ì¥ ë íŒ¨í„´ í™•ì¸
    const endPatterns = [
      /(thank you|thanks)$/i,
      /(goodbye|bye)$/i,
      /(that's all|that is all)$/i,
      /(the end)$/i
    ]
    
    if (endPatterns.some(pattern => pattern.test(trimmed))) return true
    
    // ë¬¸ì¥ ê¸¸ì´ì™€ êµ¬ì¡° ë¶„ì„
    const words = trimmed.split(' ')
    if (words.length >= 5 && words.length <= 50) {
      // ì£¼ì–´-ë™ì‚¬ êµ¬ì¡° í™•ì¸ (ê°„ë‹¨í•œ ë²„ì „)
      const hasSubject = /^(I|you|he|she|it|we|they|this|that|there)/i.test(trimmed)
      const hasVerb = /\b(am|is|are|was|were|have|has|had|do|does|did|can|could|will|would|should|may|might)\b/i.test(trimmed)
      
      if (hasSubject && hasVerb) return true
    }
    
    return false
  }, [])

  // Start recording
  const startRecording = useCallback(async () => {
    if (isStartingRef.current || isListening || mediaRecorderRef.current) {
      console.log('âš ï¸ Already recording or starting, skipping...')
      return
    }

    isStartingRef.current = true
    console.log('ğŸš€ Starting Whisper STT recording...')

    try {
      // Get permission if needed
      if (!hasPermission) {
        const granted = await requestMicrophonePermission()
        if (!granted) {
          isStartingRef.current = false
          return
        }
      }

      // Get audio stream
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
          channelCount: 1,
        },
      })

      streamRef.current = stream

      // Setup audio context for VAD
      audioContextRef.current = new AudioContext({ sampleRate: 16000 })
      const source = audioContextRef.current.createMediaStreamSource(stream)

      // Setup analyser for volume detection
      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 2048
      analyserRef.current.smoothingTimeConstant = 0.8

      source.connect(analyserRef.current)

      // Start audio analysis
      startAudioAnalysis()

      // ğŸµ MediaRecorder ì„¤ì • - ë” ì•ˆì •ì ì¸ í˜•ì‹ ì‚¬ìš©
      let mimeType = 'audio/webm;codecs=opus'
      
      // Whisper APIì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹ ìš°ì„ ìˆœìœ„
      const formats = [
        'audio/webm;codecs=opus',  // ìµœê³  í’ˆì§ˆ
        'audio/webm',              // ê¸°ë³¸ webm
        'audio/mp4',               // MP4
        'audio/wav'                // WAV
      ]
      
      // ì§€ì›ë˜ëŠ” í˜•ì‹ ì°¾ê¸°
      for (const format of formats) {
        if (MediaRecorder.isTypeSupported(format)) {
          mimeType = format
          console.log(`âœ… Using audio format: ${format}`)
          break
        }
      }
      
      // ì§€ì›ë˜ëŠ” í˜•ì‹ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        mimeType = 'audio/webm'
        console.log(`âš ï¸ Using fallback format: ${mimeType}`)
      }
      
      // ì§€ì›ë˜ëŠ” í˜•ì‹ ë¡œê¹…
      console.log(`ğŸµ Final MIME type: ${mimeType}`)
      console.log(`ğŸµ Supported formats:`, formats.map(f => `${f}: ${MediaRecorder.isTypeSupported(f)}`))
      
      // MediaRecorder ì„¤ì • - ë” ë†’ì€ í’ˆì§ˆ
      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: mimeType,
        audioBitsPerSecond: 32000, // ë” ë†’ì€ ë¹„íŠ¸ë ˆì´íŠ¸
      })

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          console.log(`ğŸµ Audio chunk received: ${event.data.size} bytes, type: ${event.data.type}`)
          audioChunksRef.current.push(event.data)
          
          // ì²­í¬ í¬ê¸° ê¸°ë°˜ ì²˜ë¦¬ (ë” ì •í™•í•œ ê³„ì‚°)
          const totalSize = audioChunksRef.current.reduce((total, chunk) => total + chunk.size, 0)
          const estimatedDuration = (totalSize / 32000) * 8 // 32kbps ê¸°ì¤€
          
          console.log(`ğŸ“Š Buffer stats: ${audioChunksRef.current.length} chunks, ${totalSize} bytes, ~${Math.round(estimatedDuration)}ms`)
          
          if (estimatedDuration >= MAX_CHUNK_DURATION) {
            console.log('â° Max duration reached, processing chunk...')
            processAudioChunk()
          }
        }
      }

      mediaRecorderRef.current.start(CHUNK_INTERVAL)
      setIsListening(true)
      setStatus('Listening with Whisper...')

      console.log('âœ… Whisper STT recording started')

    } catch (error) {
      console.error('âŒ Failed to start Whisper STT recording:', error)
      setStatus('Failed to start')
      onError('Failed to start Whisper STT recording')
    } finally {
      isStartingRef.current = false
    }
  }, [hasPermission, isListening, requestMicrophonePermission, startAudioAnalysis, processAudioChunk, onError])

  // Stop recording
  const stopRecording = useCallback(() => {
    console.log('ğŸ›‘ Stopping Whisper STT recording...')

    if (mediaRecorderRef.current && isListening) {
      mediaRecorderRef.current.stop()
    }

    // Process any remaining audio chunks and buffer
    if (audioChunksRef.current.length > 0) {
      processAudioChunk()
    }
    
    // Process any remaining audio buffer
    if (audioBufferRef.current.length > 0) {
      processAudioBuffer()
    }

    cleanup()
    setStatus('Ready to start')
  }, [isListening, processAudioChunk, cleanup])

  // Handle recording state changes
  useEffect(() => {
    if (isRecording && sessionId && !isListening && !isStartingRef.current) {
      startRecording()
    } else if (!isRecording && isListening) {
      stopRecording()
    }
  }, [isRecording, sessionId, isListening, startRecording, stopRecording])

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      cleanup()
    }
  }, [cleanup])

  if (!hasPermission) {
    return (
      <div className='space-y-3'>
        <div className='flex items-center space-x-2 text-sm'>
          <div className='h-3 w-3 rounded-full bg-gray-500' />
          <span className='text-gray-600'>{status}</span>
          <span className='rounded bg-blue-100 px-2 py-1 text-xs font-medium text-blue-700'>Whisper STT</span>
        </div>

        <button
          onClick={requestMicrophonePermission}
          className='w-full rounded-lg bg-blue-100 px-3 py-2 text-sm text-blue-800 hover:bg-blue-200'
        >
          ğŸ¤ Grant Microphone Permission for Whisper STT
        </button>
      </div>
    )
  }

  return (
    <div className='space-y-3'>
      <div className='flex items-center space-x-2 text-sm'>
        <div
          className={`h-3 w-3 rounded-full ${
            isListening ? 'animate-pulse bg-green-500' : 'bg-yellow-500'
          }`}
        />
        <span
          className={isListening ? 'font-medium text-green-600' : 'text-yellow-600'}
        >
          {isListening ? 'ğŸ¤ Listening with Whisper' : status}
        </span>
        <span className='rounded bg-purple-100 px-2 py-1 text-xs font-medium text-purple-700'>Whisper STT</span>
      </div>

      {isListening && (
        <div className='rounded-lg border border-purple-200 bg-purple-50 p-3'>
          <div className='flex items-center space-x-2 text-purple-800'>
            <div className='h-2 w-2 animate-pulse rounded-full bg-purple-500'></div>
            <span className='text-sm font-medium'>Whisper STT Active</span>
          </div>
          <p className='mt-1 text-xs text-purple-700'>
            ğŸ”‡ Noise filtering enabled â€¢ ğŸ¯ Smart chunking â€¢ ğŸŒ High accuracy
          </p>
        </div>
      )}
    </div>
  )
} 