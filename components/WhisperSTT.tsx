'use client'

import { useEffect, useRef, useState, useCallback } from 'react'
// @ts-ignore
import levenshtein from 'js-levenshtein'
import { createVAD, VoiceActivityDetector, VADState, audioBlobToFloat32Array } from '../lib/vad-utils'

interface WhisperSTTProps {
  sessionId: string
  isRecording: boolean
  onTranscriptUpdate: (transcript: string, isPartial: boolean) => void
  onError: (error: string) => void
  lang?: string
  vadConfig?: {
    threshold?: number
    silenceThreshold?: number
    speechThreshold?: number
    smoothingWindow?: number
    minBlobSize?: number
    enabled?: boolean // VAD í™œì„±í™”/ë¹„í™œì„±í™” ì˜µì…˜
  }
}

interface AudioSegment {
  id: string
  audioBlob: Blob
  startTime: number
  endTime: number
  queue: 'A' | 'B'
}

interface SpeechSegment {
  id: string
  audioBlob: Blob
  startTime: number
  endTime: number
  confidence: number
  duration: number
}

interface STTResult {
  id: string
  text: string
  startTime: number
  endTime: number
  confidence: number
  queue: 'A' | 'B'
  timestamp: number
}

class TextBuffer {
  private segments: STTResult[] = []
  private finalText: string = ''
  private lastSentenceEnd: number = 0
  private sessionId: string = ''

  addResult(result: STTResult) {
    // ì¤‘ë³µ ì„¸ê·¸ë¨¼íŠ¸ ì²´í¬ - ê°™ì€ ë‚´ìš©ì˜ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
    const isDuplicate = this.segments.some(segment => {
      // ì‹œê°„ ê²¹ì¹¨ ì²´í¬ (ë” ê´€ëŒ€í•˜ê²Œ)
      const timeOverlap = Math.abs(segment.startTime - result.startTime) < 1000 // 1ì´ˆ ì´ë‚´
      
      // ë‚´ìš© ìœ ì‚¬ë„ ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
      const similarity = this.calculateSimilarity(segment.text, result.text)
      
      return timeOverlap && similarity > 0.9 // 90% ì´ìƒ ìœ ì‚¬í•´ì•¼ë§Œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
    })
    
    if (isDuplicate) {
      console.log('ğŸ”„ Duplicate segment detected, skipping:', result.text)
      return
    }
    
    this.segments.push(result)
    this.mergeResults()
    this.detectSentenceBoundaries()
  }

  setSessionId(sessionId: string) {
    this.sessionId = sessionId
  }

  private mergeResults() {
    // ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
    this.segments.sort((a, b) => a.startTime - b.startTime)
    
    let mergedText = ''
    let currentTime = 0
    let lastProcessedSegment: STTResult | null = null

    for (let i = 0; i < this.segments.length; i++) {
      const segment = this.segments[i]
      
      // ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì™€ ì‹œê°„ ê²¹ì¹¨ ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
      const hasTimeOverlap = lastProcessedSegment && 
        segment.startTime < lastProcessedSegment.endTime
      
      if (hasTimeOverlap) {
        // ì‹œê°„ ê²¹ì¹¨ì´ ìˆëŠ” ê²½ìš° - ë” ì •êµí•œ ì¤‘ë³µ ì œê±°
        const overlap = this.removeOverlapAdvanced(mergedText, segment.text, lastProcessedSegment)
        const cleanOverlap = this.cleanIncompleteWords(overlap)
        if (cleanOverlap && cleanOverlap.length > 2) {
          mergedText += ' ' + cleanOverlap
          currentTime = segment.endTime
          lastProcessedSegment = segment
        }
      } else {
        // ìƒˆë¡œìš´ êµ¬ê°„ - ê²¹ì¹¨ ì—†ìŒ
        mergedText += ' ' + segment.text
        currentTime = segment.endTime
        lastProcessedSegment = segment
      }
    }

    this.finalText = mergedText.trim()
  }

  private removeOverlapAdvanced(existingText: string, newText: string, lastSegment: STTResult | null): string {
    // 1. ì™„ì „ ë™ì¼í•œ í…ìŠ¤íŠ¸ ì²´í¬
    if (existingText === newText) {
      console.log('ğŸ”„ Exact duplicate detected, skipping')
      return ''
    }
    
    // 2. ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì²´í¬
    if (!newText.trim() || newText.trim().length < 3) {
      console.log('ğŸ”„ Text too short or empty, skipping')
      return ''
    }
    
    // 3. ë¶ˆì™„ì „í•œ ë‹¨ì–´ë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
    const words = newText.split(' ')
    if (words[0] && (words[0].endsWith('-') || words[0].length < 2)) {
      console.log('ğŸ”„ Incomplete word at start detected, skipping')
      return ''
    }
    
    // 4. ê¸°ì¡´ í…ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ê³¼ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì˜ ì‹œì‘ ë¶€ë¶„ ë¹„êµ
    const existingWords = existingText.split(' ')
    const newWords = newText.split(' ')
    
    // ë§ˆì§€ë§‰ 3-5ê°œ ë‹¨ì–´ì™€ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ 3-5ê°œ ë‹¨ì–´ ë¹„êµ
    for (let i = 3; i <= Math.min(5, existingWords.length, newWords.length); i++) {
      const lastExisting = existingWords.slice(-i).join(' ')
      const firstNew = newWords.slice(0, i).join(' ')
      
      if (lastExisting === firstNew) {
        // ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ë°˜í™˜
        const remaining = newWords.slice(i).join(' ')
        console.log(`ğŸ”„ Overlap detected: "${lastExisting}" - remaining: "${remaining}"`)
        return remaining
      }
    }
    
    // 5. ë¶€ë¶„ ê²¹ì¹¨ ê²€ì‚¬ (ë” ì •êµí•˜ê²Œ)
    for (let i = 2; i <= Math.min(4, newWords.length); i++) {
      for (let j = 0; j <= newWords.length - i; j++) {
        const phrase = newWords.slice(j, j + i).join(' ')
        if (existingText.includes(phrase)) {
          // ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ë°˜í™˜
          const remaining = newWords.slice(j + i).join(' ')
          console.log(`ğŸ”„ Partial overlap detected: "${phrase}" - remaining: "${remaining}"`)
          return remaining
        }
      }
    }
    
    // 6. Levenshtein ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬
    const similarity = this.calculateSimilarity(existingText, newText)
    if (similarity > 0.8) { // 80%ë¡œ ë” ê´€ëŒ€í•˜ê²Œ
      console.log(`ğŸ”„ High similarity detected (${similarity.toFixed(2)}), skipping duplicate text`)
      return ''
    }
    
    // 7. ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë°˜ë³µë˜ëŠ”ì§€ ì²´í¬
    const lastSentence = this.getLastSentence(existingText)
    if (lastSentence && newText.includes(lastSentence)) {
      console.log(`ğŸ”„ Last sentence repeat detected: "${lastSentence}"`)
      return newText.replace(lastSentence, '').trim()
    }
    
    // 8. ì¼ë°˜ì ì¸ hallucination íŒ¨í„´ ì²´í¬
    const hallucinationPatterns = [
      'thank you for watching',
      'thanks for watching',
      'please like and subscribe',
      'don\'t forget to subscribe',
      'see you next time',
      'goodbye',
      'bye bye',
      'this is a live speech transcription',
      'this is a live conversation',
      'please transcribe accurately',
      'with proper spelling and punctuation',
      'use proper spelling for technical terms',
      'avoid filler words',
      'do not add any additional text'
    ]
    
    const lowerNewText = newText.toLowerCase()
    for (const pattern of hallucinationPatterns) {
      if (lowerNewText.includes(pattern)) {
        console.log(`ğŸ”„ Hallucination pattern detected: "${pattern}"`)
        return newText.replace(new RegExp(pattern, 'gi'), '').trim()
      }
    }
    
    return newText
  }

  private cleanIncompleteWords(text: string): string {
    if (!text.trim()) return ''
    
    const words = text.trim().split(' ')
    const cleanWords = []
    
    for (let i = 0; i < words.length; i++) {
      const word = words[i]
      
      // ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° (í•˜ì´í”ˆìœ¼ë¡œ ëë‚˜ê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš°)
      if (i === words.length - 1) {
        if (word.endsWith('-') || word.length < 2) {
          // ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë‹¨ì–´ ì œê±°
          continue
        }
      }
      
      // ì¤‘ê°„ ë‹¨ì–´ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° (í•˜ì´í”ˆìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°)
      if (word.endsWith('-') && i < words.length - 1) {
        // ë‹¤ìŒ ë‹¨ì–´ì™€ í•©ì³ì„œ ì™„ì„±ëœ ë‹¨ì–´ì¸ì§€ í™•ì¸
        const nextWord = words[i + 1]
        if (nextWord && !nextWord.startsWith('-')) {
          // ë‹¤ìŒ ë‹¨ì–´ì™€ í•©ì³ì„œ ì™„ì„±ëœ ë‹¨ì–´ë¡œ ì²˜ë¦¬
          cleanWords.push(word + nextWord)
          i++ // ë‹¤ìŒ ë‹¨ì–´ ê±´ë„ˆë›°ê¸°
          continue
        }
      }
      
      cleanWords.push(word)
    }
    
    return cleanWords.join(' ')
  }

  private getLastSentence(text: string): string {
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0)
    return sentences.length > 0 ? sentences[sentences.length - 1].trim() : ''
  }

  private calculateSimilarity(text1: string, text2: string): number {
    if (text1.length === 0 || text2.length === 0) return 0
    
    const distance = levenshtein(text1.toLowerCase(), text2.toLowerCase())
    const maxLength = Math.max(text1.length, text2.length)
    
    return 1 - (distance / maxLength)
  }

  private detectSentenceBoundaries() {
    // ë” ì •í™•í•œ ë¬¸ì¥ ê²½ê³„ ê°ì§€
    const sentenceEndings = /[.!?]\s+/g
    const matches = [...this.finalText.matchAll(sentenceEndings)]
    
    if (matches.length > 0) {
      const lastMatch = matches[matches.length - 1]
      const sentenceEndIndex = lastMatch.index! + lastMatch[0].length
      
      if (sentenceEndIndex > this.lastSentenceEnd) {
        const newSentence = this.finalText.substring(this.lastSentenceEnd, sentenceEndIndex).trim()
        if (newSentence && newSentence.length > 5) { // ìµœì†Œ ê¸¸ì´ ì²´í¬
          console.log(`ğŸ“ New sentence detected: "${newSentence}"`)
          // ìƒˆë¡œìš´ ë¬¸ì¥ì„ DBì— ì €ì¥
          this.saveSentence(newSentence)
          this.lastSentenceEnd = sentenceEndIndex
        }
      }
    }
    
    // ì¶”ê°€: ê¸´ í…ìŠ¤íŠ¸ì—ì„œ ê°•ì œë¡œ ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œê°€ ì—†ëŠ” ê²½ìš°)
    const currentPartial = this.finalText.substring(this.lastSentenceEnd).trim()
    if (currentPartial.length > 100 && !currentPartial.includes('.')) {
      // 100ì ì´ìƒì´ê³  ë§ˆì¹¨í‘œê°€ ì—†ìœ¼ë©´ ê°•ì œë¡œ ë¬¸ì¥ìœ¼ë¡œ ì²˜ë¦¬
      console.log(`ğŸ“ Force sentence split for long text: "${currentPartial.substring(0, 50)}..."`)
      this.saveSentence(currentPartial)
      this.lastSentenceEnd = this.finalText.length
    }
  }

    private saveSentence(sentence: string) {
    // DB ì €ì¥ ë¡œì§
    console.log('ğŸ’¾ Saving sentence:', sentence)
    
    if (!this.sessionId) {
      console.warn('âš ï¸ No session ID available for saving sentence')
      return
    }
    
    // ë¨¼ì € ì„¸ì…˜ ì‹œì‘ì„ í™•ì¸
    fetch('/api/stt-stream', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        type: 'start',
        sessionId: this.sessionId,
      }),
    }).then(() => {
      // ì„¸ì…˜ ì‹œì‘ í›„ ë¬¸ì¥ ì €ì¥
      return fetch('/api/stt-stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          type: 'transcript',
          sessionId: this.sessionId,
          transcript: sentence,
          isPartial: false,
        }),
      })
    }).catch(error => {
      console.error('âŒ Failed to save sentence:', error)
    })
  }

  getCurrentText(): string {
    return this.finalText
  }

  getPartialText(): string {
    // ë§ˆì§€ë§‰ ë¬¸ì¥ ì¢…ë£Œ ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (ë¶€ë¶„ ê²°ê³¼)
    return this.finalText.substring(this.lastSentenceEnd).trim()
  }
}

export function WhisperSTT({ sessionId, isRecording, onTranscriptUpdate, onError, lang = 'en', vadConfig = {} }: WhisperSTTProps) {
  const [isRecordingState, setIsRecordingState] = useState(false)
  const [status, setStatus] = useState('Initializing...')
  const [hasPermission, setHasPermission] = useState(false)
  const [vadStatus, setVadStatus] = useState<VADState | null>(null)
  
  // ì„¸ì…˜ ê¸°ë°˜ ì‹œê°„ ì¶”ì  (Fast Refresh ë¬¸ì œ í•´ê²°)
  const sessionStartTimeRef = useRef<number>(0)
  const currentSessionIdRef = useRef<string>('')

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const textBufferRef = useRef<TextBuffer>(new TextBuffer())
  
  // VAD ê´€ë ¨ ìƒíƒœ
  const vadRef = useRef<VoiceActivityDetector | null>(null)
  const speechBufferRef = useRef<Blob[]>([])
  const isRecordingSpeechRef = useRef(false)
  const speechStartTimeRef = useRef<number>(0)
  const lastVadUpdateRef = useRef<number>(0)
  const lastSegmentTimeRef = useRef<number | null>(null)
  
  // ì¤‘ë³µ ì„¸ê·¸ë¨¼íŠ¸ ë°©ì§€ë¥¼ ìœ„í•œ Set
  const processedSegments = useRef<Set<string>>(new Set())
  const pendingRequests = useRef<Set<string>>(new Set())
  
  // ìƒíƒœ ê´€ë¦¬
  const isActiveRef = useRef(false)
  const mountedRef = useRef(true)

  // VAD ê¸°ë°˜ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (VAD ë¹„í™œì„±í™” ì˜µì…˜)
  const processVADAudio = useCallback(async (audioBlob: Blob) => {
    if (!mountedRef.current || !isActiveRef.current) return

    // VAD ë¹„í™œì„±í™”: ì‹œê°„ ê¸°ë°˜ ì²˜ë¦¬ë¡œ ë³€ê²½
    if (!vadRef.current) {
      // ê¸°ì¡´ ì‹œê°„ ê¸°ë°˜ ì²˜ë¦¬ ë°©ì‹
      audioChunksRef.current.push(audioBlob)
      
      // 10ì´ˆë§ˆë‹¤ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
      const currentTime = Date.now()
      if (!lastSegmentTimeRef.current) {
        lastSegmentTimeRef.current = currentTime
      }
      
      if (currentTime - lastSegmentTimeRef.current >= 10000) { // 10ì´ˆ
        console.log('â° Time-based segment processing (VAD disabled)')
        await processTimeBasedSegment()
        lastSegmentTimeRef.current = currentTime
      }
      return
    }

    try {
      // ìƒˆë¡œìš´ ë¸”ë¡­ ê¸°ë°˜ VAD ì²˜ë¦¬ (WebM/Opus í˜•ì‹ì— ìµœì í™”)
      const vadState = vadRef.current.processAudioBlob(audioBlob)
      
      // VAD ìƒíƒœ ì—…ë°ì´íŠ¸ (UI í‘œì‹œìš©)
      if (Date.now() - lastVadUpdateRef.current > 100) { // 100msë§ˆë‹¤ ì—…ë°ì´íŠ¸
        setVadStatus(vadState)
        lastVadUpdateRef.current = Date.now()
      }

      // ìŒì„± ì‹œì‘ ê°ì§€
      if (vadState.isSpeech && !isRecordingSpeechRef.current) {
        console.log('ğŸ¤ Speech detected, starting speech recording...')
        isRecordingSpeechRef.current = true
        speechStartTimeRef.current = Date.now()
        speechBufferRef.current = []
      }

      // ìŒì„± ë²„í¼ë§
      if (isRecordingSpeechRef.current) {
        speechBufferRef.current.push(audioBlob)
      }

      // ìŒì„± ì¢…ë£Œ ê°ì§€
      if (!vadState.isSpeech && isRecordingSpeechRef.current) {
        // ë¬´ìŒ ì§€ì† ì‹œê°„ ì²´í¬
        if (vadState.silenceDuration >= 1.5) { // 1.5ì´ˆ ë¬´ìŒ í›„ ìŒì„± ì¢…ë£Œë¡œ íŒë‹¨
          console.log('ğŸ”‡ Speech ended, processing speech segment...')
          await processSpeechSegment()
        }
      }

      // ìŒì„± êµ¬ê°„ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ê°•ì œë¡œ ì²˜ë¦¬ (ìµœëŒ€ 15ì´ˆë¡œ ë‹¨ì¶•)
      if (isRecordingSpeechRef.current && vadState.speechDuration >= 15) {
        console.log('â° Speech segment too long, forcing processing...')
        await processSpeechSegment()
      }

    } catch (error) {
      // VAD ì²˜ë¦¬ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰ (ì˜¤ë””ì˜¤ ë…¹ìŒì€ ê³„ì†ë¨)
      console.warn('âš ï¸ VAD processing warning (continuing):', error)
    }
  }, [])

  // ì‹œê°„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (VAD ë¹„í™œì„±í™” ì‹œ)
  const processTimeBasedSegment = async () => {
    if (audioChunksRef.current.length === 0) return

    try {
      const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
      const segmentId = `time-segment-${Date.now()}`
      
      console.log(`â° Processing time-based segment: ${segmentId} (${audioBlob.size} bytes)`)
      
      // Whisper APIë¡œ ì „ì†¡
      const segment: AudioSegment = {
        id: segmentId,
        audioBlob,
        startTime: Date.now() - 10000, // 10ì´ˆ ì „
        endTime: Date.now(),
        queue: 'A'
      }
      
      await sendToWhisper(segment)
      
      // ë²„í¼ ì´ˆê¸°í™”
      audioChunksRef.current = []
      
    } catch (error) {
      console.error('âŒ Time-based segment processing error:', error)
    }
  }

  // ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
  const processSpeechSegment = async () => {
    if (!isRecordingSpeechRef.current || speechBufferRef.current.length === 0) return

    try {
      const speechEndTime = Date.now()
      const speechDuration = (speechEndTime - speechStartTimeRef.current) / 1000

      // ìµœì†Œ ê¸¸ì´ ì²´í¬ (1ì´ˆ ë¯¸ë§Œì€ ë¬´ì‹œ)
      if (speechDuration < 1.0) {
        console.log('âš ï¸ Speech segment too short, skipping...')
        resetSpeechRecording()
        return
      }

      // ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
      const speechSegment: SpeechSegment = {
        id: `speech-${Date.now()}`,
        audioBlob: new Blob(speechBufferRef.current, { type: 'audio/webm' }),
        startTime: speechStartTimeRef.current,
        endTime: speechEndTime,
        confidence: vadStatus?.confidence || 0.5,
        duration: speechDuration
      }

      console.log(`ğŸ¤ Processing speech segment: ${speechDuration.toFixed(1)}s, confidence: ${speechSegment.confidence.toFixed(2)}`)

      // Whisper APIë¡œ ì „ì†¡
      await sendSpeechToWhisper(speechSegment)

      // ìŒì„± ë…¹ìŒ ìƒíƒœ ë¦¬ì…‹
      resetSpeechRecording()

    } catch (error) {
      console.error('âŒ Speech segment processing error:', error)
      resetSpeechRecording()
    }
  }

  // ìŒì„± ë…¹ìŒ ìƒíƒœ ë¦¬ì…‹
  const resetSpeechRecording = () => {
    isRecordingSpeechRef.current = false
    speechBufferRef.current = []
    speechStartTimeRef.current = 0
  }

  // ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± - Web Audio APIë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ì‹œê°„ ê¸°ë°˜ ì¶”ì¶œ
  const createAudioSegment = async (startTime: number, endTime: number, queue: 'A' | 'B'): Promise<AudioSegment | null> => {
    const segmentStartTime = Date.now()
    
    try {
      // ì¤‘ë³µ ì„¸ê·¸ë¨¼íŠ¸ ë°©ì§€: ê°™ì€ ì‹œê°„ëŒ€ì˜ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°
      const segmentKey = `${queue}-${Math.floor(startTime / 16000)}`
      if (processedSegments.current.has(segmentKey)) {
        console.log(`â­ï¸ Segment already processed: ${segmentKey}`)
        return null
      }
      
      // í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
      const blobStartTime = Date.now()
      const fullAudioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
      const blobTime = Date.now() - blobStartTime
      
      // ìµœì†Œ ê¸¸ì´ ì²´í¬ (Whisper API ìš”êµ¬ì‚¬í•­)
      if (fullAudioBlob.size < 1000) {
        console.log('âš ï¸ Audio too small, skipping...')
        return null
      }
      
      // ìµœëŒ€ ê¸¸ì´ ì²´í¬ (Whisper API ì œí•œ: 25MB)
      if (fullAudioBlob.size > 25 * 1024 * 1024) {
        console.log('âš ï¸ Audio too large, skipping...')
        return null
      }
      
      // Web Audio APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì‹œê°„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
      const extractStartTime = Date.now()
      const audioSegment = await extractTimeSegment(fullAudioBlob, startTime, endTime)
      const extractTime = Date.now() - extractStartTime
      
      if (!audioSegment) {
        console.log('âš ï¸ Failed to extract time segment')
        return null
      }
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ - ë¬´ìŒ êµ¬ê°„ í•„í„°ë§ (ë” ê´€ëŒ€í•˜ê²Œ)
      const levelCheckStartTime = Date.now()
      const hasAudio = await checkAudioLevel(audioSegment)
      const levelCheckTime = Date.now() - levelCheckStartTime
      
      if (!hasAudio) {
        console.log('ğŸ”‡ Silent segment detected, skipping Whisper API call')
        // ì²˜ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ê¸°ë¡í•˜ë˜ Whisper í˜¸ì¶œì€ í•˜ì§€ ì•ŠìŒ
        processedSegments.current.add(segmentKey)
        return null
      }
      
      const totalSegmentTime = Date.now() - segmentStartTime
      console.log(`ğŸ“Š Audio segment created: ${queue} - Size: ${audioSegment.size} bytes, Time: ${startTime}-${endTime}ms`)
      console.log(`â±ï¸ Segment creation time: ${totalSegmentTime}ms (Blob: ${blobTime}ms, Extract: ${extractTime}ms, Level: ${levelCheckTime}ms)`)
      
      // ì²˜ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë¡
      processedSegments.current.add(segmentKey)
      
      return {
        id: `${queue}-${Date.now()}`,
        audioBlob: audioSegment,
        startTime,
        endTime,
        queue
      }
    } catch (error) {
      const errorTime = Date.now() - segmentStartTime
      console.error(`âŒ Failed to create audio segment after ${errorTime}ms:`, error)
      return null
    }
  }

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ - ë¬´ìŒ êµ¬ê°„ ê°ì§€
  const checkAudioLevel = async (audioBlob: Blob): Promise<boolean> => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      const arrayBuffer = await audioBlob.arrayBuffer()
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
      
      // ì˜¤ë””ì˜¤ ë°ì´í„° ë¶„ì„
      const channelData = audioBuffer.getChannelData(0) // ì²« ë²ˆì§¸ ì±„ë„
      const length = channelData.length
      
      // RMS (Root Mean Square) ê³„ì‚°ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •
      let sum = 0
      for (let i = 0; i < length; i++) {
        sum += channelData[i] * channelData[i]
      }
      const rms = Math.sqrt(sum / length)
      
      // ì„ê³„ê°’ ì„¤ì • (ì¡°ì • ê°€ëŠ¥)
      const threshold = 0.005 // 0.5% - ë” ë‚®ì¶°ì„œ ì‹¤ì œ ìŒì„±ë„ ê°ì§€í•˜ë„ë¡
      
      console.log(`ğŸ”Š Audio level: ${rms.toFixed(4)} (threshold: ${threshold})`)
      
      audioContext.close()
      return rms > threshold
    } catch (error) {
      console.error('âŒ Audio level check failed:', error)
      // ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ true ë°˜í™˜ (Whisper í˜¸ì¶œ í—ˆìš©)
      return true
    }
  }

  // Web Audio APIë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ ì‹œê°„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
  const extractTimeSegment = async (fullAudioBlob: Blob, startTime: number, endTime: number): Promise<Blob | null> => {
    try {
      // AudioContext ìƒì„±
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      
      // ì˜¤ë””ì˜¤ íŒŒì¼ì„ ArrayBufferë¡œ ë³€í™˜
      const arrayBuffer = await fullAudioBlob.arrayBuffer()
      
      // ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë”©
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
      
      // ì „ì²´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ë°€ë¦¬ì´ˆ)
      const totalDuration = (audioBuffer.length / audioBuffer.sampleRate) * 1000
      
      // ìš”ì²­ëœ ì‹œê°„ì´ ì „ì²´ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
      if (startTime >= totalDuration) {
        console.log(`âš ï¸ Start time ${startTime}ms exceeds total duration ${totalDuration}ms`)
        audioContext.close()
        return null
      }
      
      // ì‹¤ì œ ë ì‹œê°„ ê³„ì‚° (ìš”ì²­ëœ ë ì‹œê°„ê³¼ ì „ì²´ ê¸¸ì´ ì¤‘ ì‘ì€ ê°’)
      const actualEndTime = Math.min(endTime, totalDuration)
      
      // ìƒ˜í”Œ ì¸ë±ìŠ¤ ê³„ì‚°
      const sampleRate = audioBuffer.sampleRate
      const startSample = Math.floor(startTime / 1000 * sampleRate)
      const endSample = Math.floor(actualEndTime / 1000 * sampleRate)
      const segmentLength = endSample - startSample
      
      // ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
      if (segmentLength < sampleRate * 0.5) { // 0.5ì´ˆ ë¯¸ë§Œ
        console.log(`âš ï¸ Segment too short: ${segmentLength / sampleRate}s`)
        audioContext.close()
        return null
      }
      
      // ìƒˆë¡œìš´ AudioBuffer ìƒì„± (ì„¸ê·¸ë¨¼íŠ¸ìš©)
      const segmentBuffer = audioContext.createBuffer(
        audioBuffer.numberOfChannels,
        segmentLength,
        sampleRate
      )
      
      // ê° ì±„ë„ì˜ ë°ì´í„° ë³µì‚¬
      for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
        const channelData = audioBuffer.getChannelData(channel)
        const segmentData = segmentBuffer.getChannelData(channel)
        
        for (let i = 0; i < segmentLength; i++) {
          const sourceIndex = startSample + i
          if (sourceIndex < channelData.length) {
            segmentData[i] = channelData[sourceIndex]
          }
        }
      }
      
      // AudioBufferë¥¼ WAV Blobìœ¼ë¡œ ë³€í™˜
      const segmentBlob = await audioBufferToWavBlob(segmentBuffer)
      
      // AudioContext ì •ë¦¬
      audioContext.close()
      
      console.log(`âœ… Extracted segment: ${startTime}-${actualEndTime}ms (${segmentLength / sampleRate}s) - Size: ${segmentBlob.size} bytes`)
      return segmentBlob
      
    } catch (error) {
      console.error('âŒ Failed to extract time segment:', error)
      return null
    }
  }

  // AudioBufferë¥¼ WAV Blobìœ¼ë¡œ ë³€í™˜
  const audioBufferToWavBlob = async (audioBuffer: AudioBuffer): Promise<Blob> => {
    const length = audioBuffer.length
    const sampleRate = audioBuffer.sampleRate
    const channels = audioBuffer.numberOfChannels
    
    // Float32Arrayë¥¼ Int16Arrayë¡œ ë³€í™˜ (16ë¹„íŠ¸ PCM)
    const interleaved = new Int16Array(length * channels)
    
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < channels; channel++) {
        // Float32 (-1 to 1)ë¥¼ Int16 (-32768 to 32767)ë¡œ ë³€í™˜
        const sample = Math.max(-1, Math.min(1, audioBuffer.getChannelData(channel)[i]))
        interleaved[i * channels + channel] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF
      }
    }
    
    // WAV íŒŒì¼ í—¤ë” ìƒì„±
    const wavBlob = createWavBlob(interleaved, sampleRate, channels)
    return wavBlob
  }

  // WAV Blob ìƒì„±
  const createWavBlob = (samples: Int16Array, sampleRate: number, channels: number): Blob => {
    const buffer = new ArrayBuffer(44 + samples.length * 2)
    const view = new DataView(buffer)
    
    // WAV í—¤ë” ì‘ì„±
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    // RIFF í—¤ë”
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + samples.length * 2, true) // íŒŒì¼ í¬ê¸°
    writeString(8, 'WAVE')
    
    // fmt ì²­í¬
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true) // fmt ì²­í¬ í¬ê¸°
    view.setUint16(20, 1, true) // PCM í¬ë§·
    view.setUint16(22, channels, true) // ì±„ë„ ìˆ˜
    view.setUint32(24, sampleRate, true) // ìƒ˜í”Œë ˆì´íŠ¸
    view.setUint32(28, sampleRate * channels * 2, true) // ë°”ì´íŠ¸ë ˆì´íŠ¸
    view.setUint16(32, channels * 2, true) // ë¸”ë¡ ì–¼ë¼ì¸
    view.setUint16(34, 16, true) // ë¹„íŠ¸í¼ìƒ˜í”Œ
    
    // data ì²­í¬
    writeString(36, 'data')
    view.setUint32(40, samples.length * 2, true) // ë°ì´í„° í¬ê¸°
    
    // ìƒ˜í”Œ ë°ì´í„° ì‘ì„±
    for (let i = 0; i < samples.length; i++) {
      view.setInt16(44 + i * 2, samples[i], true)
    }
    
    return new Blob([buffer], { type: 'audio/wav' })
  }

  // ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ë¥¼ Whisper APIë¡œ ì „ì†¡
  const sendSpeechToWhisper = async (segment: SpeechSegment) => {
    const requestStartTime = Date.now()
    
    // ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€
    const requestKey = `${segment.id}-${segment.audioBlob.size}`
    if (pendingRequests.current.has(requestKey)) {
      console.log(`â­ï¸ Request already pending: ${requestKey}`)
      return
    }
    
    pendingRequests.current.add(requestKey)
    
    try {
      console.log(`ğŸ¤ Sending speech segment ${segment.id} to Whisper... (Size: ${segment.audioBlob.size} bytes, Duration: ${segment.duration.toFixed(1)}s)`)
      
      const formDataStartTime = Date.now()
      const formData = new FormData()
      formData.append('file', segment.audioBlob, 'speech.wav')
      formData.append('model', 'whisper-1')
      // ISO-639-1 í˜•ì‹ìœ¼ë¡œ ì–¸ì–´ ì½”ë“œ ë³€í™˜
      const languageCode = lang ? lang.split('-')[0] : 'en'
      formData.append('language', languageCode)
      formData.append('response_format', 'verbose_json')
      formData.append('temperature', '0.2') // ì•½ê°„ì˜ ë¬¸ë§¥ ê³ ë ¤ í—ˆìš©
      const formDataTime = Date.now() - formDataStartTime
      
      console.log(`ğŸ“¦ FormData preparation: ${formDataTime}ms`)
      
      const apiCallStartTime = Date.now()
      const response = await fetch('/api/whisper', {
        method: 'POST',
        body: formData,
      })
      
      if (!response.ok) {
        throw new Error(`Whisper API error: ${response.status}`)
      }
      
      const result = await response.json()
      const apiCallTime = Date.now() - apiCallStartTime
      
      console.log(`â±ï¸ Whisper API call completed: ${apiCallTime}ms`)
      
      // í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ í•„í„°ë§
      const filteringStartTime = Date.now()
      let filteredText = result.text
      
      const promptPatterns = [
        'thankyou',
        'This is a live speech transcription',
        'Use proper grammar',
        'Transcribe exactly what is spoken',
        'Use proper spelling',
        'Avoid filler words',
        'Use proper punctuation'
      ]
      
      for (const pattern of promptPatterns) {
        filteredText = filteredText.replace(new RegExp(pattern, 'gi'), '').trim()
      }
      
      // ë°˜ë³µ í…ìŠ¤íŠ¸ í•„í„°ë§ ê°•í™”
      const repetitivePatterns = [
        /(\b\w+\b)(?:\s+\1){2,}/gi, // ê°™ì€ ë‹¨ì–´ê°€ 3ë²ˆ ì´ìƒ ë°˜ë³µ
        /(\b\w+\s+\w+\b)(?:\s+\1){1,}/gi, // ê°™ì€ 2ë‹¨ì–´ ì¡°í•©ì´ 2ë²ˆ ì´ìƒ ë°˜ë³µ
        /(\b\w+\s+\w+\s+\w+\b)(?:\s+\1){1,}/gi, // ê°™ì€ 3ë‹¨ì–´ ì¡°í•©ì´ 2ë²ˆ ì´ìƒ ë°˜ë³µ
      ]
      
      for (const pattern of repetitivePatterns) {
        filteredText = filteredText.replace(pattern, (match: string, group: string) => {
          // ë°˜ë³µì„ ì œê±°í•˜ê³  ì›ë³¸ ê·¸ë£¹ë§Œ ìœ ì§€
          return group
        })
      }
      
      // íŠ¹ì • ë°˜ë³µ íŒ¨í„´ ì§ì ‘ ì œê±° (ë” ì—„ê²©í•˜ê²Œ)
      const specificRepetitions = [
        /korean\s+war/gi,
        /and\s+then/gi,
        /squeak\s+and\s+ding/gi,
        /amen/gi,
        /door/gi,
        /for\s+free/gi,
        /doing\s+it/gi,
        /yeah/gi,
        /okay/gi,
        /hello/gi,
        /hi/gi,
        /um/gi,
        /uh/gi,
        /ah/gi,
        /oh/gi
      ]
      
      for (const pattern of specificRepetitions) {
        filteredText = filteredText.replace(pattern, (match: string) => {
          // ì²« ë²ˆì§¸ ë°œìƒë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
          const words = match.split(/\s+/)
          return words[0] + (words[1] ? ' ' + words[1] : '')
        })
      }
      
      // ê³¼ë„í•œ ë°˜ë³µ í…ìŠ¤íŠ¸ ì™„ì „ ì œê±°
      const excessiveRepetitions = [
        /^(yeah\s*){3,}$/gi,
        /^(okay\s*){3,}$/gi,
        /^(hello\s*){3,}$/gi,
        /^(hi\s*){3,}$/gi,
        /^(hello,\s*hello\s*){5,}/gi,
        /^(hello\s*hello\s*){5,}/gi,
        /^(I'm going to go ahead and get started\s*){2,}/gi
      ]
      
      for (const pattern of excessiveRepetitions) {
        if (pattern.test(filteredText)) {
          console.log('âš ï¸ Excessive repetition detected, skipping text')
          pendingRequests.current.delete(requestKey)
          return
        }
      }
      
      // íŠ¹ì • ë°˜ë³µ êµ¬ë¬¸ ì™„ì „ ì œê±°
      if (filteredText.includes('hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello')) {
        console.log('âš ï¸ Excessive hello repetition detected, skipping text')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      // scratching noises ë°˜ë³µ ì œê±°
      if (filteredText.includes('**scratching noises** **scratching noises** **scratching noises**')) {
        console.log('âš ï¸ Excessive scratching noises detected, skipping text')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      // ë¶ˆì™„ì „í•œ ë‹¨ì–´ë“¤ë¡œ ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ ì œê±°
      if (/^[A-Za-z]\s/.test(filteredText) && filteredText.length < 10) {
        console.log('âš ï¸ Incomplete word at start detected, skipping text')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      // ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì²´í¬
      if (!filteredText || filteredText.length < 1) {
        console.log('âš ï¸ Empty or too short text after filtering')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      const filteringTime = Date.now() - filteringStartTime
      console.log(`ğŸ” Text filtering: ${filteringTime}ms`)
      
      // STT ê²°ê³¼ ì²˜ë¦¬
      const processingStartTime = Date.now()
      const sttResult: STTResult = {
        id: segment.id,
        text: filteredText,
        startTime: segment.startTime,
        endTime: segment.endTime,
        confidence: result.confidence || segment.confidence,
        queue: 'A', // VAD ê¸°ë°˜ì´ë¯€ë¡œ ë‹¨ì¼ í
        timestamp: Date.now()
      }
      
      // í…ìŠ¤íŠ¸ ë²„í¼ì— ì¶”ê°€
      textBufferRef.current.addResult(sttResult)
      
      // UI ì—…ë°ì´íŠ¸
      const currentText = textBufferRef.current.getCurrentText()
      const partialText = textBufferRef.current.getPartialText()
      
      onTranscriptUpdate(currentText, partialText.length > 0)
      
      const processingTime = Date.now() - processingStartTime
      const totalTime = Date.now() - requestStartTime
      
      console.log(`âœ… Whisper result for ${segment.id}: "${filteredText}"`)
      console.log(`â±ï¸ Total processing time: ${totalTime}ms (API: ${apiCallTime}ms, Filter: ${filteringTime}ms, Process: ${processingTime}ms)`)
      
      // ìš”ì²­ ì™„ë£Œ í›„ pendingRequestsì—ì„œ ì œê±°
      pendingRequests.current.delete(requestKey)
      // ì¤‘ìš”: audioChunksRef ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
      audioChunksRef.current = []
      
    } catch (error) {
      const errorTime = Date.now() - requestStartTime
      
      // ì„œë²„ ì—°ê²° ì˜¤ë¥˜ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
      if (error instanceof TypeError && error.message.includes('Failed to fetch')) {
        console.error(`âŒ Server connection error after ${errorTime}ms:`, error.message)
        onError('Server connection failed. Please check if the development server is running.')
      } else {
        console.error(`âŒ Whisper API error after ${errorTime}ms:`, error)
        onError(`Whisper API error: ${error}`)
      }
      
      // ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ pendingRequestsì—ì„œ ì œê±°
      pendingRequests.current.delete(requestKey)
      // ì¤‘ìš”: audioChunksRef ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
      audioChunksRef.current = []
    }
  }

  // Whisper APIë¡œ ì „ì†¡ (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼)
  const sendToWhisper = async (segment: AudioSegment) => {
    const requestStartTime = Date.now()
    
    // ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€
    const requestKey = `${segment.id}-${segment.audioBlob.size}`
    if (pendingRequests.current.has(requestKey)) {
      console.log(`â­ï¸ Request already pending: ${requestKey}`)
      return
    }
    
    pendingRequests.current.add(requestKey)
    
    try {
      console.log(`ğŸ¤ Sending segment ${segment.id} to Whisper... (Size: ${segment.audioBlob.size} bytes)`)
      
      const formDataStartTime = Date.now()
      const formData = new FormData()
      formData.append('file', segment.audioBlob, 'audio.wav')
      formData.append('model', 'whisper-1')
      // ISO-639-1 í˜•ì‹ìœ¼ë¡œ ì–¸ì–´ ì½”ë“œ ë³€í™˜
      const languageCode = lang ? lang.split('-')[0] : 'en'
      formData.append('language', languageCode)
      formData.append('response_format', 'verbose_json')
      // í”„ë¡¬í”„íŠ¸ ì œê±° - "Use proper grammar" ë¬¸ì œ í•´ê²°
      // formData.append('prompt', 'This is a live conversation. Transcribe exactly what is spoken. Use proper spelling for technical terms like "ChatGPT", "AI", "machine learning", "artificial intelligence". Avoid filler words like "um", "uh", "like". Use proper punctuation. Do not add any additional text or commentary.')
      formData.append('temperature', '0.2') // ì•½ê°„ì˜ ë¬¸ë§¥ ê³ ë ¤ í—ˆìš©
      const formDataTime = Date.now() - formDataStartTime
      
      console.log(`ğŸ“¦ FormData preparation: ${formDataTime}ms`)
      
      const apiCallStartTime = Date.now()
      const response = await fetch('/api/whisper', {
        method: 'POST',
        body: formData,
      })
      
      if (!response.ok) {
        throw new Error(`Whisper API error: ${response.status}`)
      }
      
      const result = await response.json()
      const apiCallTime = Date.now() - apiCallStartTime
      
              console.log(`â±ï¸ Whisper API call completed: ${apiCallTime}ms`)
        
        // í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ í•„í„°ë§
        const filteringStartTime = Date.now()
        let filteredText = result.text
        
        const promptPatterns = [
          'thankyou',
          'This is a live speech transcription',
          'Use proper grammar',
          'Transcribe exactly what is spoken',
          'Use proper spelling',
          'Avoid filler words',
          'Use proper punctuation'
        ]
        
        for (const pattern of promptPatterns) {
          filteredText = filteredText.replace(new RegExp(pattern, 'gi'), '').trim()
        }
      
      // ë°˜ë³µ í…ìŠ¤íŠ¸ í•„í„°ë§ ê°•í™”
      const repetitivePatterns = [
        /(\b\w+\b)(?:\s+\1){2,}/gi, // ê°™ì€ ë‹¨ì–´ê°€ 3ë²ˆ ì´ìƒ ë°˜ë³µ
        /(\b\w+\s+\w+\b)(?:\s+\1){1,}/gi, // ê°™ì€ 2ë‹¨ì–´ ì¡°í•©ì´ 2ë²ˆ ì´ìƒ ë°˜ë³µ
        /(\b\w+\s+\w+\s+\w+\b)(?:\s+\1){1,}/gi, // ê°™ì€ 3ë‹¨ì–´ ì¡°í•©ì´ 2ë²ˆ ì´ìƒ ë°˜ë³µ
      ]
      
      for (const pattern of repetitivePatterns) {
        filteredText = filteredText.replace(pattern, (match: string, group: string) => {
          // ë°˜ë³µì„ ì œê±°í•˜ê³  ì›ë³¸ ê·¸ë£¹ë§Œ ìœ ì§€
          return group
        })
      }
      
      // íŠ¹ì • ë°˜ë³µ íŒ¨í„´ ì§ì ‘ ì œê±° (ë” ì—„ê²©í•˜ê²Œ)
      const specificRepetitions = [
        /korean\s+war/gi,
        /and\s+then/gi,
        /squeak\s+and\s+ding/gi,
        /amen/gi,
        /door/gi,
        /for\s+free/gi,
        /doing\s+it/gi,
        /yeah/gi,
        /okay/gi,
        /hello/gi,
        /hi/gi,
        /um/gi,
        /uh/gi,
        /ah/gi,
        /oh/gi
      ]
      
      for (const pattern of specificRepetitions) {
        filteredText = filteredText.replace(pattern, (match: string) => {
          // ì²« ë²ˆì§¸ ë°œìƒë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
          const words = match.split(/\s+/)
          return words[0] + (words[1] ? ' ' + words[1] : '')
        })
      }
      
      // ê³¼ë„í•œ ë°˜ë³µ í…ìŠ¤íŠ¸ ì™„ì „ ì œê±°
      const excessiveRepetitions = [
        /^(yeah\s*){3,}$/gi,
        /^(okay\s*){3,}$/gi,
        /^(hello\s*){3,}$/gi,
        /^(hi\s*){3,}$/gi,
        /^(hello,\s*hello\s*){5,}/gi,
        /^(hello\s*hello\s*){5,}/gi,
        /^(I'm going to go ahead and get started\s*){2,}/gi
      ]
      
      for (const pattern of excessiveRepetitions) {
        if (pattern.test(filteredText)) {
          console.log('âš ï¸ Excessive repetition detected, skipping text')
          pendingRequests.current.delete(requestKey)
          return
        }
      }
      
      // íŠ¹ì • ë°˜ë³µ êµ¬ë¬¸ ì™„ì „ ì œê±°
      if (filteredText.includes('hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello, hello')) {
        console.log('âš ï¸ Excessive hello repetition detected, skipping text')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      // scratching noises ë°˜ë³µ ì œê±°
      if (filteredText.includes('**scratching noises** **scratching noises** **scratching noises**')) {
        console.log('âš ï¸ Excessive scratching noises detected, skipping text')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      // ë¶ˆì™„ì „í•œ ë‹¨ì–´ë“¤ë¡œ ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ ì œê±°
      if (/^[A-Za-z]\s/.test(filteredText) && filteredText.length < 10) {
        console.log('âš ï¸ Incomplete word at start detected, skipping text')
        pendingRequests.current.delete(requestKey)
        return
      }
      
      // ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì²´í¬
      if (!filteredText || filteredText.length < 1) {
        console.log('âš ï¸ Empty or too short text after filtering')
        pendingRequests.current.delete(requestKey)
        return
      }
        
                 const filteringTime = Date.now() - filteringStartTime
         console.log(`ğŸ” Text filtering: ${filteringTime}ms`)
      
      // STT ê²°ê³¼ ì²˜ë¦¬
      const processingStartTime = Date.now()
      const sttResult: STTResult = {
        id: segment.id,
        text: filteredText,
        startTime: segment.startTime,
        endTime: segment.endTime,
        confidence: result.confidence || 0.8,
        queue: segment.queue,
        timestamp: Date.now()
      }
      
      // í…ìŠ¤íŠ¸ ë²„í¼ì— ì¶”ê°€
      textBufferRef.current.addResult(sttResult)
      
      // UI ì—…ë°ì´íŠ¸
      const currentText = textBufferRef.current.getCurrentText()
      const partialText = textBufferRef.current.getPartialText()
      
      onTranscriptUpdate(currentText, partialText.length > 0)
      
      const processingTime = Date.now() - processingStartTime
      const totalTime = Date.now() - requestStartTime
      
      console.log(`âœ… Whisper result for ${segment.id}: "${filteredText}"`)
      console.log(`â±ï¸ Total processing time: ${totalTime}ms (API: ${apiCallTime}ms, Filter: ${filteringTime}ms, Process: ${processingTime}ms)`)
      
      // ìš”ì²­ ì™„ë£Œ í›„ pendingRequestsì—ì„œ ì œê±°
      pendingRequests.current.delete(requestKey)
      // ì¤‘ìš”: audioChunksRef ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
      audioChunksRef.current = []
      
    } catch (error) {
      const errorTime = Date.now() - requestStartTime
      
      // ì„œë²„ ì—°ê²° ì˜¤ë¥˜ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
      if (error instanceof TypeError && error.message.includes('Failed to fetch')) {
        console.error(`âŒ Server connection error after ${errorTime}ms:`, error.message)
        onError('Server connection failed. Please check if the development server is running.')
      } else {
        console.error(`âŒ Whisper API error after ${errorTime}ms:`, error)
        onError(`Whisper API error: ${error}`)
      }
      
      // ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ pendingRequestsì—ì„œ ì œê±°
      pendingRequests.current.delete(requestKey)
      // ì¤‘ìš”: audioChunksRef ì´ˆê¸°í™” (ëˆ„ì  ë°©ì§€)
      audioChunksRef.current = []
    }
  }

  // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
  const requestMicrophonePermission = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000,
        },
      })
      
      setHasPermission(true)
      setStatus('Permission granted')
      
      // MediaRecorder ì„¤ì • - ë” ë‚˜ì€ í’ˆì§ˆë¡œ ì„¤ì •
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus',
        audioBitsPerSecond: 128000 // 128kbps for better quality
      })
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
          // VAD ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¦‰ì‹œ ì „ë‹¬
          processVADAudio(event.data)
        }
      }
      
      mediaRecorderRef.current = mediaRecorder
      
      return true
    } catch (error) {
      console.error('âŒ Microphone permission error:', error)
      setHasPermission(false)
      setStatus('Permission denied')
      onError('Microphone permission denied')
      return false
    }
  }

  // ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    if (!hasPermission) {
      const granted = await requestMicrophonePermission()
      if (!granted) {
        onError('Microphone permission denied')
        return
      }
    }

    if (!mediaRecorderRef.current) {
      onError('MediaRecorder not initialized')
      return
    }

    try {
      // MediaRecorder ìƒíƒœ í™•ì¸ ë° ì •ë¦¬
      if (mediaRecorderRef.current.state === 'recording') {
        console.log('ğŸ”„ MediaRecorder is already recording, stopping first...')
        mediaRecorderRef.current.stop()
        // ì ì‹œ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œì‘
        await new Promise(resolve => setTimeout(resolve, 100))
      }

      // ì„¸ì…˜ ì´ˆê¸°í™”
      console.log(`ğŸš€ Initializing STT session: ${sessionId}`)
      await fetch('/api/stt-stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          type: 'start',
          sessionId: sessionId,
        }),
      })

      // í…ìŠ¤íŠ¸ ë²„í¼ì— ì„¸ì…˜ ID ì„¤ì •
      textBufferRef.current.setSessionId(sessionId)
      
      // ì„¸ì…˜ ê¸°ë°˜ ì´ˆê¸°í™”
      if (currentSessionIdRef.current !== sessionId) {
        console.log(`ğŸ”„ New session detected, initializing VAD: ${sessionId}`)
        sessionStartTimeRef.current = Date.now()
        currentSessionIdRef.current = sessionId
        
        // VAD í™œì„±í™” (ì•ˆì •ì ì¸ ì„¤ì •ìœ¼ë¡œ)
        vadRef.current = createVAD({
          sampleRate: 16000,
          threshold: 0.8, // ë§¤ìš° ì—„ê²©í•œ ì„ê³„ê°’ (80% ì´ìƒ)
          silenceThreshold: 3.0, // 3ì´ˆ ë¬´ìŒ í›„ ì¢…ë£Œ
          speechThreshold: 2.0, // 2ì´ˆ ì´ìƒ ìŒì„± ìš”êµ¬
          smoothingWindow: 10, // ë” í° ìœˆë„ìš°ë¡œ ì•ˆì •í™”
          minBlobSize: 5000 // 5KB ì´ìƒì´ë©´ ìŒì„±ìœ¼ë¡œ ê°„ì£¼ (ë§¤ìš° ì—„ê²©)
        })
        console.log('ğŸ¤ VAD enabled with very strict settings')
      }
      
      // ë…¹ìŒ ì‹œì‘
      audioChunksRef.current = []
      processedSegments.current.clear()
      pendingRequests.current.clear()
      
      // VAD ìƒíƒœ ë¦¬ì…‹
      if (vadRef.current) {
        vadRef.current.reset()
      }
      resetSpeechRecording()
      
      isActiveRef.current = true
      setIsRecordingState(true)
      setStatus('Recording with VAD...')
      
      // MediaRecorder ìƒíƒœ ì¬í™•ì¸ í›„ ì‹œì‘
      if (mediaRecorderRef.current.state === 'inactive') {
        mediaRecorderRef.current.start(500) // 500msë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘ (ë” ë¹ ë¥¸ VAD ì²˜ë¦¬)
      } else {
        console.log('âš ï¸ MediaRecorder not in inactive state, recreating...')
        // MediaRecorder ì¬ìƒì„±
        const stream = mediaRecorderRef.current.stream
        const newMediaRecorder = new MediaRecorder(stream, {
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 128000
        })
        
        newMediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data)
            // VAD ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¦‰ì‹œ ì „ë‹¬
            processVADAudio(event.data)
          }
        }
        
        mediaRecorderRef.current = newMediaRecorder
        mediaRecorderRef.current.start(500)
      }
      
      console.log('ğŸ¤ Recording started with Whisper STT')
      
    } catch (error) {
      console.error('âŒ Failed to start recording:', error)
      onError(`Failed to start recording: ${error}`)
    }
  }

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = () => {
    if (!isActiveRef.current) return

    console.log('ğŸ›‘ Stopping Whisper STT recording...')
    
    isActiveRef.current = false
    setIsRecordingState(false)
    setStatus('Stopped')

    // MediaRecorder ì •ì§€
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }

    // ì§„í–‰ ì¤‘ì¸ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
    if (isRecordingSpeechRef.current && speechBufferRef.current.length > 0) {
      console.log('ğŸ”„ Processing final speech segment...')
      processSpeechSegment().then(() => {
        // ì„¸ì…˜ ì¢…ë£Œ
        fetch('/api/stt-stream', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            type: 'end',
            sessionId: sessionId,
          }),
        }).catch(error => {
          console.error('âŒ Failed to end session:', error)
        })
      })
    } else {
      // ì„¸ì…˜ ì¢…ë£Œ
      fetch('/api/stt-stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          type: 'end',
          sessionId: sessionId,
        }),
      }).catch(error => {
        console.error('âŒ Failed to end session:', error)
      })
    }

    console.log('âœ… Whisper STT recording stopped')
  }

  // ì„¸ì…˜ ìƒíƒœ ë³€ê²½ ì²˜ë¦¬
  useEffect(() => {
    if (isRecording && sessionId) {
      isActiveRef.current = true
      textBufferRef.current.setSessionId(sessionId)
      startRecording()
    } else if (!isRecording) {
      isActiveRef.current = false
      stopRecording()
    }
  }, [isRecording, sessionId])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸ ê´€ë¦¬
  useEffect(() => {
    mountedRef.current = true
    
    // Fast Refresh ë¬¸ì œ ë°©ì§€: ì„¸ì…˜ ê¸°ë°˜ ì´ˆê¸°í™”
    sessionStartTimeRef.current = 0
    currentSessionIdRef.current = ''
    
    return () => {
      mountedRef.current = false
      // ì •ë¦¬ ì‘ì—…
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        try {
          mediaRecorderRef.current.stop()
        } catch (error) {
          console.log('MediaRecorder stop error during cleanup:', error)
        }
      }
      
      // VAD ì •ë¦¬
      if (vadRef.current) {
        vadRef.current.reset()
      }
      
      // ìƒíƒœ ì´ˆê¸°í™”
      isActiveRef.current = false
      audioChunksRef.current = []
      speechBufferRef.current = []
      processedSegments.current.clear()
      pendingRequests.current.clear()
      resetSpeechRecording()
    }
  }, [])

  // isRecording prop ë³€ê²½ ì‹œ ì²˜ë¦¬
  useEffect(() => {
    if (!mountedRef.current) return
    
    if (isRecording && !isRecordingState) {
      console.log('ğŸ¤ Starting recording (prop changed)')
      startRecording()
    } else if (!isRecording && isRecordingState) {
      console.log('ğŸ›‘ Stopping recording (prop changed)')
      stopRecording()
    }
  }, [isRecording, isRecordingState])

  return (
    <div className='space-y-3'>
      {/* Status Display */}
      <div className='flex items-center space-x-2 text-sm'>
        <div
          className={`h-3 w-3 rounded-full ${
            isRecording ? 'animate-pulse bg-green-500' : hasPermission ? 'bg-yellow-500' : 'bg-gray-500'
          }`}
        />
        <span
          className={isRecording ? 'font-medium text-green-600' : hasPermission ? 'text-yellow-600' : 'text-gray-600'}
        >
          {isRecording ? 'ğŸ¤ Recording with VAD + Whisper' : status}
        </span>
        <span className='rounded bg-blue-100 px-2 py-1 text-xs font-medium text-blue-700'>VAD + Whisper-1</span>
      </div>

      {/* VAD Status */}
      {isRecording && (
        <div className='rounded border bg-green-50 p-2 text-xs'>
          {vadRef.current ? (
            // VAD í™œì„±í™” ìƒíƒœ
            vadStatus && (
              <>
                <div className='flex items-center justify-between'>
                  <span className='font-medium text-green-800'>
                    {vadStatus.isSpeech ? 'ğŸ¤ Speech Detected' : 'ğŸ”‡ Listening for Speech'}
                  </span>
                  <span className='text-green-700'>
                    Confidence: {(vadStatus.confidence * 100).toFixed(0)}%
                  </span>
                </div>
                <div className='mt-1 text-green-700'>
                  <span>Duration: {vadStatus.duration.toFixed(1)}s</span>
                  {vadStatus.isSpeech && (
                    <span className='ml-2'>â€¢ Speech: {vadStatus.speechDuration.toFixed(1)}s</span>
                  )}
                  {!vadStatus.isSpeech && (
                    <span className='ml-2'>â€¢ Silence: {vadStatus.silenceDuration.toFixed(1)}s</span>
                  )}
                </div>
                <div className='mt-1 text-xs text-green-600'>
                  {vadStatus.isSpeech ? 'Recording speech segment...' : 'Waiting for speech...'}
                </div>
                <div className='mt-1 text-xs text-orange-600'>
                  VAD: Active â€¢ Threshold: 80% â€¢ Min Size: 5KB â€¢ Min Duration: 2s
                </div>
              </>
            )
          ) : (
            // VAD ë¹„í™œì„±í™” ìƒíƒœ (fallback)
            <>
              <div className='flex items-center justify-between'>
                <span className='font-medium text-blue-800'>
                  â° Time-based Processing
                </span>
                <span className='text-blue-700'>
                  Every 10s
                </span>
              </div>
              <div className='mt-1 text-blue-700'>
                <span>Processing: Fixed 10-second segments</span>
              </div>
              <div className='mt-1 text-xs text-blue-600'>
                VAD: Disabled â€¢ Using traditional time-based approach
              </div>
            </>
          )}
        </div>
      )}

      {/* Controls */}
      {!hasPermission && (
        <button
          onClick={requestMicrophonePermission}
          className='w-full rounded-lg bg-blue-100 px-3 py-2 text-sm text-blue-800 hover:bg-blue-200'
        >
          ğŸ¤ Grant Microphone Permission
        </button>
      )}

      {/* Debug Info */}
      {process.env.NODE_ENV === 'development' && (
        <div className='rounded border bg-gray-50 p-2 text-xs'>
          <p className='text-gray-600'>ğŸ” Debug Info:</p>
          <p className='text-gray-600'>â€¢ Permission: {hasPermission ? 'Granted' : 'Not granted'}</p>
          <p className='text-gray-600'>â€¢ Recording: {isRecording ? 'Yes' : 'No'}</p>
          <p className='text-gray-600'>â€¢ Session: {sessionId || 'None'}</p>
          <p className='text-gray-600'>â€¢ Active: {isActiveRef.current ? 'Yes' : 'No'}</p>
          <p className='text-gray-600'>â€¢ VAD: {vadRef.current ? 'Active (strict settings)' : 'Not initialized'}</p>
          <p className='text-gray-600'>â€¢ Speech Recording: {isRecordingSpeechRef.current ? 'Yes' : 'No'}</p>
          <p className='text-gray-600'>â€¢ Status: {status}</p>
        </div>
      )}
    </div>
  )
} 