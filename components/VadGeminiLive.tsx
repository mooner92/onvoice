"use client"

import { useEffect, useRef, useState, useCallback } from 'react'
import { Mic, MicOff, Volume2 } from 'lucide-react'

interface VadGeminiLiveProps {
  sessionId: string
  onTranscriptUpdate: (text: string, translations: Record<string, string>) => void
  onPartialUpdate?: (text: string) => void
}

export function VadGeminiLive({ 
  sessionId, 
  onTranscriptUpdate, 
  onPartialUpdate 
}: VadGeminiLiveProps) {
  // ìƒíƒœ ê´€ë¦¬
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [audioLevel, setAudioLevel] = useState(0)
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const chunkIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const audioLevelIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const isProcessingRef = useRef(false)
  
  // ì„¤ì •
  const CHUNK_DURATION = 2000 // 2ì´ˆ ì²­í¬
  const AUDIO_LEVEL_UPDATE_INTERVAL = 100 // 100msë§ˆë‹¤ ì˜¤ë””ì˜¤ ë ˆë²¨ ì—…ë°ì´íŠ¸

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§
  const updateAudioLevel = useCallback(() => {
    if (!analyserRef.current) return

    const bufferLength = analyserRef.current.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    analyserRef.current.getByteFrequencyData(dataArray)

    // RMS ê³„ì‚°
    const rms = Math.sqrt(
      dataArray.reduce((sum, value) => sum + value * value, 0) / bufferLength
    ) / 255

    setAudioLevel(rms)
  }, [])

  // ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
  const processAudioSegment = useCallback(async (audioBlob: Blob) => {
    if (audioBlob.size < 1000 || isProcessingRef.current) {
      return
    }

    console.log('ğŸµ Processing audio segment:', {
      size: `${(audioBlob.size / 1024).toFixed(1)}KB`,
      duration: `~${(audioBlob.size / 16000).toFixed(1)}s`
    })

    isProcessingRef.current = true
    setIsProcessing(true)

    try {
      // Blobì„ Base64ë¡œ ë³€í™˜
      const arrayBuffer = await audioBlob.arrayBuffer()
      const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)))

      // Gemini Live API í˜¸ì¶œ
      const response = await fetch('/api/gemini-live', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          type: 'audio',
          sessionId,
          audioData: base64Audio,
          realtime: true,
          audioQuality: {
            confidence: 0.9,
            hasSpeech: true
          }
        })
      })

      if (!response.ok) {
        throw new Error(`API error: ${response.status}`)
      }

      const result = await response.json()
      
      if (result.success && result.result?.transcriptionText) {
        const { transcriptionText, translations } = result.result
        
        console.log('âœ… Transcription:', {
          text: transcriptionText,
          translations: Object.keys(translations || {})
        })
        
        // ì½œë°± í˜¸ì¶œ
        onTranscriptUpdate(transcriptionText, translations || {})
      }

    } catch (error) {
      console.error('âŒ Audio processing error:', error)
      setError(`Processing failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
    } finally {
      isProcessingRef.current = false
      setIsProcessing(false)
    }
  }, [sessionId, onTranscriptUpdate])

  // ë§ˆì´í¬ ì´ˆê¸°í™”
  const initializeMicrophone = useCallback(async () => {
    try {
      console.log('ğŸ¤ Initializing microphone...')
      
      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })

      streamRef.current = stream

      // AudioContext ì„¤ì •
      audioContextRef.current = new AudioContext({ sampleRate: 16000 })
      const source = audioContextRef.current.createMediaStreamSource(stream)
      analyserRef.current = audioContextRef.current.createAnalyser()
      
      analyserRef.current.fftSize = 2048
      analyserRef.current.smoothingTimeConstant = 0.3
      
      source.connect(analyserRef.current)

      // MediaRecorder ì„¤ì •
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : MediaRecorder.isTypeSupported('audio/mp4')
        ? 'audio/mp4'
        : 'audio/wav'

      mediaRecorderRef.current = new MediaRecorder(stream, { mimeType })

      // ë…¹ìŒ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
        if (audioBlob.size > 0) {
          processAudioSegment(audioBlob)
        }
        audioChunksRef.current = []
      }

      // Gemini Live ì„¸ì…˜ ì‹œì‘
      const response = await fetch('/api/gemini-live', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          type: 'start',
          sessionId,
          targetLanguages: ['ko', 'zh', 'hi']
        })
      })

      if (!response.ok) {
        throw new Error(`Failed to start Gemini session: ${response.status}`)
      }

      setIsRecording(true)
      setError(null)
      
      console.log('âœ… Microphone initialized')

    } catch (error) {
      console.error('âŒ Microphone initialization failed:', error)
      setError(`Microphone access failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
    }
  }, [sessionId, processAudioSegment])

  // ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(() => {
    if (!mediaRecorderRef.current || !isRecording) return

    try {
      audioChunksRef.current = []
      mediaRecorderRef.current.start()
      console.log('ğŸ”´ Recording started')
    } catch (error) {
      console.error('âŒ Failed to start recording:', error)
    }
  }, [isRecording])

  // ì •ë¦¬ í•¨ìˆ˜
  const cleanup = useCallback(() => {
    console.log('ğŸ§¹ Cleaning up resources...')
    
    setIsRecording(false)
    setIsProcessing(false)
    setAudioLevel(0)
    setError(null)
    
    // ì¸í„°ë²Œ ì •ë¦¬
    if (chunkIntervalRef.current) {
      clearInterval(chunkIntervalRef.current)
      chunkIntervalRef.current = null
    }
    
    if (audioLevelIntervalRef.current) {
      clearInterval(audioLevelIntervalRef.current)
      audioLevelIntervalRef.current = null
    }

    // MediaRecorder ì •ë¦¬
    if (mediaRecorderRef.current) {
      try {
        if (mediaRecorderRef.current.state === 'recording') {
          mediaRecorderRef.current.stop()
        }
      } catch (e) {
        console.warn('MediaRecorder stop error:', e)
      }
      mediaRecorderRef.current = null
    }

    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        try {
          track.stop()
        } catch (e) {
          console.warn('Track stop error:', e)
        }
      })
      streamRef.current = null
    }

    // AudioContext ì •ë¦¬
    if (audioContextRef.current) {
      try {
        if (audioContextRef.current.state !== 'closed') {
          audioContextRef.current.close()
        }
      } catch (e) {
        console.warn('AudioContext close error:', e)
      }
      audioContextRef.current = null
    }

    // ì˜¤ë””ì˜¤ ì²­í¬ ì •ë¦¬
    audioChunksRef.current = []
    isProcessingRef.current = false

    // Gemini Live ì„¸ì…˜ ì¢…ë£Œ
    fetch('/api/gemini-live', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        type: 'end',
        sessionId
      })
    }).catch(console.error)
  }, [sessionId])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸/ì–¸ë§ˆìš´íŠ¸
  useEffect(() => {
    initializeMicrophone()
    return cleanup
  }, [initializeMicrophone, cleanup])

  // ì²­í¬ ê¸°ë°˜ ë…¹ìŒ ì‹œì‘
  useEffect(() => {
    if (isRecording) {
      // ì •ê¸°ì ìœ¼ë¡œ ì²­í¬ ìƒì„±
      chunkIntervalRef.current = setInterval(() => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
          mediaRecorderRef.current.stop()
          setTimeout(startRecording, 100) // 100ms í›„ ë‹¤ì‹œ ì‹œì‘
        } else {
          startRecording()
        }
      }, CHUNK_DURATION)

      // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§
      audioLevelIntervalRef.current = setInterval(updateAudioLevel, AUDIO_LEVEL_UPDATE_INTERVAL)
      
      console.log('ğŸ”„ Chunk-based recording started')
    }
    
    return () => {
      if (chunkIntervalRef.current) {
        clearInterval(chunkIntervalRef.current)
        chunkIntervalRef.current = null
      }
      if (audioLevelIntervalRef.current) {
        clearInterval(audioLevelIntervalRef.current)
        audioLevelIntervalRef.current = null
      }
    }
  }, [isRecording, startRecording, updateAudioLevel])

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ì‹œê°í™”
  const getAudioLevelColor = () => {
    if (audioLevel < 0.01) return 'bg-gray-300'
    if (audioLevel < 0.03) return 'bg-yellow-400'
    if (audioLevel < 0.06) return 'bg-orange-500'
    return 'bg-red-500'
  }

  const getAudioLevelWidth = () => {
    return Math.min(audioLevel * 200, 100) // ìµœëŒ€ 100%
  }

  return (
    <div className="space-y-4">
      {/* ìƒíƒœ í‘œì‹œ */}
      <div className="flex items-center justify-between p-4 bg-gray-50 rounded-lg">
        <div className="flex items-center space-x-3">
          {/* ë§ˆì´í¬ ì•„ì´ì½˜ */}
          <div className={`p-2 rounded-full ${isRecording ? 'bg-green-100' : 'bg-gray-100'}`}>
            {isRecording ? (
              <Mic className="h-5 w-5 text-green-600" />
            ) : (
              <MicOff className="h-5 w-5 text-gray-400" />
            )}
          </div>
          
          {/* ìƒíƒœ í…ìŠ¤íŠ¸ */}
          <div>
            <div className="font-medium">
              {isProcessing ? 'ğŸ¤– Processing...' : 
               isRecording ? 'ğŸ¤ Recording' : 'âŒ Disconnected'}
            </div>
            <div className="text-sm text-gray-500">
              Chunk-based Speech Recognition
            </div>
          </div>
        </div>

        {/* ì˜¤ë””ì˜¤ ë ˆë²¨ í‘œì‹œ */}
        <div className="flex items-center space-x-2">
          <Volume2 className="h-4 w-4 text-gray-400" />
          <div className="w-20 h-2 bg-gray-200 rounded-full overflow-hidden">
            <div 
              className={`h-full transition-all duration-100 ${getAudioLevelColor()}`}
              style={{ width: `${getAudioLevelWidth()}%` }}
            />
          </div>
        </div>
      </div>

      {/* ì—ëŸ¬ í‘œì‹œ */}
      {error && (
        <div className="p-3 bg-red-50 border border-red-200 rounded-lg">
          <div className="text-sm text-red-800">{error}</div>
        </div>
      )}

      {/* ì„¤ì • ì •ë³´ */}
      <div className="text-xs text-gray-500 space-y-1">
        <div>â€¢ Chunk Duration: {CHUNK_DURATION}ms</div>
        <div>â€¢ Audio Level: {(audioLevel * 100).toFixed(1)}%</div>
        <div>â€¢ Status: {isRecording ? 'Active' : 'Inactive'}</div>
      </div>
    </div>
  )
} 